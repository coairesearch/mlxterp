{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mlxterp","text":"<p>Mechanistic Interpretability for MLX on Apple Silicon</p> <p>A clean, intuitive library for mechanistic interpretability research on Apple Silicon Macs, powered by MLX. Inspired by nnsight and nnterp, mlxterp brings their elegant API design to the MLX ecosystem.</p>"},{"location":"#why-mlxterp","title":"Why mlxterp?","text":"<ul> <li>\ud83c\udfaf Simple &amp; Clean API: Context managers and direct attribute access - no verbose method chains</li> <li>\ud83d\udd0c Model Agnostic: Works with ANY MLX model - no model-specific implementations needed</li> <li>\ud83d\ude80 Apple Silicon Optimized: Leverages MLX's unified memory and Metal acceleration</li> <li>\ud83d\udce6 Minimal Boilerplate: Lean codebase focused on accessibility</li> <li>\ud83d\udd27 Flexible Interventions: Easy activation patching, steering, and modification</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from mlxterp import InterpretableModel\nimport mlx.core as mx\n\n# Load any MLX model\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Trace execution and capture activations\nwith model.trace(\"The capital of France is\"):\n    # Direct attribute access to layers (use self_attn for mlx-lm models)\n    attn_3 = model.layers[3].self_attn.output.save()\n    mlp_8 = model.layers[8].mlp.output.save()\n    logits = model.output.save()\n\nprint(f\"Attention layer 3 output shape: {attn_3.shape}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code># Install uv if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Clone the repository\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Create environment and install\nuv sync\n\n# Activate the environment\nsource .venv/bin/activate\n</code></pre>"},{"location":"#using-pip","title":"Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"#features-at-a-glance","title":"Features at a Glance","text":""},{"location":"#context-manager-api","title":"Context Manager API","text":"<pre><code>with model.trace(\"Hello world\") as trace:\n    layer_5 = model.layers[5].output.save()\n\n# Access saved values after trace\nprint(trace.saved_values)\n</code></pre>"},{"location":"#activation-collection","title":"Activation Collection","text":"<pre><code>from mlxterp import get_activations\n\nactivations = get_activations(\n    model,\n    prompts=[\"Hello\", \"World\", \"Test\"],\n    layers=[3, 8, 12],\n    positions=-1  # Last token\n)\n</code></pre>"},{"location":"#interventions","title":"Interventions","text":"<pre><code>from mlxterp import interventions as iv\n\n# Scale activations\nwith model.trace(\"Test\", interventions={\"layers.3\": iv.scale(0.5)}):\n    output = model.output.save()\n\n# Add steering vectors\nsteering_vector = mx.random.normal((hidden_dim,))\nwith model.trace(\"Test\", interventions={\"layers.5\": iv.add_vector(steering_vector)}):\n    output = model.output.save()\n</code></pre>"},{"location":"#activation-patching","title":"Activation Patching","text":"<p>Find important layers with one function call:</p> <pre><code># Identify which layers are critical for factual recall\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    plot=True\n)\n\n# Analyze results\nsorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\nfor layer_idx, recovery in sorted_results[:3]:\n    print(f\"Layer {layer_idx}: {recovery:.1f}% recovery\")\n\n# Output:\n# Layer  0: +43.1% recovery  \u2190 Very important!\n# Layer 15: +24.2% recovery  \u2190 Important\n# Layer  6: +17.6% recovery  \u2190 Somewhat important\n</code></pre> <p>The <code>activation_patching()</code> helper automates the entire workflow. Learn more in the Activation Patching Guide for comprehensive coverage including interpretation and best practices.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>API Reference - Complete API documentation</li> <li>Examples - Detailed usage examples</li> <li>Activation Patching Guide - In-depth guide with interpretation</li> <li>Architecture - Design principles and implementation</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"#citation","title":"Citation","text":"<pre><code>@software{mlxterp,\n  title = {mlxterp: Mechanistic Interpretability for MLX},\n  author = {Sigurd Schacht},\n  year = {2025},\n  url = {https://github.com/coairesearch/mlxterp}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"API/","title":"mlxterp API Documentation","text":"<p>Complete API reference for mlxterp.</p>"},{"location":"API/#table-of-contents","title":"Table of Contents","text":"<ol> <li>InterpretableModel</li> <li>Tracing</li> <li>Interventions</li> <li>Utilities</li> <li>Core Components</li> </ol>"},{"location":"API/#interpretablemodel","title":"InterpretableModel","text":""},{"location":"API/#class-interpretablemodel","title":"Class: <code>InterpretableModel</code>","text":"<p>Main entry point for wrapping MLX models to add interpretability features.</p>"},{"location":"API/#constructor","title":"Constructor","text":"<pre><code>InterpretableModel(\n    model: Union[nn.Module, str],\n    tokenizer: Optional[Any] = None,\n    layer_attr: str = \"layers\",\n    embedding_path: Optional[str] = None,\n    norm_path: Optional[str] = None,\n    lm_head_path: Optional[str] = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li>model (<code>nn.Module</code> or <code>str</code>): Either:</li> <li>An MLX <code>nn.Module</code> instance to wrap</li> <li> <p>A model name/path string (attempts to load via <code>mlx_lm.load()</code>)</p> </li> <li> <p>tokenizer (<code>Optional[Any]</code>): Tokenizer for processing text inputs. If <code>None</code> and model is loaded from string, attempts to load tokenizer automatically.</p> </li> <li> <p>layer_attr (<code>str</code>, default: <code>\"layers\"</code>): Name of the attribute containing the model's transformer layers. Common values:</p> </li> <li><code>\"layers\"</code> (Llama, Mistral)</li> <li><code>\"h\"</code> (GPT-2)</li> <li> <p><code>\"transformer.h\"</code> (some GPT variants)</p> </li> <li> <p>embedding_path (<code>Optional[str]</code>, default: <code>None</code>): Override path for the token embedding layer. Used for weight-tied output projection in <code>get_token_predictions</code>. Auto-detected if not specified. Tried paths: <code>model.embed_tokens</code>, <code>model.model.embed_tokens</code>, <code>embed_tokens</code>, <code>tok_embeddings</code>, <code>wte</code>.</p> </li> <li> <p>norm_path (<code>Optional[str]</code>, default: <code>None</code>): Override path for the final layer normalization. Used in <code>logit_lens</code> for projecting intermediate activations. Auto-detected if not specified. Tried paths: <code>model.norm</code>, <code>model.model.norm</code>, <code>norm</code>, <code>ln_f</code>, <code>model.ln_f</code>.</p> </li> <li> <p>lm_head_path (<code>Optional[str]</code>, default: <code>None</code>): Override path for the output projection layer. If not found, falls back to weight-tied embedding. Tried paths: <code>lm_head</code>, <code>model.lm_head</code>, <code>model.model.lm_head</code>, <code>output</code>, <code>head</code>.</p> </li> </ul> <p>Returns: <code>InterpretableModel</code> instance</p> <p>Example:</p> <pre><code>from mlxterp import InterpretableModel\n\n# Load from model name (auto-detection works)\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Wrap existing model\nimport mlx.nn as nn\nbase_model = nn.Module()  # Your model\nmodel = InterpretableModel(base_model, tokenizer=my_tokenizer)\n\n# Custom layer attribute\nmodel = InterpretableModel(gpt2_model, layer_attr=\"h\")\n\n# Custom model with non-standard attribute names\nmodel = InterpretableModel(\n    custom_model,\n    tokenizer=my_tokenizer,\n    embedding_path=\"my_custom_embeddings\",\n    norm_path=\"my_final_norm\",\n    lm_head_path=\"my_output_projection\"\n)\n</code></pre>"},{"location":"API/#method-trace","title":"Method: <code>trace</code>","text":"<p>Create a tracing context for capturing activations and applying interventions.</p> <pre><code>model.trace(\n    inputs: Union[str, List[str], mx.array, List[int]],\n    interventions: Optional[Dict[str, Callable]] = None\n) -&gt; Trace\n</code></pre> <p>Parameters:</p> <ul> <li>inputs: Input data in various formats:</li> <li><code>str</code>: Single text prompt (requires tokenizer)</li> <li><code>List[str]</code>: Batch of text prompts (requires tokenizer)</li> <li><code>mx.array</code>: Token array, shape <code>(batch, seq_len)</code></li> <li> <p><code>List[int]</code>: Single sequence of token IDs</p> </li> <li> <p>interventions (<code>Optional[Dict[str, Callable]]</code>): Dictionary mapping module names to intervention functions. Module names use dot notation (e.g., <code>\"layers.3.self_attn\"</code> for mlx-lm models).</p> </li> </ul> <p>Returns: <code>Trace</code> context manager</p> <p>Example:</p> <pre><code># Basic tracing\nwith model.trace(\"Hello world\"):\n    output = model.output.save()\n\n# With interventions\nfrom mlxterp import interventions as iv\nwith model.trace(\"Test\", interventions={\"layers.3\": iv.scale(0.5)}):\n    output = model.output.save()\n\n# Batch inputs\nwith model.trace([\"Hello\", \"World\"]):\n    acts = model.layers[3].output.save()\n</code></pre>"},{"location":"API/#method-encode","title":"Method: <code>encode</code>","text":"<p>Encode text to token IDs using the model's tokenizer.</p> <pre><code>model.encode(text: str) -&gt; List[int]\n</code></pre> <p>Parameters:</p> <ul> <li>text (<code>str</code>): Text string to encode</li> </ul> <p>Returns: List of token IDs</p> <p>Raises: <code>ValueError</code> if no tokenizer is available</p> <p>Example:</p> <pre><code>tokens = model.encode(\"Hello world\")\nprint(tokens)  # [128000, 9906, 1917]\n</code></pre>"},{"location":"API/#method-decode","title":"Method: <code>decode</code>","text":"<p>Decode token IDs to text using the model's tokenizer.</p> <pre><code>model.decode(tokens: Union[List[int], mx.array]) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li>tokens (<code>List[int]</code> or <code>mx.array</code>): Token IDs to decode</li> </ul> <p>Returns: Decoded text string</p> <p>Raises: <code>ValueError</code> if no tokenizer is available</p> <p>Example:</p> <pre><code>text = model.decode([128000, 9906, 1917])\nprint(text)  # \"&lt;|begin_of_text|&gt;Hello world\"\n\n# Also works with mx.array\nimport mlx.core as mx\ntokens_array = mx.array([128000, 9906, 1917])\ntext = model.decode(tokens_array)\n</code></pre>"},{"location":"API/#method-encode_batch","title":"Method: <code>encode_batch</code>","text":"<p>Encode multiple texts to token IDs.</p> <pre><code>model.encode_batch(texts: List[str]) -&gt; List[List[int]]\n</code></pre> <p>Parameters:</p> <ul> <li>texts (<code>List[str]</code>): List of text strings to encode</li> </ul> <p>Returns: List of token ID lists</p> <p>Raises: <code>ValueError</code> if no tokenizer is available</p> <p>Example:</p> <pre><code>token_lists = model.encode_batch([\"Hello\", \"World\", \"Test\"])\nprint(token_lists)\n# [[128000, 9906], [128000, 10343], [128000, 2323]]\n</code></pre>"},{"location":"API/#method-token_to_str","title":"Method: <code>token_to_str</code>","text":"<p>Convert a single token ID to its string representation.</p> <pre><code>model.token_to_str(token_id: int) -&gt; str\n</code></pre> <p>Parameters:</p> <ul> <li>token_id (<code>int</code>): Token ID to decode</li> </ul> <p>Returns: String representation of the token</p> <p>Raises: <code>ValueError</code> if no tokenizer is available</p> <p>Example:</p> <pre><code># Decode individual tokens\ntokens = model.encode(\"Hello world\")\nfor i, token_id in enumerate(tokens):\n    token_str = model.token_to_str(token_id)\n    print(f\"Token {i}: {token_id} -&gt; '{token_str}'\")\n\n# Output:\n# Token 0: 128000 -&gt; '&lt;|begin_of_text|&gt;'\n# Token 1: 9906 -&gt; 'Hello'\n# Token 2: 1917 -&gt; ' world'\n</code></pre>"},{"location":"API/#property-vocab_size","title":"Property: <code>vocab_size</code>","text":"<p>Get the vocabulary size of the tokenizer.</p> <pre><code>model.vocab_size -&gt; Optional[int]\n</code></pre> <p>Returns: Vocabulary size, or <code>None</code> if no tokenizer is available</p> <p>Example:</p> <pre><code>print(f\"Vocabulary size: {model.vocab_size}\")  # 128000\n</code></pre>"},{"location":"API/#attribute-tokenizer","title":"Attribute: <code>tokenizer</code>","text":"<p>Direct access to the underlying tokenizer for advanced operations.</p> <p>Type: Tokenizer object (varies by model)</p> <p>Example:</p> <pre><code># Access tokenizer directly for advanced features\ntokenizer = model.tokenizer\n\n# Use tokenizer-specific methods\nif hasattr(tokenizer, 'special_tokens'):\n    print(tokenizer.special_tokens)\n</code></pre>"},{"location":"API/#method-get_token_predictions","title":"Method: <code>get_token_predictions</code>","text":"<p>Decode hidden states to token predictions using the model's output projection.</p> <pre><code>model.get_token_predictions(\n    hidden_state: mx.array,\n    top_k: int = 10,\n    return_scores: bool = False,\n    embedding_layer: Optional[Any] = None,\n    lm_head: Optional[Any] = None\n) -&gt; Union[List[int], List[tuple]]\n</code></pre> <p>Parameters:</p> <ul> <li>hidden_state (<code>mx.array</code>): Hidden state tensor, shape <code>(hidden_dim,)</code> or <code>(batch, hidden_dim)</code></li> <li>top_k (<code>int</code>, default: <code>10</code>): Number of top predictions to return</li> <li>return_scores (<code>bool</code>, default: <code>False</code>): If True, return <code>(token_id, score)</code> tuples</li> <li>embedding_layer (<code>Optional[Any]</code>, default: <code>None</code>): Override embedding layer for weight-tied projection. If provided, uses this layer's weights for output projection.</li> <li>lm_head (<code>Optional[Any]</code>, default: <code>None</code>): Override lm_head layer. If provided, uses this layer directly. Takes precedence over <code>embedding_layer</code>.</li> </ul> <p>Returns: List of token IDs or <code>(token_id, score)</code> tuples</p> <p>Example:</p> <pre><code># Get predictions from a specific layer\nwith model.trace(\"The capital of France is\") as trace:\n    layer_6 = trace.activations[\"model.model.layers.6\"]\n\n# Get last token's hidden state\nlast_token_hidden = layer_6[0, -1, :]\n\n# Get top predictions\npredictions = model.get_token_predictions(last_token_hidden, top_k=5)\n\n# Decode to words\nfor token_id in predictions:\n    print(model.token_to_str(token_id))\n\n# With scores\npredictions_with_scores = model.get_token_predictions(\n    last_token_hidden,\n    top_k=5,\n    return_scores=True\n)\nfor token_id, score in predictions_with_scores:\n    token_str = model.token_to_str(token_id)\n    print(f\"{token_str}: {score:.2f}\")\n\n# Custom model with override at call time\npredictions = model.get_token_predictions(\n    hidden,\n    top_k=5,\n    lm_head=custom_model.my_lm_head\n)\n</code></pre> <p>Notes: - Automatically handles weight-tied models (uses embedding weights transposed) - Works with quantized embeddings (dequantizes automatically) - Model-agnostic: auto-detects embedding/lm_head paths for various architectures - Useful for analyzing what any layer \"thinks\" at a specific position</p>"},{"location":"API/#method-logit_lens","title":"Method: <code>logit_lens</code>","text":"<p>Apply logit lens technique to see what each layer predicts at each token position.</p> <p>The logit lens projects each layer's hidden states through the final layer norm and embedding matrix to see what tokens each layer predicts at each position in the input sequence.</p> <pre><code>model.logit_lens(\n    text: str,\n    top_k: int = 1,\n    layers: Optional[List[int]] = None,\n    position: Optional[int] = None,\n    plot: bool = False,\n    max_display_tokens: int = 15,\n    figsize: tuple = (16, 10),\n    cmap: str = 'viridis',\n    font_family: Optional[str] = None,\n    final_norm: Optional[Any] = None,\n    skip_norm: bool = False\n) -&gt; Dict[int, List[List[tuple]]]\n</code></pre> <p>Parameters:</p> <ul> <li>text (<code>str</code>): Input text to analyze</li> <li>top_k (<code>int</code>, default: <code>1</code>): Number of top predictions to return per position</li> <li>layers (<code>Optional[List[int]]</code>, default: <code>None</code>): Specific layers to analyze (None = all)</li> <li>position (<code>Optional[int]</code>, default: <code>None</code>): Specific position to analyze (None = all). Supports negative indexing (-1 = last position).</li> <li>plot (<code>bool</code>, default: <code>False</code>): If True, display a heatmap visualization showing predictions</li> <li>max_display_tokens (<code>int</code>, default: <code>15</code>): Maximum number of tokens to show in visualization (from the end)</li> <li>figsize (<code>tuple</code>, default: <code>(16, 10)</code>): Figure size for plot (width, height)</li> <li>cmap (<code>str</code>, default: <code>'viridis'</code>): Colormap for heatmap</li> <li>font_family (<code>Optional[str]</code>, default: <code>None</code>): Font for plot (use 'Arial Unicode MS' for CJK support)</li> <li>final_norm (<code>Optional[Any]</code>, default: <code>None</code>): Override final layer normalization module. If provided, uses this module instead of auto-detected norm layer.</li> <li>skip_norm (<code>bool</code>, default: <code>False</code>): If True, skip final layer normalization entirely. Useful for models without a final norm layer.</li> </ul> <p>Returns: Dict mapping <code>layer_idx</code> -&gt; list of positions -&gt; list of <code>(token_id, score, token_str)</code> tuples</p> <p>Structure: <code>{layer_idx: [[pos_0_predictions], [pos_1_predictions], ...]}</code></p> <p>Model Compatibility: This method automatically detects model structure and works with: - mlx-lm models (<code>model.model.norm</code>, <code>model.model.embed_tokens</code>) - GPT-2 style models (<code>ln_f</code>, <code>wte</code>) - Custom models (use <code>norm_path</code> constructor arg or <code>final_norm</code>/<code>skip_norm</code> parameters)</p> <p>Example:</p> <pre><code># Get predictions at all positions for all layers\nresults = model.logit_lens(\"The capital of France is\")\n\n# Access predictions for layer 10 at position 3\nlayer_10_predictions = results[10]\npos_3_top_pred = layer_10_predictions[3][0]  # (token_id, score, token_str)\nprint(f\"Layer 10, Position 3: {pos_3_top_pred[2]}\")\n\n# Show what each layer predicts at the LAST position\ntext = \"The capital of France is\"\nresults = model.logit_lens(text, layers=[0, 5, 10, 15])\n\nfor layer_idx in [0, 5, 10, 15]:\n    # Get prediction at last position\n    last_pos_pred = results[layer_idx][-1][0][2]\n    print(f\"Layer {layer_idx}: '{last_pos_pred}'\")\n\n# Output:\n# Layer  0: ' the'\n# Layer  5: ' a'\n# Layer 10: ' Paris'\n# Layer 15: ' Paris'\n\n# Show predictions at each position for a specific layer\ntext = \"The capital of France\"\nresults = model.logit_lens(text, layers=[10])\ntokens = model.encode(text)\n\nfor pos_idx, predictions in enumerate(results[10]):\n    input_token = model.token_to_str(tokens[pos_idx])\n    pred_token = predictions[0][2]  # Top prediction\n    print(f\"Position {pos_idx} ('{input_token}') -&gt; '{pred_token}'\")\n\n# Visualize with heatmap\nresults = model.logit_lens(\n    \"The Eiffel Tower is located in the city of\",\n    plot=True,\n    max_display_tokens=15,\n    figsize=(16, 10)\n)\n# Displays a heatmap with:\n#   - X-axis: Input token positions\n#   - Y-axis: Model layers\n#   - Cell values: Top predicted token at each (layer, position)\n#   - Colors: Different predictions shown with different colors\n\n# Model without final normalization\nresults = model.logit_lens(\"Hello world\", skip_norm=True)\n\n# Custom final norm at call time\nresults = model.logit_lens(\n    \"Hello world\",\n    final_norm=custom_model.my_final_norm\n)\n</code></pre> <p>Note: Plotting requires matplotlib: <code>pip install matplotlib</code></p> <p>Use Cases: - Understand how model predictions evolve through layers - Debug model behavior at intermediate layers - Visualize progressive refinement of predictions - Identify where in the model certain facts are computed</p>"},{"location":"API/#method-tuned_logit_lens","title":"Method: <code>tuned_logit_lens</code>","text":"<p>Apply tuned lens for improved layer-wise predictions.</p> <p>The tuned lens technique (Belrose et al., 2023) uses learned affine transformations for each layer to correct for coordinate system mismatches between layers, producing more accurate intermediate predictions than the standard logit lens.</p> <pre><code>model.tuned_logit_lens(\n    text: str,\n    tuned_lens: TunedLens,\n    top_k: int = 1,\n    layers: Optional[List[int]] = None,\n    position: Optional[int] = None,\n    plot: bool = False,\n    max_display_tokens: int = 15,\n    figsize: tuple = (16, 10),\n    cmap: str = 'viridis',\n    font_family: Optional[str] = None\n) -&gt; Dict[int, List[List[tuple]]]\n</code></pre> <p>Parameters:</p> <ul> <li>text (<code>str</code>): Input text to analyze</li> <li>tuned_lens (<code>TunedLens</code>): Trained TunedLens instance with layer translators</li> <li>top_k (<code>int</code>, default: <code>1</code>): Number of top predictions to return per position</li> <li>layers (<code>Optional[List[int]]</code>, default: <code>None</code>): Specific layers to analyze (None = all)</li> <li>position (<code>Optional[int]</code>, default: <code>None</code>): Specific position to analyze (None = all). Supports negative indexing.</li> <li>plot (<code>bool</code>, default: <code>False</code>): If True, display a heatmap visualization</li> <li>max_display_tokens (<code>int</code>, default: <code>15</code>): Maximum number of tokens to show in visualization</li> <li>figsize (<code>tuple</code>, default: <code>(16, 10)</code>): Figure size for plot</li> <li>cmap (<code>str</code>, default: <code>'viridis'</code>): Colormap for heatmap</li> <li>font_family (<code>Optional[str]</code>): Font for plot (auto-detected if None)</li> </ul> <p>Returns: Dict mapping <code>layer_idx</code> -&gt; list of positions -&gt; list of <code>(token_id, score, token_str)</code> tuples</p> <p>Example:</p> <pre><code>from mlxterp import InterpretableModel, TunedLens\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Option 1: Train new tuned lens\ntuned_lens = model.train_tuned_lens(\n    dataset=[\"Sample text 1\", \"Sample text 2\", ...],\n    num_steps=250,\n    save_path=\"tuned_lens_llama.npz\"\n)\n\n# Option 2: Load pre-trained tuned lens\ntuned_lens = model.load_tuned_lens(\"tuned_lens_llama.npz\")\n\n# Apply tuned lens\nresults = model.tuned_logit_lens(\n    \"The capital of France is\",\n    tuned_lens,\n    layers=[0, 5, 10, 15],\n    plot=True\n)\n\n# Compare with regular logit lens\nregular_results = model.logit_lens(\"The capital of France is\", layers=[0, 5, 10, 15])\n</code></pre> <p>Reference: Belrose et al., \"Eliciting Latent Predictions from Transformers with the Tuned Lens\" (https://arxiv.org/abs/2303.08112)</p>"},{"location":"API/#method-train_tuned_lens","title":"Method: <code>train_tuned_lens</code>","text":"<p>Train a tuned lens for this model.</p> <p>The tuned lens technique trains small affine transformations for each layer to correct for coordinate system mismatches, producing more accurate intermediate predictions.</p> <pre><code>model.train_tuned_lens(\n    dataset: List[str],\n    num_steps: int = 250,\n    learning_rate: float = 1.0,\n    momentum: float = 0.9,\n    max_seq_len: int = 2048,\n    batch_size: int = 1,\n    gradient_clip: float = 1.0,\n    save_path: Optional[str] = None,\n    verbose: bool = True,\n    callback: Optional[Callable[[int, float], None]] = None\n) -&gt; TunedLens\n</code></pre> <p>Parameters:</p> <ul> <li>dataset (<code>List[str]</code>): List of text strings for training</li> <li>num_steps (<code>int</code>, default: <code>250</code>): Number of training steps</li> <li>learning_rate (<code>float</code>, default: <code>1.0</code>): Initial learning rate (uses linear decay)</li> <li>momentum (<code>float</code>, default: <code>0.9</code>): Nesterov momentum coefficient</li> <li>max_seq_len (<code>int</code>, default: <code>2048</code>): Maximum sequence length for training chunks</li> <li>batch_size (<code>int</code>, default: <code>1</code>): Batch size</li> <li>gradient_clip (<code>float</code>, default: <code>1.0</code>): Gradient clipping norm</li> <li>save_path (<code>Optional[str]</code>): Path to save trained weights</li> <li>verbose (<code>bool</code>, default: <code>True</code>): Print training progress</li> <li>callback (<code>Optional[Callable]</code>): Callback function called with <code>(step, loss)</code> after each step</li> </ul> <p>Returns: Trained <code>TunedLens</code> instance</p> <p>Training Details (from paper): - Optimizer: SGD with Nesterov momentum (0.9) - Learning rate: 1.0 with linear decay over training steps - Gradient clipping: norm 1.0 - Loss: KL divergence between translator prediction and final output</p> <p>Example:</p> <pre><code># Load sample texts for training\ntexts = [\n    \"The capital of France is Paris.\",\n    \"Machine learning is a subset of artificial intelligence.\",\n    # ... more training texts\n]\n\n# Train tuned lens\ntuned_lens = model.train_tuned_lens(\n    dataset=texts,\n    num_steps=250,\n    save_path=\"my_tuned_lens.npz\",\n    verbose=True\n)\n</code></pre>"},{"location":"API/#method-load_tuned_lens","title":"Method: <code>load_tuned_lens</code>","text":"<p>Load a pre-trained tuned lens from a file.</p> <pre><code>model.load_tuned_lens(path: str) -&gt; TunedLens\n</code></pre> <p>Parameters:</p> <ul> <li>path (<code>str</code>): Path to the saved tuned lens weights (expects <code>.npz</code> and <code>.json</code> files)</li> </ul> <p>Returns: Loaded <code>TunedLens</code> instance</p> <p>Example:</p> <pre><code>tuned_lens = model.load_tuned_lens(\"tuned_lens_llama.npz\")\nresults = model.tuned_logit_lens(\"Hello world\", tuned_lens)\n</code></pre>"},{"location":"API/#method-activation_patching","title":"Method: <code>activation_patching</code>","text":"<p>Automated activation patching to identify important layers for a task.</p> <p>This helper method performs activation patching across all (or specified) layers to determine which components are critical for a specific task. It automates the boilerplate of running clean/corrupted inputs, patching activations, and measuring recovery.</p> <pre><code>model.activation_patching(\n    clean_text: str,\n    corrupted_text: str,\n    component: str = \"mlp\",\n    layers: Optional[List[int]] = None,\n    metric: str = \"l2\",\n    plot: bool = False,\n    figsize: tuple = (12, 8),\n    cmap: str = \"RdBu_r\"\n) -&gt; Dict[int, float]\n</code></pre> <p>Parameters:</p> <ul> <li>clean_text (<code>str</code>): Clean/correct input text</li> <li>corrupted_text (<code>str</code>): Corrupted input text (differs in the aspect you're studying)</li> <li>component (<code>str</code>, default: <code>\"mlp\"</code>): Component to patch. Options:</li> <li><code>\"mlp\"</code> - Full MLP block</li> <li><code>\"self_attn\"</code> - Full attention block</li> <li><code>\"mlp.gate_proj\"</code> - MLP gate projection</li> <li><code>\"mlp.up_proj\"</code> - MLP up projection</li> <li><code>\"mlp.down_proj\"</code> - MLP down projection</li> <li><code>\"self_attn.q_proj\"</code> - Query projection</li> <li><code>\"self_attn.k_proj\"</code> - Key projection</li> <li><code>\"self_attn.v_proj\"</code> - Value projection</li> <li><code>\"self_attn.o_proj\"</code> - Output projection</li> <li>layers (<code>Optional[List[int]]</code>, default: <code>None</code>): Specific layers to test (None = all layers)</li> <li>metric (<code>str</code>, default: <code>\"l2\"</code>): Distance metric. Options:</li> <li><code>\"l2\"</code>: Euclidean distance (default, with overflow protection)</li> <li><code>\"cosine\"</code>: Cosine distance (recommended for large vocabularies)</li> <li><code>\"mse\"</code>: Mean squared error (most stable for huge models &gt; 100k vocab)</li> </ul> <p>Recommendation:   - Vocab &lt; 50k: use <code>\"l2\"</code>   - Vocab 50k-100k: use <code>\"l2\"</code> or <code>\"cosine\"</code>   - Vocab &gt; 100k: use <code>\"mse\"</code> or <code>\"cosine\"</code> - plot (<code>bool</code>, default: <code>False</code>): If True, display a bar chart of recovery percentages - figsize (<code>tuple</code>, default: <code>(12, 8)</code>): Figure size for plot - cmap (<code>str</code>, default: <code>\"RdBu_r\"</code>): Colormap for plot (blue = positive, red = negative)</p> <p>Returns: Dict mapping <code>layer_idx</code> -&gt; recovery percentage</p> <p>Recovery Interpretation: - Positive % (e.g., +40%): Layer is important for the task - Negative % (e.g., -20%): Layer encodes the corruption - ~0%: Layer is not relevant to this task</p> <p>Example:</p> <pre><code># Find which MLP layers are important for factual recall\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    plot=True\n)\n\n# Analyze results\nsorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\nprint(\"\\nMost important layers:\")\nfor layer_idx, recovery in sorted_results[:3]:\n    print(f\"  Layer {layer_idx}: {recovery:.1f}% recovery\")\n\n# Output:\n# Layer  0: +43.1% recovery  \u2190 Very important!\n# Layer 15: +24.2% recovery  \u2190 Important\n# Layer  6: +17.6% recovery  \u2190 Somewhat important\n\n# Test specific layers only\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"self_attn\",\n    layers=[0, 5, 10, 15],\n    plot=False\n)\n\n# Test MLP sub-components\nresults = model.activation_patching(\n    clean_text=\"The cat sits on the mat\",\n    corrupted_text=\"The cat sit on the mat\",\n    component=\"mlp.gate_proj\",\n    layers=[3, 8, 12]\n)\n\n# For large vocabulary models (&gt; 100k tokens), use MSE metric\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    metric=\"mse\",  # Most stable for Qwen, GPT-4 scale models\n    plot=True\n)\n</code></pre> <p>Note: Plotting requires matplotlib: <code>pip install matplotlib</code></p> <p>Distance Metrics:</p> <p>The method supports three distance metrics for measuring output differences:</p> <ol> <li> <p>L2 (Euclidean) - Default, best for models with &lt; 50k vocab    <pre><code>d(a,b) = \u221a(\u03a3(a\u1d62-b\u1d62)\u00b2)\n</code></pre></p> </li> <li> <p>Cosine - Best for direction-based similarity, good for 50k-150k vocab    <pre><code>d(a,b) = 1 - (a\u00b7b)/(||a||\u00d7||b||)\n</code></pre></p> </li> <li> <p>MSE (Mean Squared Error) - Most stable for very large models (&gt; 100k vocab)    <pre><code>d(a,b) = (1/N)\u00d7\u03a3(a\u1d62-b\u1d62)\u00b2\n</code></pre></p> </li> </ol> <p>Example with large vocabulary model (Qwen: 151k tokens): <pre><code># Without correct metric - gets NaN\nresults = model.activation_patching(..., metric=\"l2\")  # \u274c May overflow\n\n# With correct metric - works perfectly\nresults = model.activation_patching(..., metric=\"mse\")  # \u2705 Stable\n</code></pre></p> <p>See Also: Activation Patching Guide for comprehensive coverage including metric selection and numerical details</p>"},{"location":"API/#method-named_modules","title":"Method: <code>named_modules</code>","text":"<p>Iterator over all modules in the wrapped model with their names.</p> <pre><code>model.named_modules() -&gt; Iterator[Tuple[str, nn.Module]]\n</code></pre> <p>Returns: Iterator yielding <code>(name, module)</code> tuples</p> <p>Example:</p> <pre><code>for name, module in model.named_modules():\n    print(f\"{name}: {type(module).__name__}\")\n</code></pre>"},{"location":"API/#attribute-layers","title":"Attribute: <code>layers</code>","text":"<p>Indexed access to model layers via <code>LayerListProxy</code>.</p> <p>Type: <code>LayerListProxy</code></p> <p>Example:</p> <pre><code># Access specific layer\nlayer_3 = model.layers[3]\n\n# Iterate over layers\nfor layer in model.layers:\n    print(layer)\n\n# Get number of layers\nnum_layers = len(model.layers)\n</code></pre>"},{"location":"API/#attribute-output","title":"Attribute: <code>output</code>","text":"<p>Access to model output within a trace context. Returns an <code>OutputProxy</code> that can be saved.</p> <p>Type: <code>OutputProxy</code></p> <p>Example:</p> <pre><code>with model.trace(input_text):\n    final_output = model.output.save()\n</code></pre>"},{"location":"API/#tracing","title":"Tracing","text":""},{"location":"API/#class-trace","title":"Class: <code>Trace</code>","text":"<p>Context manager for tracing model execution. Created by <code>InterpretableModel.trace()</code>.</p>"},{"location":"API/#attributes","title":"Attributes","text":"<ul> <li>output (<code>mx.array</code>): Model output after trace completes</li> <li>saved_values (<code>Dict[str, mx.array]</code>): Values saved with <code>.save()</code></li> <li>activations (<code>Dict[str, mx.array]</code>): All captured activations by module name</li> </ul>"},{"location":"API/#methods","title":"Methods","text":""},{"location":"API/#getname-str-optionalmxarray","title":"<code>get(name: str) -&gt; Optional[mx.array]</code>","text":"<p>Get a saved value by name.</p> <pre><code>with model.trace(input) as trace:\n    model.layers[3].output.save()\n\nactivation = trace.get(\"layers.3.output\")\n</code></pre>"},{"location":"API/#get_activationname-str-optionalmxarray","title":"<code>get_activation(name: str) -&gt; Optional[mx.array]</code>","text":"<p>Get an activation by module name.</p> <pre><code>with model.trace(input) as trace:\n    pass  # Activations captured automatically\n\n# Note: get_activation requires the full key (not normalized)\nattn_act = trace.get_activation(\"model.model.layers.3.self_attn\")\n</code></pre>"},{"location":"API/#class-outputproxy","title":"Class: <code>OutputProxy</code>","text":"<p>Wraps module outputs to provide <code>.save()</code> functionality.</p>"},{"location":"API/#method-save-any","title":"Method: <code>save() -&gt; Any</code>","text":"<p>Save the wrapped value to the current trace context and return the unwrapped value.</p> <p>Returns: The underlying value (usually <code>mx.array</code>)</p> <p>Example:</p> <pre><code>with model.trace(input):\n    # Save returns the actual array (use self_attn for mlx-lm models)\n    attn = model.layers[3].self_attn.output.save()\n    print(attn.shape)  # Can use immediately\n</code></pre>"},{"location":"API/#interventions","title":"Interventions","text":"<p>Intervention functions modify activations during forward passes.</p>"},{"location":"API/#module-mlxterpinterventions","title":"Module: <code>mlxterp.interventions</code>","text":"<p>Namespace containing pre-built intervention functions.</p> <pre><code>from mlxterp import interventions as iv\n\n# Use intervention functions\nwith model.trace(input, interventions={\"layers.3\": iv.scale(0.5)}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#function-zero_out","title":"Function: <code>zero_out</code>","text":"<p>Set activations to zero.</p> <pre><code>zero_out(x: mx.array) -&gt; mx.array\n</code></pre> <p>Example:</p> <pre><code>with model.trace(input, interventions={\"layers.4\": iv.zero_out}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#function-scale","title":"Function: <code>scale</code>","text":"<p>Multiply activations by a constant factor.</p> <pre><code>scale(factor: float) -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Parameters: - factor (<code>float</code>): Scaling factor</p> <p>Example:</p> <pre><code># Reduce by 50%\nwith model.trace(input, interventions={\"layers.3\": iv.scale(0.5)}):\n    output = model.output.save()\n\n# Amplify\nwith model.trace(input, interventions={\"layers.3\": iv.scale(2.0)}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#function-add_vector","title":"Function: <code>add_vector</code>","text":"<p>Add a steering vector to activations.</p> <pre><code>add_vector(vector: mx.array) -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Parameters: - vector (<code>mx.array</code>): Vector to add (must be broadcastable to activation shape)</p> <p>Example:</p> <pre><code>import mlx.core as mx\n\n# Create steering vector\nsteering = mx.random.normal((hidden_dim,))\n\nwith model.trace(input, interventions={\"layers.5\": iv.add_vector(steering)}):\n    steered_output = model.output.save()\n</code></pre>"},{"location":"API/#function-replace_with","title":"Function: <code>replace_with</code>","text":"<p>Replace activations with a fixed value.</p> <pre><code>replace_with(value: Union[mx.array, float]) -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Parameters: - value: Replacement value (array or scalar)</p> <p>Example:</p> <pre><code># Replace with zeros\nwith model.trace(input, interventions={\"layers.3\": iv.replace_with(0.0)}):\n    output = model.output.save()\n\n# Replace with custom array\ncustom = mx.ones((batch, seq_len, hidden_dim))\nwith model.trace(input, interventions={\"layers.3\": iv.replace_with(custom)}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#function-clamp","title":"Function: <code>clamp</code>","text":"<p>Clamp activation values to a range.</p> <pre><code>clamp(min_val: float = None, max_val: float = None) -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Parameters: - min_val (<code>Optional[float]</code>): Minimum value - max_val (<code>Optional[float]</code>): Maximum value</p> <p>Example:</p> <pre><code># Clamp to [-1, 1]\nwith model.trace(input, interventions={\"layers.3\": iv.clamp(-1.0, 1.0)}):\n    output = model.output.save()\n\n# Only maximum\nwith model.trace(input, interventions={\"layers.3\": iv.clamp(max_val=10.0)}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#function-noise","title":"Function: <code>noise</code>","text":"<p>Add Gaussian noise to activations.</p> <pre><code>noise(std: float = 0.1) -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Parameters: - std (<code>float</code>, default: <code>0.1</code>): Standard deviation of noise</p> <p>Example:</p> <pre><code>with model.trace(input, interventions={\"layers.3\": iv.noise(std=0.2)}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#class-interventioncomposer","title":"Class: <code>InterventionComposer</code>","text":"<p>Compose multiple interventions into a single function.</p>"},{"location":"API/#method-add","title":"Method: <code>add</code>","text":"<p>Add an intervention to the composition.</p> <pre><code>add(fn: Callable[[mx.array], mx.array]) -&gt; InterventionComposer\n</code></pre> <p>Returns: <code>self</code> for chaining</p>"},{"location":"API/#method-build","title":"Method: <code>build</code>","text":"<p>Build the composed intervention function.</p> <pre><code>build() -&gt; Callable[[mx.array], mx.array]\n</code></pre> <p>Returns: Composed intervention function</p> <p>Example:</p> <pre><code>from mlxterp import interventions as iv\n\n# Compose multiple interventions\ncombined = iv.compose() \\\n    .add(iv.scale(0.8)) \\\n    .add(iv.noise(std=0.1)) \\\n    .add(iv.clamp(-5.0, 5.0)) \\\n    .build()\n\nwith model.trace(input, interventions={\"layers.3\": combined}):\n    output = model.output.save()\n</code></pre>"},{"location":"API/#custom-interventions","title":"Custom Interventions","text":"<p>Create your own intervention functions:</p> <pre><code>import mlx.core as mx\n\ndef my_intervention(activation: mx.array) -&gt; mx.array:\n    \"\"\"Custom activation modification\"\"\"\n    # Your logic here\n    return mx.tanh(activation)\n\nwith model.trace(input, interventions={\"layers.3\": my_intervention}):\n    output = model.output.save()\n</code></pre> <p>Requirements: - Function signature: <code>(mx.array) -&gt; mx.array</code> - Must return array with same shape as input - Can use any MLX operations</p>"},{"location":"API/#utilities","title":"Utilities","text":""},{"location":"API/#function-get_activations","title":"Function: <code>get_activations</code>","text":"<p>Collect activations for specified layers and token positions.</p> <pre><code>get_activations(\n    model: InterpretableModel,\n    prompts: Union[str, List[str]],\n    layers: Optional[List[int]] = None,\n    positions: Union[int, List[int]] = -1\n) -&gt; Dict[str, mx.array]\n</code></pre> <p>Parameters:</p> <ul> <li>model: InterpretableModel instance</li> <li>prompts: Single prompt or list of prompts</li> <li>layers: Layer indices to collect (None = all layers)</li> <li>positions: Token position(s) to extract</li> <li><code>-1</code>: Last token</li> <li><code>0</code>: First token</li> <li><code>[0, -1]</code>: First and last tokens</li> </ul> <p>Returns: Dict mapping <code>\"layer_{i}\"</code> to activation arrays</p> <p>Shapes: - Single position: <code>(batch_size, hidden_dim)</code> - Multiple positions: <code>(batch_size, num_positions, hidden_dim)</code></p> <p>Example:</p> <pre><code>from mlxterp import get_activations\n\n# Single prompt, multiple layers\nacts = get_activations(model, \"Hello world\", layers=[3, 8, 12])\nprint(acts[\"layer_3\"].shape)  # (1, hidden_dim)\n\n# Batch prompts\nacts = get_activations(\n    model,\n    [\"Hello\", \"World\", \"Test\"],\n    layers=[5],\n    positions=-1\n)\nprint(acts[\"layer_5\"].shape)  # (3, hidden_dim)\n\n# Multiple positions\nacts = get_activations(\n    model,\n    \"Test prompt\",\n    layers=[3],\n    positions=[0, -1]  # First and last token\n)\nprint(acts[\"layer_3\"].shape)  # (1, 2, hidden_dim)\n</code></pre>"},{"location":"API/#function-batch_get_activations","title":"Function: <code>batch_get_activations</code>","text":"<p>Memory-efficient batch processing for large datasets.</p> <pre><code>batch_get_activations(\n    model: InterpretableModel,\n    prompts: List[str],\n    layers: Optional[List[int]] = None,\n    positions: Union[int, List[int]] = -1,\n    batch_size: int = 8\n) -&gt; Dict[str, mx.array]\n</code></pre> <p>Parameters:</p> <ul> <li>model: InterpretableModel instance</li> <li>prompts: List of prompts</li> <li>layers: Layer indices to collect</li> <li>positions: Token position(s) to extract</li> <li>batch_size: Number of prompts per batch</li> </ul> <p>Returns: Dict mapping <code>\"layer_{i}\"</code> to concatenated activation arrays</p> <p>Example:</p> <pre><code>from mlxterp import batch_get_activations\n\n# Process 1000 prompts efficiently\nlarge_dataset = [f\"Prompt {i}\" for i in range(1000)]\n\nacts = batch_get_activations(\n    model,\n    prompts=large_dataset,\n    layers=[3, 8, 12],\n    batch_size=32\n)\n\nprint(acts[\"layer_3\"].shape)  # (1000, hidden_dim)\n</code></pre>"},{"location":"API/#function-collect_activations","title":"Function: <code>collect_activations</code>","text":"<p>Direct activation collection with caching.</p> <pre><code>collect_activations(\n    model: InterpretableModel,\n    inputs: Any,\n    layers: Optional[List[str]] = None\n) -&gt; ActivationCache\n</code></pre> <p>Parameters:</p> <ul> <li>model: InterpretableModel instance</li> <li>inputs: Input data</li> <li>layers: List of layer names to cache (None = all)</li> </ul> <p>Returns: <code>ActivationCache</code> object</p> <p>Example:</p> <pre><code>from mlxterp import collect_activations\n\ncache = collect_activations(\n    model,\n    \"Test input\",\n    layers=[\"layers.3\", \"layers.8\"]\n)\n\n# Access cached activations\nact_3 = cache.get(\"layers.3\")\nprint(f\"Cached {len(cache)} activations\")\nprint(f\"Available keys: {cache.keys()}\")\n</code></pre>"},{"location":"API/#core-components","title":"Core Components","text":"<p>Advanced usage: Direct access to core components.</p>"},{"location":"API/#class-moduleproxy","title":"Class: <code>ModuleProxy</code>","text":"<p>Wraps <code>nn.Module</code> to intercept forward passes. Created automatically by <code>InterpretableModel</code>.</p> <p>Attributes: - <code>output</code>: OutputProxy for the module's output</p> <p>Example:</p> <pre><code># Access through InterpretableModel.layers\nproxy = model.layers[3]  # Returns ModuleProxy\nprint(type(proxy))  # ModuleProxy\n\nwith model.trace(input):\n    act = proxy.output.save()\n</code></pre>"},{"location":"API/#class-layerlistproxy","title":"Class: <code>LayerListProxy</code>","text":"<p>Provides indexed access to model layers.</p> <p>Methods: - <code>__getitem__(idx)</code>: Get layer at index - <code>__len__()</code>: Number of layers - <code>__iter__()</code>: Iterate over layers</p> <p>Example:</p> <pre><code># Created automatically\nlayers = model.layers\n\n# Access\nlayer_3 = layers[3]\n\n# Length\nprint(len(layers))  # 12\n\n# Iterate\nfor i, layer in enumerate(layers):\n    print(f\"Layer {i}: {layer}\")\n</code></pre>"},{"location":"API/#class-activationcache","title":"Class: <code>ActivationCache</code>","text":"<p>Storage for cached activations.</p> <p>Attributes: - activations (<code>Dict[str, mx.array]</code>): Activation storage - metadata (<code>Optional[Dict]</code>): Additional information</p> <p>Methods: - <code>get(name)</code>: Get activation by name - <code>keys()</code>: List all cached names - <code>__contains__(name)</code>: Check if activation exists - <code>__len__()</code>: Number of cached activations</p> <p>Example:</p> <pre><code>from mlxterp import collect_activations\n\ncache = collect_activations(model, input)\n\n# Access\nact = cache.get(\"layers.3\")\n\n# Check existence\nif \"layers.3\" in cache:\n    print(\"Found!\")\n\n# List all\nfor name in cache.keys():\n    print(name)\n</code></pre>"},{"location":"API/#class-moduleresolver","title":"Class: <code>ModuleResolver</code>","text":"<p>Generic module resolution for different MLX model architectures. Automatically finds embedding, final norm, and lm_head modules using fallback chains.</p> <p>Constructor:</p> <pre><code>ModuleResolver(\n    model: nn.Module,\n    embedding_path: Optional[str] = None,\n    norm_path: Optional[str] = None,\n    lm_head_path: Optional[str] = None\n)\n</code></pre> <p>Methods:</p> <ul> <li><code>get_embedding_layer()</code>: Get token embedding layer</li> <li><code>get_final_norm()</code>: Get final layer normalization</li> <li><code>get_lm_head()</code>: Get output projection layer</li> <li><code>get_output_projection()</code>: Get output projection with weight-tied detection. Returns <code>(module, path, is_weight_tied)</code></li> <li><code>clear_cache()</code>: Clear resolved module cache (call after modifying model structure)</li> </ul> <p>Fallback Chains:</p> Component Resolution Order Embedding <code>model.embed_tokens</code>, <code>model.model.embed_tokens</code>, <code>embed_tokens</code>, <code>tok_embeddings</code>, <code>wte</code> Final Norm <code>model.norm</code>, <code>model.model.norm</code>, <code>norm</code>, <code>ln_f</code>, <code>model.ln_f</code> LM Head <code>lm_head</code>, <code>model.lm_head</code>, <code>model.model.lm_head</code>, <code>output</code>, <code>head</code> (falls back to embedding if not found) <p>Example:</p> <pre><code>from mlxterp.core import ModuleResolver\n\n# Create resolver for a model\nresolver = ModuleResolver(model)\n\n# Get components\nembedding = resolver.get_embedding_layer()\nnorm = resolver.get_final_norm()\nproj, path, is_tied = resolver.get_output_projection()\n\nif is_tied:\n    print(\"Model uses weight-tied embedding for output\")\n\n# With custom paths\nresolver = ModuleResolver(\n    model,\n    embedding_path=\"my_embed\",\n    norm_path=\"my_norm\"\n)\n\n# Cache invalidation (after modifying model structure)\nresolver.clear_cache()  # Force re-resolution on next access\n</code></pre>"},{"location":"API/#class-tunedlens","title":"Class: <code>TunedLens</code>","text":"<p>Learned affine translators for each layer, implementing the Tuned Lens technique from Belrose et al. (2023).</p> <p>The tuned lens uses layer-specific affine transformations (Wx + b) to map hidden states from each layer into a space where the final output projection can make accurate predictions. This corrects for coordinate system mismatches between layers.</p> <pre><code>from mlxterp import TunedLens\n\ntuned_lens = TunedLens(num_layers=32, hidden_dim=4096)\n</code></pre> <p>Parameters:</p> <ul> <li>num_layers (<code>int</code>): Number of transformer layers in the model</li> <li>hidden_dim (<code>int</code>): Dimension of hidden states</li> </ul> <p>Attributes:</p> <ul> <li><code>num_layers</code>: Number of layers</li> <li><code>hidden_dim</code>: Hidden dimension</li> <li><code>translators</code>: List of linear layers, one per transformer layer</li> </ul> <p>Methods:</p> Method Description <code>__call__(h, layer_idx)</code> Apply translator for a specific layer <code>save(path)</code> Save weights and config to files (.npz and .json) <code>load(path)</code> Load tuned lens from saved files (classmethod) <p>Example:</p> <pre><code>from mlxterp import TunedLens\n\n# Create tuned lens\ntuned_lens = TunedLens(num_layers=32, hidden_dim=4096)\n\n# Apply to hidden state from layer 10\ntranslated = tuned_lens(hidden_state, layer_idx=10)\n\n# Save and load\ntuned_lens.save(\"my_tuned_lens\")\nloaded = TunedLens.load(\"my_tuned_lens\")\n</code></pre> <p>Reference: Belrose et al., \"Eliciting Latent Predictions from Transformers with the Tuned Lens\" (https://arxiv.org/abs/2303.08112)</p>"},{"location":"API/#function-normalize_layer_key","title":"Function: <code>normalize_layer_key</code>","text":"<p>Normalize activation keys by removing model prefixes.</p> <pre><code>normalize_layer_key(key: str) -&gt; str\n</code></pre> <p>Example:</p> <pre><code>from mlxterp.core import normalize_layer_key\n\nnormalize_layer_key(\"model.model.layers.0\")  # \"layers.0\"\nnormalize_layer_key(\"model.layers.5.self_attn\")  # \"layers.5.self_attn\"\n</code></pre>"},{"location":"API/#function-find_layer_key_pattern","title":"Function: <code>find_layer_key_pattern</code>","text":"<p>Find the correct activation key pattern for a layer index.</p> <pre><code>find_layer_key_pattern(\n    activations: dict,\n    layer_idx: int,\n    component: Optional[str] = None\n) -&gt; Optional[str]\n</code></pre> <p>Example:</p> <pre><code>from mlxterp.core import find_layer_key_pattern\n\n# Find layer 5's key in activations dict\nkey = find_layer_key_pattern(trace.activations, 5)\n# Returns \"model.model.layers.5\" or \"layers.5\" etc.\n\n# Find specific component\nkey = find_layer_key_pattern(trace.activations, 5, \"self_attn\")\n</code></pre>"},{"location":"API/#type-annotations","title":"Type Annotations","text":"<p>Common types used in mlxterp:</p> <pre><code>from typing import Union, List, Dict, Callable, Optional, Any\nimport mlx.core as mx\nimport mlx.nn as nn\n\n# Input types\nInputType = Union[str, List[str], mx.array, List[int]]\n\n# Intervention function type\nInterventionFn = Callable[[mx.array], mx.array]\n\n# Interventions dict\nInterventionsDict = Dict[str, InterventionFn]\n\n# Model type\nModelType = Union[nn.Module, str]\n</code></pre>"},{"location":"API/#error-handling","title":"Error Handling","text":""},{"location":"API/#common-exceptions","title":"Common Exceptions","text":""},{"location":"API/#valueerror","title":"<code>ValueError</code>","text":"<p>Raised when: - String input provided without tokenizer - Invalid input format - Model cannot be loaded from string</p> <pre><code># Will raise ValueError\nmodel = InterpretableModel(base_model)  # No tokenizer\nwith model.trace(\"text input\"):  # Needs tokenizer!\n    pass\n</code></pre> <p>Solution: Provide tokenizer</p> <pre><code>model = InterpretableModel(base_model, tokenizer=my_tokenizer)\n</code></pre>"},{"location":"API/#attributeerror","title":"<code>AttributeError</code>","text":"<p>Raised when: - Accessing non-existent module attribute - Layer attribute doesn't exist</p> <pre><code># Will raise AttributeError\nmodel = InterpretableModel(custom_model, layer_attr=\"transformer\")\n# If custom_model doesn't have 'transformer' attribute\n</code></pre> <p>Solution: Specify correct layer attribute</p> <pre><code>model = InterpretableModel(custom_model, layer_attr=\"layers\")\n</code></pre>"},{"location":"API/#best-practices","title":"Best Practices","text":""},{"location":"API/#1-always-use-context-managers","title":"1. Always Use Context Managers","text":"<pre><code># \u2705 Good\nwith model.trace(input):\n    act = model.layers[3].output.save()\n\n# \u274c Avoid\ntrace = model.trace(input)\n# Missing context manager!\n</code></pre>"},{"location":"API/#2-save-early-access-later","title":"2. Save Early, Access Later","text":"<pre><code># \u2705 Good\nwith model.trace(input) as t:\n    model.layers[3].output.save()\n\n# Access after trace completes\nact = t.get(\"layers.3.output\")\n\n# \u274c Avoid trying to access during trace\nwith model.trace(input) as t:\n    act = t.get(\"layers.3.output\")  # Not saved yet!\n</code></pre>"},{"location":"API/#3-use-utility-functions-for-common-tasks","title":"3. Use Utility Functions for Common Tasks","text":"<pre><code># \u2705 Good - Use get_activations\nfrom mlxterp import get_activations\nacts = get_activations(model, prompts, layers=[3, 8])\n\n# \u274c Avoid manual loops\nacts = {}\nfor layer_idx in [3, 8]:\n    with model.trace(prompts):\n        acts[layer_idx] = model.layers[layer_idx].output.save()\n</code></pre>"},{"location":"API/#4-batch-large-datasets","title":"4. Batch Large Datasets","text":"<pre><code># \u2705 Good - Use batching\nfrom mlxterp import batch_get_activations\nacts = batch_get_activations(model, large_list, batch_size=32)\n\n# \u274c Avoid loading everything at once\nwith model.trace(large_list):  # May run out of memory\n    acts = model.layers[3].output.save()\n</code></pre>"},{"location":"API/#version-history","title":"Version History","text":""},{"location":"API/#010-current","title":"0.1.0 (Current)","text":"<ul> <li>Initial release</li> <li>Core tracing functionality</li> <li>Intervention system</li> <li>Basic utility functions</li> <li>Support for any MLX model</li> </ul>"},{"location":"JUPYTER_GUIDE/","title":"Using mlxterp in Jupyter Notebooks","text":""},{"location":"JUPYTER_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"JUPYTER_GUIDE/#1-installation","title":"1. Installation","text":"<p>In your notebook, first install mlx-lm if you haven't already:</p> <pre><code># Run this once\n!uv add mlx-lm\n</code></pre>"},{"location":"JUPYTER_GUIDE/#2-import","title":"2. Import","text":"<pre><code>from mlxterp import InterpretableModel, interventions as iv\nfrom mlx_lm import load\nimport mlx.core as mx\nimport mlx.nn as nn\n</code></pre>"},{"location":"JUPYTER_GUIDE/#working-with-real-models","title":"Working with Real Models","text":"<p>mlxterp now fully supports real mlx-lm models with comprehensive activation capture!</p> <pre><code># Load a real model\nbase_model, tokenizer = load('mlx-community/Llama-3.2-1B-Instruct-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\n# Run a forward pass and capture ALL activations\nwith model.trace(\"Hello, how are you?\") as trace:\n    pass  # Automatically captures ~196 activations!\n\n# Access any captured activation by name\nprint(f\"Captured {len(trace.activations)} activations\")\n\n# Access specific modules\nlayer_5_attn = trace.activations['model.model.layers.5.self_attn']\nq_proj_3 = trace.activations['model.model.layers.3.self_attn.q_proj']\nmlp_7 = trace.activations['model.model.layers.7.mlp']\noutput = trace.activations['__model_output__']\n\nprint(f\"Layer 5 attention: {layer_5_attn.shape}\")\nprint(f\"Layer 3 Q projection: {q_proj_3.shape}\")\nprint(f\"Output: {output.shape}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#available-activations","title":"Available Activations","text":"<p>Real models capture fine-grained activations including:</p> <ul> <li>Embeddings: <code>model.model.embed_tokens</code></li> <li>Layer outputs: <code>model.model.layers.{i}</code></li> <li>Attention components:</li> <li><code>model.model.layers.{i}.self_attn</code></li> <li><code>model.model.layers.{i}.self_attn.q_proj</code></li> <li><code>model.model.layers.{i}.self_attn.k_proj</code></li> <li><code>model.model.layers.{i}.self_attn.v_proj</code></li> <li><code>model.model.layers.{i}.self_attn.o_proj</code></li> <li><code>model.model.layers.{i}.self_attn.rope</code></li> <li>MLP components:</li> <li><code>model.model.layers.{i}.mlp</code></li> <li><code>model.model.layers.{i}.mlp.gate_proj</code></li> <li><code>model.model.layers.{i}.mlp.up_proj</code></li> <li><code>model.model.layers.{i}.mlp.down_proj</code></li> <li>Layer norms:</li> <li><code>model.model.layers.{i}.input_layernorm</code></li> <li><code>model.model.layers.{i}.post_attention_layernorm</code></li> <li>Final output: <code>__model_output__</code></li> </ul>"},{"location":"JUPYTER_GUIDE/#exploring-captured-activations","title":"Exploring Captured Activations","text":"<pre><code># See what was captured\nwith model.trace(\"Hello world\") as trace:\n    pass\n\n# List all captured activation names\nprint(\"Captured activations:\")\nfor name in list(trace.activations.keys())[:20]:\n    act = trace.activations[name]\n    print(f\"  {name}: {act.shape}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#interventions-on-real-models","title":"Interventions on Real Models","text":"<p>You can intervene on any captured module:</p> <pre><code># Baseline\nwith model.trace(\"The capital of France is\") as baseline:\n    baseline_output = baseline.activations['__model_output__']\n\n# Scale down attention in layer 5\nwith model.trace(\"The capital of France is\",\n                 interventions={'model.model.layers.5.self_attn': iv.scale(0.5)}) as modified:\n    modified_output = modified.activations['__model_output__']\n\n# Compare\ndiff = mx.linalg.norm(baseline_output - modified_output)\nprint(f\"Output difference: {diff:.4f}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#available-interventions","title":"Available Interventions","text":"<pre><code>from mlxterp import interventions as iv\n\n# Scale activations\niv.scale(0.5)\n\n# Zero out\niv.zero_out\n\n# Add steering vector\nsteering = mx.random.normal((2048,))\niv.add_vector(steering)\n\n# Replace with value\niv.replace_with(1.0)\n\n# Clamp to range\niv.clamp(-1.0, 1.0)\n\n# Add noise\niv.noise(std=0.1)\n\n# Compose multiple\ncombined = iv.compose() \\\n    .add(iv.scale(0.8)) \\\n    .add(iv.noise(0.1)) \\\n    .build()\n</code></pre>"},{"location":"JUPYTER_GUIDE/#custom-models","title":"Custom Models","text":"<p>Simple custom models also work perfectly:</p> <pre><code># Define a simple transformer\nclass MyTransformer(nn.Module):\n    def __init__(self, hidden_dim=64, num_layers=4):\n        super().__init__()\n        self.layers = [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = nn.relu(x)\n        return x\n\n# Wrap it\nmodel = InterpretableModel(MyTransformer())\n\n# Trace it\ninput_data = mx.random.normal((1, 64))\nwith model.trace(input_data) as trace:\n    pass\n\n# Access activations\nlayer_0 = trace.activations['model.layers.0']\nprint(f\"Layer 0 shape: {layer_0.shape}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#complete-example-for-mechanistic-interpretability","title":"Complete Example for Mechanistic Interpretability","text":"<pre><code>from mlxterp import InterpretableModel, interventions as iv\nfrom mlx_lm import load\nimport mlx.core as mx\n\n# 1. Load model\nbase_model, tokenizer = load('mlx-community/Llama-3.2-1B-Instruct-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\n# 2. Run clean forward pass\nprompt = \"The Eiffel Tower is located in\"\nwith model.trace(prompt) as clean:\n    clean_attn_8 = clean.activations['model.model.layers.8.self_attn']\n    clean_output = clean.activations['__model_output__']\n\n# 3. Run with corrupted input\ncorrupted_prompt = \"The Big Ben is located in\"\nwith model.trace(corrupted_prompt) as corrupted:\n    corrupted_attn_8 = corrupted.activations['model.model.layers.8.self_attn']\n\n# 4. Activation patching: restore clean attention in layer 8\nfrom mlxterp import interventions as iv\n\n# Evaluate clean activation first\nmx.eval(clean_attn_8)\n\nwith model.trace(corrupted_prompt,\n                 interventions={'layers.8.self_attn': iv.replace_with(clean_attn_8)}) as patched:\n    patched_output = patched.activations['__model_output__']\n\n# 5. Analyze results\nclean_logits = clean_output[0, -1, :]\npatched_logits = patched_output[0, -1, :]\n\nprint(\"Top predictions (clean):\", mx.argmax(clean_logits))\nprint(\"Top predictions (patched):\", mx.argmax(patched_logits))\nprint(f\"Logit difference: {mx.linalg.norm(clean_logits - patched_logits):.4f}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#common-patterns","title":"Common Patterns","text":""},{"location":"JUPYTER_GUIDE/#activation-patching","title":"Activation Patching","text":"<p>Replace activations from one run into another:</p> <pre><code># Get clean activation\nwith model.trace(\"clean input\") as clean:\n    clean_mlp = clean.activations['model.model.layers.8.mlp']\n\n# Patch into corrupted run\nwith model.trace(\"corrupted input\",\n                 interventions={'model.model.layers.8.mlp': lambda x: clean_mlp}) as patched:\n    result = patched.activations['__model_output__']\n</code></pre>"},{"location":"JUPYTER_GUIDE/#steering-vectors","title":"Steering Vectors","text":"<p>Add directional vectors to guide model behavior:</p> <pre><code># Compute steering vector (difference between contrasting examples)\nwith model.trace(\"I love this\") as pos:\n    pos_h = pos.activations['model.model.layers.10']\n\nwith model.trace(\"I hate this\") as neg:\n    neg_h = neg.activations['model.model.layers.10']\n\nsteering_vector = pos_h - neg_h\n\n# Apply steering\nwith model.trace(\"This movie is\",\n                 interventions={'model.model.layers.10': iv.add_vector(steering_vector)}) as steered:\n    steered_output = steered.activations['__model_output__']\n</code></pre>"},{"location":"JUPYTER_GUIDE/#probing-multiple-layers","title":"Probing Multiple Layers","text":"<p>Collect activations from multiple layers for analysis:</p> <pre><code>with model.trace(\"Input text\") as trace:\n    pass\n\n# Extract layer representations\nlayer_activations = []\nfor i in range(16):  # For Llama-3.2-1B\n    layer_name = f'model.model.layers.{i}'\n    if layer_name in trace.activations:\n        layer_activations.append(trace.activations[layer_name])\n\n# Analyze representations\nfor i, act in enumerate(layer_activations):\n    norm = mx.linalg.norm(act)\n    print(f\"Layer {i} norm: {norm:.4f}\")\n</code></pre>"},{"location":"JUPYTER_GUIDE/#tips","title":"Tips","text":"<ol> <li> <p>Check what's available: Always run once and inspect <code>trace.activations.keys()</code> to see exactly what was captured</p> </li> <li> <p>Use specific module names: Target interventions precisely (e.g., <code>model.model.layers.5.self_attn.q_proj</code> instead of just <code>layers.5</code>)</p> </li> <li> <p>Memory management: Traces store all activations which remain available after the context:    <pre><code>with model.trace(input) as trace:\n    act = trace.activations['some.module']\n# trace.activations remains available after exiting context\n# Delete trace when done to free memory: del trace\n</code></pre></p> </li> <li> <p>Model downloads: First run downloads ~1-2GB model, cached at <code>~/.cache/huggingface/</code></p> </li> </ol>"},{"location":"JUPYTER_GUIDE/#current-status","title":"Current Status","text":"<p>\u2705 Fully Working: - Real mlx-lm models (Llama, Mistral, etc.) - Custom simple models - Fine-grained activation capture (~196 per forward pass) - All intervention types - Activation patching - Steering vectors</p> <p>\ud83c\udfaf Best Practices: - Use <code>trace.activations</code> dict for direct access by name - Target specific submodules for precise interventions - Cache clean activations when doing multiple patching experiments</p>"},{"location":"JUPYTER_GUIDE/#troubleshooting","title":"Troubleshooting","text":"<p>Issue: Model download fails Fix: Check internet connection, models download on first use</p> <p>Issue: <code>KeyError</code> when accessing activation Fix: Print <code>trace.activations.keys()</code> to see available names</p> <p>Issue: Out of memory Fix: Use smaller batch sizes or clear traces after use</p> <p>Issue: Interventions don't affect output Fix: Verify intervention targets correct module name, check effect magnitude</p>"},{"location":"QUICKSTART/","title":"mlxterp Quick Start Guide","text":"<p>Get started with mlxterp in 5 minutes!</p>"},{"location":"QUICKSTART/#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Install in development mode\npip install -e .\n\n# Or install with optional dependencies\npip install -e \".[viz]\"\n</code></pre>"},{"location":"QUICKSTART/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS with Apple Silicon (M1/M2/M3)</li> <li>Python 3.9+</li> <li>MLX framework (<code>pip install mlx</code>)</li> </ul>"},{"location":"QUICKSTART/#your-first-script","title":"Your First Script","text":"<p>Create a file <code>test_mlxterp.py</code>:</p> <pre><code>from mlxterp import InterpretableModel\nimport mlx.core as mx\nimport mlx.nn as nn\n\n# 1. Create a simple model\nclass SimpleTransformer(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super().__init__()\n        self.layers = [\n            nn.Linear(hidden_dim, hidden_dim)\n            for _ in range(4)\n        ]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = nn.relu(x)\n        return x\n\n# 2. Wrap with InterpretableModel\nbase_model = SimpleTransformer()\nmodel = InterpretableModel(base_model)\n\n# 3. Create input\ninput_data = mx.random.normal((1, 10, 64))  # (batch, seq, hidden)\n\n# 4. Trace execution\nwith model.trace(input_data):\n    layer_0 = model.layers[0].output.save()\n    layer_2 = model.layers[2].output.save()\n\nprint(f\"Layer 0 shape: {layer_0.shape}\")\nprint(f\"Layer 2 shape: {layer_2.shape}\")\n</code></pre> <p>Run it: <pre><code>python test_mlxterp.py\n</code></pre></p>"},{"location":"QUICKSTART/#common-use-cases","title":"Common Use Cases","text":""},{"location":"QUICKSTART/#1-load-a-real-model","title":"1. Load a Real Model","text":"<pre><code>from mlxterp import InterpretableModel\n\n# Automatically loads model and tokenizer\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Use with text (use self_attn for mlx-lm models)\nwith model.trace(\"Hello world\"):\n    attn = model.layers[5].self_attn.output.save()\n</code></pre>"},{"location":"QUICKSTART/#2-work-with-tokenizers","title":"2. Work with Tokenizers","text":"<pre><code># Encode text to tokens\ntokens = model.encode(\"Hello world\")\nprint(tokens)  # [128000, 9906, 1917]\n\n# Decode tokens to text\ntext = model.decode(tokens)\nprint(text)  # \"Hello world\"\n\n# Analyze individual tokens\nfor i, token_id in enumerate(tokens):\n    token_str = model.token_to_str(token_id)\n    print(f\"Token {i}: '{token_str}'\")\n\n# Get vocabulary size\nprint(f\"Vocab size: {model.vocab_size}\")\n</code></pre>"},{"location":"QUICKSTART/#3-collect-activations","title":"3. Collect Activations","text":"<pre><code>from mlxterp import get_activations\n\nacts = get_activations(\n    model,\n    prompts=[\"Hello\", \"World\"],\n    layers=[3, 8, 12],\n    positions=-1  # last token\n)\n\nprint(acts[\"layer_3\"].shape)  # (2, hidden_dim)\n</code></pre>"},{"location":"QUICKSTART/#4-apply-interventions","title":"4. Apply Interventions","text":"<pre><code>from mlxterp import interventions as iv\n\n# Scale down layer 4\nwith model.trace(\"Test\", interventions={\"layers.4\": iv.scale(0.5)}):\n    output = model.output.save()\n\n# Add steering vector\nsteering = mx.random.normal((hidden_dim,))\nwith model.trace(\"Test\", interventions={\"layers.5\": iv.add_vector(steering)}):\n    output = model.output.save()\n</code></pre>"},{"location":"QUICKSTART/#5-logit-lens","title":"5. Logit Lens","text":"<p>See what each layer predicts at each token position:</p> <pre><code># Analyze predictions across layers\ntext = \"The capital of France is\"\nresults = model.logit_lens(text, layers=[0, 5, 10, 15])\n\n# See how prediction at LAST position evolves through layers\nfor layer_idx in [0, 5, 10, 15]:\n    last_pos_pred = results[layer_idx][-1][0][2]\n    print(f\"Layer {layer_idx}: '{last_pos_pred}'\")\n\n# Output:\n# Layer  0: ' the'\n# Layer  5: ' a'\n# Layer 10: ' Paris'\n# Layer 15: ' Paris'\n\n# Visualize with heatmap showing ALL positions\nresults = model.logit_lens(\n    \"The Eiffel Tower is located in the city of\",\n    plot=True,\n    max_display_tokens=15\n)\n# Shows: Input tokens (x-axis) \u00d7 Layers (y-axis) with predicted tokens\n</code></pre>"},{"location":"QUICKSTART/#6-activation-patching","title":"6. Activation Patching","text":"<pre><code># Get clean activation\nwith model.trace(\"The capital of France is\"):\n    clean_act = model.layers[8].output.save()\n\n# Patch into different input\nwith model.trace(\"The capital of Spain is\",\n                interventions={\"layers.8\": lambda x: clean_act}):\n    patched = model.output.save()\n</code></pre>"},{"location":"QUICKSTART/#whats-next","title":"What's Next?","text":"<ol> <li>Read the full README for detailed examples</li> <li>Check the API Reference for complete API documentation</li> <li>Explore the examples for more use cases</li> <li>Read the Architecture Guide to understand the design</li> </ol>"},{"location":"QUICKSTART/#troubleshooting","title":"Troubleshooting","text":""},{"location":"QUICKSTART/#string-input-provided-but-no-tokenizer-available","title":"\"String input provided but no tokenizer available\"","text":"<p>Problem: You're passing text but no tokenizer was loaded.</p> <p>Solution: Pass a tokenizer or use token arrays: <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"model-name\")\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n</code></pre></p>"},{"location":"QUICKSTART/#model-does-not-have-layers-attribute","title":"\"Model does not have 'layers' attribute\"","text":"<p>Problem: Your model uses a different attribute name.</p> <p>Solution: Specify the correct attribute: <pre><code># For GPT-2\nmodel = InterpretableModel(gpt2_model, layer_attr=\"h\")\n\n# For custom models\nmodel = InterpretableModel(custom_model, layer_attr=\"transformer.blocks\")\n</code></pre></p>"},{"location":"QUICKSTART/#import-error","title":"Import Error","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'mlxterp'</code></p> <p>Solution: Install the package: <pre><code>cd mlxterp\npip install -e .\n</code></pre></p>"},{"location":"QUICKSTART/#community-support","title":"Community &amp; Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Examples: See <code>examples/</code> directory</li> </ul>"},{"location":"QUICKSTART/#next-steps","title":"Next Steps","text":"<p>Try these examples in order:</p> <ol> <li>Basic tracing (you just did this!)</li> <li>examples/basic_usage.py - More comprehensive examples</li> <li>Explore your own models</li> <li>Experiment with interventions</li> <li>Analyze activation patterns</li> </ol>"},{"location":"TUTORIAL_PAPERS_PLAN/","title":"Tutorial Papers Planning Document","text":"<p>This document outlines a plan for creating tutorials that reimplement key mechanistic interpretability papers using mlxterp. The goal is to demonstrate the library's capabilities through concrete, educational examples.</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#library-capabilities-summary","title":"Library Capabilities Summary","text":"<p>Before selecting papers, here's what mlxterp currently supports:</p> Capability Implementation Status Activation Tracing <code>with model.trace(\"text\") as trace:</code> \u2705 Full Fine-grained Access ~196 activations (Q/K/V, MLP, attention, etc.) \u2705 Full Logit Lens <code>model.logit_lens()</code> \u2705 Built-in Activation Patching <code>model.activation_patching()</code> \u2705 Built-in Token Predictions <code>model.get_token_predictions()</code> \u2705 Built-in Steering Vectors <code>interventions.add_vector()</code> \u2705 Full Zero Ablation <code>interventions.zero_out</code> \u2705 Full Scaling <code>interventions.scale()</code> \u2705 Full Activation Replacement <code>interventions.replace_with()</code> \u2705 Full Noise Injection <code>interventions.noise()</code> \u2705 Full SAE Training <code>model.train_sae()</code> \u2705 Full SAE Feature Analysis <code>get_top_features_for_text()</code> \u2705 Full"},{"location":"TUTORIAL_PAPERS_PLAN/#recommended-papers-for-tutorials","title":"Recommended Papers for Tutorials","text":""},{"location":"TUTORIAL_PAPERS_PLAN/#paper-1-the-logit-lens-beginner","title":"Paper 1: The Logit Lens (Beginner)","text":"<p>Original Work: interpreting GPT: the logit lens by nostalgebraist (2020)</p> <p>Why This Paper: - Simplicity: Conceptually straightforward - just project intermediate layers through the unembedding matrix - Already Implemented: mlxterp has <code>model.logit_lens()</code> built-in, making verification easy - Educational Value: Introduces the key concept that transformers iteratively refine predictions - Visual: Produces intuitive heatmap visualizations</p> <p>Core Concept: The logit lens reveals that intermediate hidden states, when projected through the output embedding matrix, produce sensible token distributions that progressively refine toward the final prediction.</p> <p>mlxterp Implementation: <pre><code>from mlxterp import InterpretableModel\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Built-in logit lens with visualization\nresults = model.logit_lens(\n    \"The capital of France is\",\n    layers=[0, 4, 8, 12, 15],\n    plot=True\n)\n\n# Show how predictions evolve across layers\nfor layer_idx in [0, 4, 8, 12, 15]:\n    top_pred = results[layer_idx][-1][0][2]  # Top token at last position\n    print(f\"Layer {layer_idx}: '{top_pred}'\")\n</code></pre></p> <p>Tutorial Structure: 1. Introduction to the residual stream concept 2. Manual implementation using <code>get_token_predictions()</code> 3. Comparison with built-in <code>logit_lens()</code> 4. Visualization and interpretation 5. Exercises: Try different prompts, observe when predictions \"crystallize\"</p> <p>Estimated Complexity: \u2b50 (Beginner)</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#paper-1b-the-tuned-lens-beginner-intermediate","title":"Paper 1b: The Tuned Lens (Beginner-Intermediate)","text":"<p>Original Work: Eliciting Latent Predictions from Transformers with the Tuned Lens by Belrose et al. (NeurIPS 2023)</p> <p>GitHub: https://github.com/AlignmentResearch/tuned-lens</p> <p>Why This Paper: - Direct Extension: Natural follow-up to the Logit Lens tutorial - Addresses Limitations: Fixes the \"brittleness\" of raw logit lens - Trainable Component: Introduces the concept of learned probes for interpretability - Practical Improvement: More accurate layer-wise predictions</p> <p>Core Concept: The raw logit lens assumes all layers use the same \"coordinate system\" as the final layer. In reality, representations are rotated, shifted, and stretched between layers. The tuned lens learns a small affine transformation (Wx + b) for each layer that maps hidden states to the final layer's coordinate system before unembedding.</p> <p>Key Insight: <pre><code>Logit Lens:  prediction = unembed(layer_norm(h_layer))           # Assumes same coords\nTuned Lens:  prediction = unembed(layer_norm(W_layer @ h + b))   # Learns correction\n</code></pre></p> <p>The affine \"translator\" for each layer is trained to minimize KL divergence between its prediction and the model's final output distribution.</p> <p>mlxterp Implementation:</p> <pre><code>import mlx.core as mx\nimport mlx.nn as nn\nfrom mlxterp import InterpretableModel\n\nclass TunedLens(nn.Module):\n    \"\"\"Tuned Lens: learned affine probes for each layer.\"\"\"\n\n    def __init__(self, num_layers: int, hidden_dim: int):\n        super().__init__()\n        # One affine translator per layer: h -&gt; W @ h + b\n        self.translators = [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)]\n\n        # Initialize close to identity for stable training\n        for translator in self.translators:\n            translator.weight = mx.eye(hidden_dim)\n            translator.bias = mx.zeros((hidden_dim,))\n\n    def __call__(self, hidden_state: mx.array, layer_idx: int) -&gt; mx.array:\n        \"\"\"Apply the tuned lens translator for a specific layer.\"\"\"\n        return self.translators[layer_idx](hidden_state)\n\n\ndef train_tuned_lens(\n    model: InterpretableModel,\n    texts: list,\n    num_steps: int = 250,\n    learning_rate: float = 1.0,\n    chunk_size: int = 2048\n) -&gt; TunedLens:\n    \"\"\"\n    Train tuned lens translators on a dataset.\n\n    Each translator learns to minimize KL divergence between:\n    - Its prediction: softmax(unembed(layer_norm(translator(h_layer))))\n    - Target: softmax(unembed(layer_norm(h_final)))\n    \"\"\"\n    num_layers = len(model.layers)\n    hidden_dim = model.model.model.embed_tokens.weight.shape[1]\n\n    tuned_lens = TunedLens(num_layers, hidden_dim)\n\n    # Get final layer norm for projection\n    final_norm = model.model.model.norm\n\n    def loss_fn(tuned_lens, hidden_states, final_logits):\n        \"\"\"KL divergence between tuned predictions and final output.\"\"\"\n        total_loss = 0.0\n\n        for layer_idx in range(num_layers):\n            h = hidden_states[layer_idx]\n\n            # Apply translator and layer norm\n            h_translated = tuned_lens(h, layer_idx)\n            h_normed = final_norm(h_translated)\n\n            # Get logits via unembedding\n            translated_logits = model.get_token_predictions(\n                h_normed[0, -1, :],\n                top_k=model.vocab_size,\n                return_scores=True\n            )\n\n            # Compute KL divergence\n            # KL(final || translated) = sum(final * log(final / translated))\n            final_probs = mx.softmax(final_logits[0, -1, :])\n            translated_probs = mx.softmax(translated_logits)\n            kl = mx.sum(final_probs * (mx.log(final_probs + 1e-10) - mx.log(translated_probs + 1e-10)))\n\n            total_loss += kl\n\n        return total_loss / num_layers\n\n    # Training loop with SGD + Nesterov momentum\n    optimizer = nn.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n\n    for step in range(num_steps):\n        text = texts[step % len(texts)]\n\n        with model.trace(text) as trace:\n            final_logits = trace.activations['__model_output__']\n            hidden_states = [\n                trace.activations[f'model.model.layers.{i}']\n                for i in range(num_layers)\n            ]\n\n        # Compute loss and gradients\n        loss, grads = mx.value_and_grad(loss_fn)(tuned_lens, hidden_states, final_logits)\n        optimizer.update(tuned_lens, grads)\n\n        if step % 50 == 0:\n            print(f\"Step {step}: loss = {loss.item():.4f}\")\n\n    return tuned_lens\n\n\ndef tuned_logit_lens(\n    model: InterpretableModel,\n    tuned_lens: TunedLens,\n    text: str,\n    layers: list = None,\n    top_k: int = 5\n) -&gt; dict:\n    \"\"\"\n    Apply tuned lens to get improved intermediate predictions.\n\n    Returns dict mapping layer_idx -&gt; list of (token_id, score, token_str) tuples\n    \"\"\"\n    final_norm = model.model.model.norm\n\n    with model.trace(text) as trace:\n        pass\n\n    if layers is None:\n        layers = list(range(len(model.layers)))\n\n    results = {}\n    for layer_idx in layers:\n        h = trace.activations[f'model.model.layers.{layer_idx}']\n\n        # Apply tuned lens translator\n        h_translated = tuned_lens(h, layer_idx)\n\n        # Apply final layer norm\n        h_normed = final_norm(h_translated[0, -1, :])\n\n        # Get predictions\n        predictions = model.get_token_predictions(h_normed, top_k=top_k, return_scores=True)\n        predictions_with_str = [\n            (token_id, score, model.token_to_str(token_id))\n            for token_id, score in predictions\n        ]\n        results[layer_idx] = predictions_with_str\n\n    return results\n\n\n# Example usage\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Train tuned lens (or load pre-trained)\ntexts = [\"Sample text 1...\", \"Sample text 2...\", ...]  # Training data\ntuned_lens = train_tuned_lens(model, texts, num_steps=250)\n\n# Compare raw vs. tuned logit lens\ntext = \"The capital of France is\"\n\nraw_results = model.logit_lens(text, layers=[0, 5, 10, 15])\ntuned_results = tuned_logit_lens(model, tuned_lens, text, layers=[0, 5, 10, 15])\n\nprint(\"Layer | Raw Logit Lens | Tuned Lens\")\nprint(\"-\" * 45)\nfor layer in [0, 5, 10, 15]:\n    raw_pred = raw_results[layer][-1][0][2]\n    tuned_pred = tuned_results[layer][0][2]\n    print(f\"  {layer:2d}  | {raw_pred:14s} | {tuned_pred}\")\n</code></pre> <p>Tutorial Structure: 1. Limitations of the raw logit lens (coordinate system mismatch) 2. The tuned lens solution: learned affine translators 3. Training objective: minimizing KL divergence 4. Implementing the TunedLens class in MLX 5. Training loop with SGD + momentum 6. Comparing raw vs. tuned predictions 7. Applications: malicious input detection, understanding prediction trajectories</p> <p>Key Experiments to Reproduce: - Figure 2: Comparison of logit lens vs. tuned lens accuracy - Prediction trajectory visualization - Layer-wise KL divergence analysis</p> <p>Estimated Complexity: \u2b50\u2b50 (Beginner-Intermediate)</p> <p>Note: This paper requires implementing a new feature in mlxterp. See GitHub Issue #2 for the implementation plan.</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#paper-2-locating-and-editing-factual-associations-in-gpt-intermediate","title":"Paper 2: Locating and Editing Factual Associations in GPT (Intermediate)","text":"<p>Original Work: Locating and Editing Factual Associations in GPT by Meng et al. (NeurIPS 2022)</p> <p>Project Page: https://rome.baulab.info/</p> <p>Why This Paper: - Foundational: One of the most cited mech interp papers - Clear Methodology: Causal tracing has a well-defined experimental setup - Direct Match: mlxterp's <code>activation_patching()</code> implements the core technique - Practical Impact: Shows where factual knowledge is stored in transformers</p> <p>Core Concept: Factual associations (e.g., \"Eiffel Tower is in Paris\") are stored in specific MLP modules at middle layers, specifically when processing the subject's last token. This is discovered through \"causal tracing\" - corrupting inputs with noise, then patching clean activations back in to see which restore the correct answer.</p> <p>mlxterp Implementation: <pre><code>from mlxterp import InterpretableModel, interventions as iv\nimport mlx.core as mx\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Causal tracing: Find which layers store \"Paris\" for \"Eiffel Tower\"\n# Step 1: Get clean and corrupted baselines\nclean_text = \"The Eiffel Tower is located in the city of\"\ncorrupted_text = \"The Eiffel Tower is located in the city of\"  # Will add noise\n\n# Step 2: Use activation patching to identify critical layers\nresults = model.activation_patching(\n    clean_text=\"The Eiffel Tower is located in the city of\",\n    corrupted_text=\"The Louvre Museum is located in the city of\",  # Different subject\n    component=\"mlp\",\n    plot=True\n)\n\n# Step 3: Identify the \"causal\" layers\nsorted_layers = sorted(results.items(), key=lambda x: x[1], reverse=True)\nprint(\"Most important MLP layers for factual recall:\")\nfor layer, recovery in sorted_layers[:5]:\n    print(f\"  Layer {layer}: {recovery:.1f}% recovery\")\n</code></pre></p> <p>Tutorial Structure: 1. Background: Where is knowledge stored in LLMs? 2. The causal tracing methodology 3. Implementing clean vs. corrupted runs 4. Using <code>activation_patching()</code> for MLP and attention 5. Visualizing the localization of factual associations 6. Discussion: Implications for model editing</p> <p>Key Experiments to Reproduce: - Figure 2: Causal tracing showing MLP importance at subject's last token - Comparison of MLP vs. attention contributions - Layer-wise importance profile</p> <p>Estimated Complexity: \u2b50\u2b50 (Intermediate)</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#paper-3-steering-llama-2-via-contrastive-activation-addition-intermediate","title":"Paper 3: Steering Llama 2 via Contrastive Activation Addition (Intermediate)","text":"<p>Original Work: Steering Llama 2 via Contrastive Activation Addition by Rimsky et al. (ACL 2024)</p> <p>Why This Paper: - Practical: Directly applicable technique for controlling model behavior - Simple Core Idea: Subtract negative from positive activations \u2192 steering vector - Perfect Match: mlxterp's <code>add_vector</code> intervention is designed for this - Immediate Results: Effects are visible in model outputs</p> <p>Core Concept: Create a \"steering vector\" by computing the difference between activations on positive examples (e.g., honest responses) and negative examples (e.g., deceptive responses). Adding this vector during inference steers the model toward the desired behavior.</p> <p>mlxterp Implementation: <pre><code>from mlxterp import InterpretableModel, interventions as iv\nimport mlx.core as mx\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Step 1: Collect activations from contrastive examples\npositive_prompts = [\n    \"I think this is great because\",\n    \"I love the way this works since\",\n    \"This makes me happy because\",\n]\n\nnegative_prompts = [\n    \"I hate this because\",\n    \"This is terrible since\",\n    \"This makes me angry because\",\n]\n\n# Step 2: Compute mean activations\nlayer = 10  # Middle layer typically works well\npositive_acts = []\nnegative_acts = []\n\nfor prompt in positive_prompts:\n    with model.trace(prompt) as trace:\n        act = trace.activations[f'model.model.layers.{layer}']\n        positive_acts.append(act[0, -1, :])  # Last token\n\nfor prompt in negative_prompts:\n    with model.trace(prompt) as trace:\n        act = trace.activations[f'model.model.layers.{layer}']\n        negative_acts.append(act[0, -1, :])\n\n# Step 3: Compute steering vector\npositive_mean = mx.mean(mx.stack(positive_acts), axis=0)\nnegative_mean = mx.mean(mx.stack(negative_acts), axis=0)\nsteering_vector = positive_mean - negative_mean\n\n# Step 4: Apply steering\ntest_prompt = \"This movie is\"\n\n# Without steering\nwith model.trace(test_prompt) as trace:\n    normal_output = trace.activations['__model_output__']\n\n# With positive steering (scale factor for strength)\nwith model.trace(test_prompt,\n                 interventions={f'model.model.layers.{layer}': iv.add_vector(steering_vector * 2.0)}):\n    steered_output = model.output.save()\n\n# Compare predictions\nprint(\"Normal:\", model.get_token_predictions(normal_output[0, -1, :], top_k=5))\nprint(\"Steered:\", model.get_token_predictions(steered_output[0, -1, :], top_k=5))\n</code></pre></p> <p>Tutorial Structure: 1. Introduction to representation engineering 2. The contrastive activation addition method 3. Collecting activations from contrastive pairs 4. Computing and normalizing steering vectors 5. Applying steering with different strengths 6. Evaluating behavioral changes 7. Exercises: Create your own steering vectors (honesty, creativity, formality)</p> <p>Key Experiments to Reproduce: - Sycophancy steering - Corrigibility/refusal steering - Multi-layer steering comparison</p> <p>Estimated Complexity: \u2b50\u2b50 (Intermediate)</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#paper-4-in-context-learning-and-induction-heads-intermediate-advanced","title":"Paper 4: In-Context Learning and Induction Heads (Intermediate-Advanced)","text":"<p>Original Work: In-context Learning and Induction Heads by Olsson et al. (Anthropic, 2022)</p> <p>Why This Paper: - Mechanistic: One of the clearest examples of reverse-engineering a specific capability - Foundational: Induction heads are a fundamental building block in transformers - Testable: Clear algorithm (A...[A] \u2192 [B]) that can be verified - Attention Focus: Demonstrates how to analyze attention patterns</p> <p>Core Concept: Induction heads are attention heads that implement a simple pattern-completion algorithm: given a sequence A...[A], they predict [B]. They work through a two-step process involving a \"previous token head\" and the induction head itself.</p> <p>mlxterp Implementation: <pre><code>from mlxterp import InterpretableModel\nimport mlx.core as mx\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Test prompt with repeating pattern\ntest_prompt = \"The cat sat on the mat. The cat\"\n# Expected: model should predict tokens that followed \"The cat\" earlier\n\n# Step 1: Trace and get attention patterns\nwith model.trace(test_prompt) as trace:\n    pass\n\n# Get tokens for analysis\ntokens = model.encode(test_prompt)\nprint(\"Tokens:\", [model.token_to_str(t) for t in tokens])\n\n# Step 2: Find induction heads by checking attention patterns\n# An induction head attends to positions where the current token appeared before\n# and copies what came after\n\n# Analyze attention at different layers\nfor layer_idx in range(len(model.layers)):\n    attn_key = f'model.model.layers.{layer_idx}.self_attn'\n    if attn_key in trace.activations:\n        attn_output = trace.activations[attn_key]\n        print(f\"Layer {layer_idx} attention shape: {attn_output.shape}\")\n\n# Step 3: Measure induction score\n# The induction score measures how much the model's loss decreases\n# at repeated tokens (indicating it's using the pattern)\ndef measure_induction_score(model, text_with_repetition):\n    \"\"\"Measure how well the model predicts repeated sequences.\"\"\"\n    tokens = model.encode(text_with_repetition)\n\n    # Find repeated subsequences\n    # Compare loss at first occurrence vs. repeated occurrence\n    with model.trace(text_with_repetition) as trace:\n        logits = trace.activations['__model_output__']\n\n    # Compute per-token log probabilities\n    # Higher at repeated positions = induction behavior\n    return logits\n\n# Step 4: Ablation study - knock out potential induction heads\nfor layer_idx in [4, 5, 6]:  # Common locations for induction heads\n    with model.trace(test_prompt,\n                     interventions={f'model.model.layers.{layer_idx}.self_attn': iv.zero_out}):\n        ablated_output = model.output.save()\n\n    predictions = model.get_token_predictions(ablated_output[0, -1, :], top_k=3)\n    print(f\"Layer {layer_idx} ablated: {[model.token_to_str(p) for p in predictions]}\")\n</code></pre></p> <p>Tutorial Structure: 1. What are induction heads and why do they matter? 2. The A...[A] \u2192 [B] pattern 3. How induction heads compose with previous token heads 4. Measuring induction behavior in language models 5. Finding induction heads through attention analysis 6. Ablation studies to confirm causal role 7. Connection to in-context learning</p> <p>Key Experiments to Reproduce: - Induction score computation - Layer-wise ablation to find induction heads - Attention pattern visualization showing diagonal offset pattern</p> <p>Estimated Complexity: \u2b50\u2b50\u2b50 (Intermediate-Advanced)</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#paper-5-towards-monosemanticity-sparse-autoencoders-advanced","title":"Paper 5: Towards Monosemanticity - Sparse Autoencoders (Advanced)","text":"<p>Original Work: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning by Anthropic (2023)</p> <p>Why This Paper: - State of the Art: SAEs are the current frontier of mech interp research - Built-in Support: mlxterp has SAE training and analysis built-in - Feature Discovery: Shows how to find interpretable features in superposition - Practical: Enables feature-level understanding of model behavior</p> <p>Core Concept: Individual neurons are often polysemantic (responding to multiple unrelated concepts) due to superposition. Sparse autoencoders learn to decompose activations into a larger set of more interpretable \"features\" that correspond to single concepts.</p> <p>mlxterp Implementation: <pre><code>from mlxterp import InterpretableModel, SAEConfig\nfrom datasets import load_dataset\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Step 1: Collect training data\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\ntexts = [item[\"text\"] for item in dataset if len(item[\"text\"]) &gt; 100][:5000]\n\n# Step 2: Configure and train SAE\nconfig = SAEConfig(\n    expansion_factor=32,  # 32x overcomplete dictionary\n    k=64,                 # Top-k sparsity\n    learning_rate=3e-4,\n    num_epochs=3\n)\n\nsae = model.train_sae(\n    layer=10,\n    dataset=texts,\n    component=\"mlp\",\n    config=config,\n    save_path=\"tutorial_sae_layer10.mlx\"\n)\n\n# Step 3: Analyze learned features\n# Find what features activate for specific concepts\ntest_texts = [\n    \"Paris is the capital of France\",\n    \"Tokyo is a city in Japan\",\n    \"The president signed the bill\",\n    \"def fibonacci(n): return n if n &lt; 2 else\",\n]\n\nfor text in test_texts:\n    top_features = model.get_top_features_for_text(\n        text=text,\n        sae=sae,\n        layer=10,\n        component=\"mlp\",\n        top_k=5\n    )\n    print(f\"\\n'{text[:50]}...'\")\n    for feat_id, activation in top_features:\n        print(f\"  Feature {feat_id}: {activation:.3f}\")\n\n# Step 4: Interpret a specific feature\n# Find what texts maximize activation of a feature\nfeature_to_analyze = top_features[0][0]\nexamples = model.get_top_texts_for_feature(\n    feature_id=feature_to_analyze,\n    sae=sae,\n    texts=texts[:1000],  # Search through dataset\n    layer=10,\n    component=\"mlp\",\n    top_k=10\n)\n\nprint(f\"\\nTexts that activate feature {feature_to_analyze}:\")\nfor text, activation, pos in examples[:5]:\n    print(f\"  [{activation:.3f}] {text[:80]}...\")\n</code></pre></p> <p>Tutorial Structure: 1. The superposition hypothesis and why neurons are polysemantic 2. Sparse autoencoders: architecture and training objective 3. Training an SAE on model activations 4. Identifying and interpreting learned features 5. Feature steering: using SAE features for targeted interventions 6. Limitations and future directions</p> <p>Key Experiments to Reproduce: - Training an SAE on a specific layer - Feature identification and interpretation - Finding maximally activating examples for features - Comparing feature sparsity across different configurations</p> <p>Estimated Complexity: \u2b50\u2b50\u2b50\u2b50 (Advanced)</p>"},{"location":"TUTORIAL_PAPERS_PLAN/#implementation-priority","title":"Implementation Priority","text":"<p>Based on educational value and difficulty progression:</p> Priority Paper Complexity Estimated Time Requires New Feature 1 Logit Lens \u2b50 Beginner 1-2 hours No 1b Tuned Lens \u2b50\u2b50 Beginner-Int 3-4 hours Yes (Issue #2) 2 ROME / Causal Tracing \u2b50\u2b50 Intermediate 3-4 hours No 3 Steering Vectors (CAA) \u2b50\u2b50 Intermediate 2-3 hours No 4 Induction Heads \u2b50\u2b50\u2b50 Int-Advanced 4-5 hours No 5 Sparse Autoencoders \u2b50\u2b50\u2b50\u2b50 Advanced 5-6 hours No"},{"location":"TUTORIAL_PAPERS_PLAN/#tutorial-format","title":"Tutorial Format","text":"<p>Each tutorial should include:</p> <ol> <li>Introduction</li> <li>Paper summary and motivation</li> <li>Prerequisites and background</li> <li> <p>Learning objectives</p> </li> <li> <p>Conceptual Overview</p> </li> <li>Key ideas explained simply</li> <li>Diagrams and visualizations</li> <li> <p>Connection to broader mech interp themes</p> </li> <li> <p>Step-by-Step Implementation</p> </li> <li>Working code with explanations</li> <li>Expected outputs shown</li> <li> <p>Common pitfalls noted</p> </li> <li> <p>Experiments</p> </li> <li>Reproduce key paper results</li> <li>Try variations and extensions</li> <li> <p>Compare with paper figures</p> </li> <li> <p>Exercises</p> </li> <li>Hands-on challenges</li> <li>Questions for deeper understanding</li> <li> <p>Suggestions for further exploration</p> </li> <li> <p>References</p> </li> <li>Original paper and related work</li> <li>Additional resources</li> <li>mlxterp documentation links</li> </ol>"},{"location":"TUTORIAL_PAPERS_PLAN/#file-organization","title":"File Organization","text":"<pre><code>examples/tutorials/\n\u251c\u2500\u2500 01_logit_lens/\n\u2502   \u251c\u2500\u2500 tutorial.md\n\u2502   \u251c\u2500\u2500 logit_lens_tutorial.py\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 01b_tuned_lens/\n\u2502   \u251c\u2500\u2500 tutorial.md\n\u2502   \u251c\u2500\u2500 tuned_lens_tutorial.py\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 02_causal_tracing/\n\u2502   \u251c\u2500\u2500 tutorial.md\n\u2502   \u251c\u2500\u2500 causal_tracing_tutorial.py\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 03_steering_vectors/\n\u2502   \u251c\u2500\u2500 tutorial.md\n\u2502   \u251c\u2500\u2500 steering_tutorial.py\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 04_induction_heads/\n\u2502   \u251c\u2500\u2500 tutorial.md\n\u2502   \u251c\u2500\u2500 induction_heads_tutorial.py\n\u2502   \u2514\u2500\u2500 figures/\n\u2514\u2500\u2500 05_sparse_autoencoders/\n    \u251c\u2500\u2500 tutorial.md\n    \u251c\u2500\u2500 sae_tutorial.py\n    \u2514\u2500\u2500 figures/\n</code></pre>"},{"location":"TUTORIAL_PAPERS_PLAN/#references","title":"References","text":""},{"location":"TUTORIAL_PAPERS_PLAN/#papers","title":"Papers","text":"<ol> <li>Logit Lens</li> <li>nostalgebraist. (2020). interpreting GPT: the logit lens</li> </ol> <p>1b. Tuned Lens    - Belrose, N., et al. (2023). Eliciting Latent Predictions from Transformers with the Tuned Lens. NeurIPS 2023.    - GitHub: https://github.com/AlignmentResearch/tuned-lens    - Documentation: https://tuned-lens.readthedocs.io/</p> <ol> <li>ROME / Causal Tracing</li> <li>Meng, K., et al. (2022). Locating and Editing Factual Associations in GPT. NeurIPS 2022.</li> <li> <p>Project page: https://rome.baulab.info/</p> </li> <li> <p>Contrastive Activation Addition</p> </li> <li> <p>Rimsky, N., et al. (2024). Steering Llama 2 via Contrastive Activation Addition. ACL 2024.</p> </li> <li> <p>Induction Heads</p> </li> <li> <p>Olsson, C., et al. (2022). In-context Learning and Induction Heads. Anthropic.</p> </li> <li> <p>Sparse Autoencoders</p> </li> <li>Anthropic. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.</li> </ol>"},{"location":"TUTORIAL_PAPERS_PLAN/#additional-resources","title":"Additional Resources","text":"<ul> <li>Mechanistic Interpretability for AI Safety \u2014 A Review</li> <li>TransformerLens Documentation</li> <li>nnsight Documentation</li> <li>Anthropic's Transformer Circuits Thread</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This document explains the design and implementation of mlxterp.</p>"},{"location":"architecture/#design-philosophy","title":"Design Philosophy","text":"<p>mlxterp follows these core principles:</p> <ol> <li>Simplicity First: Make mechanistic interpretability accessible with minimal code</li> <li>Clean API: Context managers and direct attribute access (inspired by nnterp)</li> <li>Model Agnostic: Generic wrapping works with any MLX model (inspired by nnsight)</li> <li>MLX Native: Leverage MLX's unique features (lazy evaluation, unified memory)</li> <li>Minimal Abstractions: Avoid unnecessary complexity and boilerplate</li> </ol>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        InterpretableModel               \u2502\n\u2502  (User-facing API)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Trace    \u2502  \u2502LayerListProxy\u2502\n\u2502(Context Mgr)\u2502  \u2502(Layer Access)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502               \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 TraceContext \u2502\n        \u2502(Global State)\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502          \u2502          \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u25bc\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n\u2502Proxies\u2502  \u2502Cache\u2502  \u2502Intervene\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-interpretablemodel","title":"1. InterpretableModel","text":"<p>File: <code>mlxterp/model.py</code></p> <p>The main entry point for users. Provides:</p> <ul> <li>Model loading (from string or nn.Module)</li> <li>Layer discovery and wrapping</li> <li>Trace context creation</li> <li>Delegation to underlying model</li> </ul> <p>Key Methods: - <code>trace()</code>: Create tracing context - <code>named_modules()</code>: Iterate over modules - <code>parameters()</code>: Access parameters</p> <p>Design Decision: Uses composition over inheritance - wraps models rather than subclassing.</p>"},{"location":"architecture/#2-proxy-system","title":"2. Proxy System","text":"<p>File: <code>mlxterp/core/proxy.py</code></p> <p>Implements transparent wrapping for clean attribute access.</p>"},{"location":"architecture/#moduleproxy","title":"ModuleProxy","text":"<p>Wraps <code>nn.Module</code> instances to intercept forward passes:</p> <pre><code>class ModuleProxy:\n    def __call__(self, *args, **kwargs):\n        # Call original module\n        result = self._module(*args, **kwargs)\n\n        # Capture in context\n        ctx = TraceContext.current()\n        if ctx:\n            ctx.activations[self._name] = result\n\n        return result\n</code></pre> <p>Key Features: - Transparent wrapping (user doesn't see proxy) - Automatic activation capture - Intervention support - Lazy submodule wrapping</p>"},{"location":"architecture/#outputproxy","title":"OutputProxy","text":"<p>Provides <code>.save()</code> functionality:</p> <pre><code>class OutputProxy:\n    def save(self):\n        ctx = TraceContext.current()\n        ctx.save(self._name, self._value)\n        return self._value\n</code></pre>"},{"location":"architecture/#layerlistproxy","title":"LayerListProxy","text":"<p>Provides indexed access to layers:</p> <pre><code>model.layers[3]  # Returns ModuleProxy\nmodel.layers[3].self_attn.output  # Returns OutputProxy (use self_attn for mlx-lm models)\n</code></pre>"},{"location":"architecture/#3-trace-context","title":"3. Trace Context","text":"<p>File: <code>mlxterp/core/trace.py</code></p> <p>Context manager for tracing execution.</p>"},{"location":"architecture/#lifecycle","title":"Lifecycle","text":"<ol> <li><code>__enter__</code>: Setup context, push to global stack, execute forward pass</li> <li>User code: Access activations, call <code>.save()</code> on outputs</li> <li><code>__exit__</code>: Copy saved values and activations, restore layers, pop context</li> </ol> <p>Design Decision: Forward pass executes in <code>__enter__</code> so activations are immediately available inside the context. Saved values and activations are copied in <code>__exit__</code> so they remain available after the context.</p>"},{"location":"architecture/#global-state-management","title":"Global State Management","text":"<p>Uses a stack to support nested traces (though typically only one active):</p> <pre><code>class TraceContext:\n    _stack = []\n\n    @classmethod\n    def current(cls):\n        return cls._stack[-1] if cls._stack else None\n</code></pre>"},{"location":"architecture/#4-intervention-system","title":"4. Intervention System","text":"<p>File: <code>mlxterp/core/intervention.py</code></p> <p>Interventions are simply functions: <code>(mx.array) -&gt; mx.array</code></p> <p>Design Decision: Functional approach over class hierarchies for simplicity.</p>"},{"location":"architecture/#pre-built-interventions","title":"Pre-built Interventions","text":"<ul> <li><code>zero_out</code>: Set to zeros</li> <li><code>scale(factor)</code>: Multiply by constant</li> <li><code>add_vector(vec)</code>: Add steering vector</li> <li><code>replace_with(value)</code>: Replace with fixed value</li> <li><code>clamp(min, max)</code>: Clamp to range</li> <li><code>noise(std)</code>: Add Gaussian noise</li> </ul>"},{"location":"architecture/#composition","title":"Composition","text":"<pre><code>combined = InterventionComposer() \\\n    .add(scale(0.5)) \\\n    .add(noise(0.1)) \\\n    .build()\n</code></pre>"},{"location":"architecture/#5-caching-system","title":"5. Caching System","text":"<p>File: <code>mlxterp/core/cache.py</code></p> <p>Simple dictionary-based caching with utility functions:</p> <ul> <li><code>ActivationCache</code>: Storage container</li> <li><code>collect_activations()</code>: Helper for common patterns</li> </ul> <p>Design Decision: Simple dict storage rather than complex cache invalidation - rely on MLX's lazy evaluation.</p>"},{"location":"architecture/#mlx-integration","title":"MLX Integration","text":""},{"location":"architecture/#leveraging-mlx-features","title":"Leveraging MLX Features","text":""},{"location":"architecture/#1-lazy-evaluation","title":"1. Lazy Evaluation","text":"<p>MLX arrays are lazy - computation deferred until <code>mx.eval()</code>:</p> <pre><code># These don't execute immediately\na = mx.random.normal((100, 100))\nb = mx.matmul(a, a)\nc = mx.sum(b)\n\n# Execute now\nmx.eval(c)\n</code></pre> <p>How mlxterp uses it: - Activations captured as lazy arrays - Only materialized when accessed via <code>.save()</code> - Efficient memory usage</p>"},{"location":"architecture/#2-unified-memory","title":"2. Unified Memory","text":"<p>MLX arrays live in shared memory - no CPU/GPU transfers:</p> <pre><code># Automatically available on both CPU and GPU\narr = mx.array([1, 2, 3])\n# No need for .to('cuda') or .cpu()\n</code></pre> <p>How mlxterp uses it: - Zero-copy activation caching - Efficient intervention application - No device management needed</p>"},{"location":"architecture/#3-dynamic-graphs","title":"3. Dynamic Graphs","text":"<p>Computation graphs built dynamically (like PyTorch eager mode):</p> <pre><code># Graph constructed on-the-fly\nfor layer in model.layers:\n    x = layer(x)  # Each call adds to graph\n</code></pre> <p>How mlxterp uses it: - Easy to intercept operations - No need for graph tracing hooks - Simple proxy-based wrapping works</p>"},{"location":"architecture/#4-module-system","title":"4. Module System","text":"<p>MLX modules are dict-based:</p> <pre><code>class Module(dict):\n    def __init__(self):\n        self._no_grad = set()\n        self._training = True\n</code></pre> <p>How mlxterp uses it: - Easy parameter inspection via <code>tree_flatten()</code> - Natural module traversal via <code>named_modules()</code> - Simple attribute access patterns</p>"},{"location":"architecture/#implementation-patterns","title":"Implementation Patterns","text":""},{"location":"architecture/#pattern-1-proxy-based-interception","title":"Pattern 1: Proxy-Based Interception","text":"<p>Instead of monkey-patching or hooks:</p> <pre><code># MLX module\noriginal_module = nn.Linear(128, 128)\n\n# Wrap with proxy\nproxy = ModuleProxy(original_module, \"layer_name\")\n\n# Calls are intercepted transparently\noutput = proxy(input)  # Captured in trace context\n</code></pre> <p>Benefits: - No modification of original module - Type-safe (proxy delegates to original) - Easy to add/remove</p>"},{"location":"architecture/#pattern-2-context-manager-pattern","title":"Pattern 2: Context Manager Pattern","text":"<p>Clean setup/teardown:</p> <pre><code>class Trace:\n    def __enter__(self):\n        # Setup: Create and push context\n        self.context = TraceContext()\n        TraceContext.push(self.context)\n\n        # Execute: Run model forward pass immediately\n        # This allows users to access activations inside the with block\n        self.output = self.model_forward(self.inputs)\n        return self\n\n    def __exit__(self, *args):\n        # Copy saved_values from context (for values saved inside the block)\n        self.saved_values = self.context.saved_values.copy()\n\n        # Cleanup: Pop context\n        TraceContext.pop()\n</code></pre> <p>Benefits: - Guaranteed cleanup - Clear lifecycle - Pythonic API</p>"},{"location":"architecture/#pattern-3-functional-interventions","title":"Pattern 3: Functional Interventions","text":"<p>Simple functions over classes:</p> <pre><code># Intervention is just a function\ndef scale(factor: float):\n    def _scale(x: mx.array) -&gt; mx.array:\n        return x * factor\n    return _scale\n\n# Use it\ninterventions = {\"layers.3\": scale(0.5)}\n</code></pre> <p>Benefits: - Easy to compose - No inheritance complexity - Clear semantics</p>"},{"location":"architecture/#pattern-4-lazy-proxy-creation","title":"Pattern 4: Lazy Proxy Creation","text":"<p>Proxies created on-demand:</p> <pre><code>class ModuleProxy:\n    def __getattr__(self, name):\n        if name not in self._subproxies:\n            # Create proxy only when accessed\n            attr = getattr(self._module, name)\n            if isinstance(attr, nn.Module):\n                self._subproxies[name] = ModuleProxy(attr, f\"{self._name}.{name}\")\n        return self._subproxies[name]\n</code></pre> <p>Benefits: - Efficient memory usage - Automatic submodule discovery - No upfront cost</p>"},{"location":"architecture/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":""},{"location":"architecture/#vs-transformerlens","title":"vs TransformerLens","text":"Aspect mlxterp TransformerLens Approach Generic wrapping Model-specific classes Models Any MLX model Specific architectures API Context managers Direct methods Framework MLX PyTorch Lines of Code ~600 ~10,000+ <p>Trade-offs: - mlxterp: More flexible, less feature-complete - TransformerLens: More features, less flexible</p>"},{"location":"architecture/#vs-nnsight","title":"vs nnsight","text":"Aspect mlxterp nnsight Approach Similar (generic) Generic wrapping API Style Context managers Context managers Framework MLX PyTorch Focus Simplicity Completeness <p>Inspiration: mlxterp closely follows nnsight's design philosophy.</p>"},{"location":"architecture/#vs-nnterp","title":"vs nnterp","text":"Aspect mlxterp nnterp Approach Similar Standardized naming API Very similar Clean context managers Framework MLX PyTorch/nnsight Focus Apple Silicon Model unification <p>Inspiration: mlxterp adopts nnterp's clean API design.</p>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#memory-efficiency","title":"Memory Efficiency","text":"<ol> <li>Lazy Evaluation: Activations not computed until needed</li> <li>Selective Saving: Only save what you explicitly <code>.save()</code></li> <li>Unified Memory: No device transfers</li> <li>Batch Processing: Utilities for large datasets</li> </ol>"},{"location":"architecture/#computational-efficiency","title":"Computational Efficiency","text":"<ol> <li>Minimal Overhead: Proxy calls are lightweight</li> <li>Metal Acceleration: Inherits MLX's Metal optimizations</li> <li>Zero-Copy Operations: Interventions don't copy data unnecessarily</li> </ol>"},{"location":"architecture/#best-practices","title":"Best Practices","text":"<pre><code># \u2705 Good: Only save needed activations\nwith model.trace(input):\n    important = [3, 8, 12]\n    acts = {i: model.layers[i].output.save() for i in important}\n\n# \u274c Avoid: Saving everything\nwith model.trace(input):\n    all_acts = [model.layers[i].output.save() for i in range(100)]\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#custom-interventions","title":"Custom Interventions","text":"<p>Easy to add:</p> <pre><code>def my_intervention(x: mx.array) -&gt; mx.array:\n    # Your logic\n    return transformed_x\n\n# Use immediately\nwith model.trace(input, interventions={\"layers.3\": my_intervention}):\n    output = model.output.save()\n</code></pre>"},{"location":"architecture/#custom-utilities","title":"Custom Utilities","text":"<p>Built on core components:</p> <pre><code>from mlxterp.core import collect_activations\n\ndef my_analysis_function(model, prompts):\n    cache = collect_activations(model, prompts)\n    # Your analysis\n    return results\n</code></pre>"},{"location":"architecture/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>Compatible with MLX ecosystem:</p> <pre><code># Use with mlx_lm\nfrom mlx_lm import load\nmodel_base, tokenizer = load(\"model-name\")\nmodel = InterpretableModel(model_base, tokenizer)\n\n# Use with mlx training\n# Access underlying model\nmodel.model.train()\noptimizer = optim.Adam(model.parameters())\n</code></pre>"},{"location":"architecture/#future-enhancements","title":"Future Enhancements","text":"<p>Potential additions (maintaining simplicity):</p> <ol> <li>Visualization Tools: Attention pattern plotting</li> <li>Circuit Discovery: Automated path finding</li> <li>Sparse Autoencoders: Feature extraction support</li> <li>Logit Lens: Intermediate decoding utilities</li> <li>Benchmarks: Standard interpretability tasks</li> </ol>"},{"location":"architecture/#summary","title":"Summary","text":"<p>mlxterp achieves its goals through:</p> <ul> <li>Clean abstraction layers: Each component has clear responsibility</li> <li>MLX integration: Leverages framework strengths</li> <li>Simple patterns: Proxies, context managers, functions</li> <li>Minimal code: ~600 lines of core functionality</li> <li>Extensible design: Easy to add custom behavior</li> </ul> <p>The architecture prioritizes simplicity and usability over feature completeness, making mechanistic interpretability accessible to researchers on Apple Silicon.</p>"},{"location":"citation/","title":"Citation","text":"<p>If you use mlxterp in your research, please cite it as follows:</p>"},{"location":"citation/#bibtex","title":"BibTeX","text":"<pre><code>@software{mlxterp2025,\n  author = {Sigurd Schacht},\n  title = {mlxterp: Mechanistic Interpretability for MLX},\n  year = {2025},\n  url = {https://github.com/coairesearch/mlxterp},\n  version = {0.1.0}\n}\n</code></pre>"},{"location":"citation/#apa","title":"APA","text":"<pre><code>Schacht, S. (2025). mlxterp: Mechanistic Interpretability for MLX (Version 0.1.0) [Computer software]. https://github.com/coairesearch/mlxterp\n</code></pre>"},{"location":"citation/#mla","title":"MLA","text":"<pre><code>Schacht, Sigurd. mlxterp: Mechanistic Interpretability for MLX. Version 0.1.0, 2025. GitHub, https://github.com/coairesearch/mlxterp.\n</code></pre>"},{"location":"citation/#related-work","title":"Related Work","text":"<p>If you use mlxterp, you may also want to cite the frameworks that inspired it:</p>"},{"location":"citation/#nnsight","title":"nnsight","text":"<pre><code>@software{nnsight2024,\n  author = {Fiotto-Kaufman, Jaden and others},\n  title = {nnsight: A Python package for interpreting language models},\n  year = {2024},\n  url = {https://nnsight.net}\n}\n</code></pre>"},{"location":"citation/#nnterp","title":"nnterp","text":"<pre><code>@software{nnterp2024,\n  author = {Butanium},\n  title = {nnterp: Unified LLM Interpretability Interface},\n  year = {2024},\n  url = {https://github.com/Butanium/nnterp}\n}\n</code></pre>"},{"location":"citation/#mlx","title":"MLX","text":"<pre><code>@software{mlx2023,\n  author = {Apple Inc.},\n  title = {MLX: An array framework for Apple silicon},\n  year = {2023},\n  url = {https://github.com/ml-explore/mlx}\n}\n</code></pre>"},{"location":"citation/#acknowledgments","title":"Acknowledgments","text":"<p>We thank:</p> <ul> <li>The MLX team at Apple for creating an excellent ML framework</li> <li>The nnsight and nnterp teams for pioneering clean interpretability APIs</li> <li>The mechanistic interpretability community for advancing the field</li> </ul>"},{"location":"citation/#contributing","title":"Contributing","text":"<p>Contributions to mlxterp are welcome! See the Contributing Guide for guidelines.</p>"},{"location":"contributing/","title":"Contributing to mlxterp","text":"<p>Thank you for your interest in contributing to mlxterp! This document provides guidelines for contributing.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally</li> <li>Set up the development environment</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/mlxterp\ncd mlxterp\n\n# Install with all dependencies\nuv sync --all-extras\n\n# Run tests\nuv run pytest\n\n# Start development server for docs\nuv run mkdocs serve\n</code></pre>"},{"location":"contributing/#using-pip","title":"Using pip","text":"<pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/mlxterp\ncd mlxterp\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install in editable mode\npip install -e \".[dev,viz]\"\n\n# Run tests\npytest\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#python-code","title":"Python Code","text":"<p>We use Black for formatting and flake8 for linting:</p> <pre><code># Format code\nblack mlxterp/ examples/\n\n# Check linting\nflake8 mlxterp/\n\n# Type checking\nmypy mlxterp/\n</code></pre>"},{"location":"contributing/#guidelines","title":"Guidelines","text":"<ul> <li>Follow PEP 8</li> <li>Maximum line length: 100 characters</li> <li>Use type hints for all functions</li> <li>Write docstrings (Google style)</li> </ul> <p>Example:</p> <pre><code>def my_function(x: mx.array, scale: float = 1.0) -&gt; mx.array:\n    \"\"\"\n    Brief description of function.\n\n    Args:\n        x: Input array\n        scale: Scaling factor\n\n    Returns:\n        Scaled array\n\n    Example:\n        &gt;&gt;&gt; result = my_function(mx.array([1, 2, 3]), scale=2.0)\n    \"\"\"\n    return x * scale\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_proxy.py\n\n# Run with coverage\npytest --cov=mlxterp --cov-report=html\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<p>Place tests in <code>tests/</code> directory:</p> <pre><code># tests/test_feature.py\nimport mlxterp\nimport mlx.core as mx\n\ndef test_my_feature():\n    \"\"\"Test description\"\"\"\n    model = mlxterp.InterpretableModel(...)\n    # Your test\n    assert result == expected\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#building-docs","title":"Building Docs","text":"<pre><code># Serve locally\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"contributing/#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Use Markdown format</li> <li>Include code examples</li> <li>Add type hints to examples</li> <li>Update API.md for new functions</li> </ul>"},{"location":"contributing/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md          - Home page\n\u251c\u2500\u2500 QUICKSTART.md     - Getting started\n\u251c\u2500\u2500 installation.md   - Install instructions\n\u251c\u2500\u2500 examples.md       - Usage examples\n\u251c\u2500\u2500 API.md           - API reference\n\u251c\u2500\u2500 architecture.md  - Design docs\n\u2514\u2500\u2500 contributing.md  - This file\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run tests: Ensure all tests pass</li> <li>Format code: Run <code>black</code> on changed files</li> <li>Update docs: Add documentation for new features</li> <li>Write tests: Include tests for new functionality</li> <li>Update CHANGELOG: Add entry for your changes</li> </ol>"},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests pass locally</li> <li> Code is formatted (black)</li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> <li> Commit messages are clear</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Use clear, descriptive commit messages:</p> <pre><code>Add intervention composition feature\n\n- Implement InterventionComposer class\n- Add tests for composition\n- Update documentation with examples\n</code></pre>"},{"location":"contributing/#pr-title-format","title":"PR Title Format","text":"<pre><code>[Type] Brief description\n\nTypes:\n- feat: New feature\n- fix: Bug fix\n- docs: Documentation\n- refactor: Code refactoring\n- test: Add/update tests\n- chore: Maintenance\n</code></pre>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Open an issue with:</p> <ul> <li>Clear description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment (Python version, macOS version, chip type)</li> <li>Minimal code example</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Open an issue with:</p> <ul> <li>Use case description</li> <li>Proposed API (if applicable)</li> <li>Example usage</li> <li>Rationale</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>Areas for contribution:</p> <ol> <li>Core Features</li> <li>New intervention types</li> <li>Performance optimizations</li> <li> <p>Better error messages</p> </li> <li> <p>Utilities</p> </li> <li>Activation analysis functions</li> <li>Visualization tools</li> <li> <p>Data processing helpers</p> </li> <li> <p>Documentation</p> </li> <li>More examples</li> <li>Tutorials</li> <li> <p>API clarifications</p> </li> <li> <p>Testing</p> </li> <li>Increase coverage</li> <li>Add edge case tests</li> <li>Performance benchmarks</li> </ol>"},{"location":"contributing/#design-principles","title":"Design Principles","text":"<p>When contributing, keep these principles in mind:</p> <ol> <li>Simplicity: Prefer simple solutions over complex ones</li> <li>Composability: Make components work together naturally</li> <li>MLX Native: Leverage MLX's features (lazy eval, unified memory)</li> <li>Clean API: Maintain intuitive, Pythonic interfaces</li> <li>Minimal Abstractions: Avoid unnecessary layers</li> </ol>"},{"location":"contributing/#example-adding-an-intervention","title":"Example: Adding an Intervention","text":"<p>Good:</p> <pre><code>def my_intervention(x: mx.array) -&gt; mx.array:\n    \"\"\"Simple, functional intervention\"\"\"\n    return mx.clip(x, -1, 1)\n</code></pre> <p>Avoid:</p> <pre><code>class MyInterventionClass:\n    \"\"\"Unnecessary class wrapper\"\"\"\n    def __init__(self, min_val, max_val):\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def apply(self, x):\n        return mx.clip(x, self.min_val, self.max_val)\n</code></pre>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Maintainer reviews your PR</li> <li>Feedback provided (if needed)</li> <li>Make requested changes</li> <li>PR approved and merged</li> </ol>"},{"location":"contributing/#review-criteria","title":"Review Criteria","text":"<ul> <li>Code quality and style</li> <li>Test coverage</li> <li>Documentation completeness</li> <li>Design consistency</li> <li>Performance impact</li> </ul>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and welcoming</li> <li>Help others learn</li> <li>Give constructive feedback</li> <li>Credit others' work</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Don't hesitate to ask questions in Issues or Discussions. We're here to help!</p> <p>Thank you for contributing to mlxterp! \ud83c\udf89</p>"},{"location":"examples/","title":"Examples","text":"<p>This page provides practical examples for using mlxterp.</p>"},{"location":"examples/#basic-tracing","title":"Basic Tracing","text":""},{"location":"examples/#simple-model-tracing","title":"Simple Model Tracing","text":"<pre><code>from mlxterp import InterpretableModel\nimport mlx.core as mx\nimport mlx.nn as nn\n\n# Create a simple model\nclass SimpleTransformer(nn.Module):\n    def __init__(self, hidden_dim=64, num_layers=4):\n        super().__init__()\n        self.layers = [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = nn.relu(x)\n        return x\n\n# Wrap with InterpretableModel\nbase_model = SimpleTransformer()\nmodel = InterpretableModel(base_model)\n\n# Create input\ninput_data = mx.random.normal((1, 10, 64))  # (batch, seq, hidden)\n\n# Trace execution\nwith model.trace(input_data) as trace:\n    layer_0 = model.layers[0].output.save()\n    layer_2 = model.layers[2].output.save()\n\nprint(f\"Layer 0 shape: {layer_0.shape}\")\nprint(f\"Layer 2 shape: {layer_2.shape}\")\n</code></pre>"},{"location":"examples/#loading-real-models","title":"Loading Real Models","text":"<pre><code>from mlxterp import InterpretableModel\n\n# Load from model hub (automatically loads tokenizer)\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Use with text input\nwith model.trace(\"The capital of France is\") as trace:\n    # Access attention outputs (use self_attn for mlx-lm models)\n    for i in [3, 6, 9]:\n        attn = model.layers[i].self_attn.output.save()\n        print(f\"Layer {i} attention shape: {attn.shape}\")\n\n    # Get final output\n    logits = model.output.save()\n\nprint(f\"Output shape: {logits.shape}\")\n</code></pre>"},{"location":"examples/#working-with-tokenizers","title":"Working with Tokenizers","text":""},{"location":"examples/#encoding-and-decoding-text","title":"Encoding and Decoding Text","text":"<pre><code>from mlxterp import InterpretableModel\n\n# Load model with tokenizer\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Encode text to tokens\ntext = \"Hello world\"\ntokens = model.encode(text)\nprint(f\"Text: '{text}'\")\nprint(f\"Tokens: {tokens}\")\n# Output: Tokens: [128000, 9906, 1917]\n\n# Decode tokens back to text\ndecoded = model.decode(tokens)\nprint(f\"Decoded: '{decoded}'\")\n# Output: Decoded: '&lt;|begin_of_text|&gt;Hello world'\n\n# Get vocabulary size\nprint(f\"Vocabulary size: {model.vocab_size}\")\n# Output: Vocabulary size: 128000\n</code></pre>"},{"location":"examples/#analyzing-individual-tokens","title":"Analyzing Individual Tokens","text":"<pre><code># Inspect each token\ntext = \"The capital of France is Paris\"\ntokens = model.encode(text)\n\nprint(\"Token breakdown:\")\nfor i, token_id in enumerate(tokens):\n    token_str = model.token_to_str(token_id)\n    print(f\"  Position {i}: {token_id:6d} -&gt; '{token_str}'\")\n\n# Output:\n# Position 0: 128000 -&gt; '&lt;|begin_of_text|&gt;'\n# Position 1:    791 -&gt; 'The'\n# Position 2:   6864 -&gt; ' capital'\n# Position 3:    315 -&gt; ' of'\n# Position 4:   9822 -&gt; ' France'\n# Position 5:    374 -&gt; ' is'\n# Position 6:  12366 -&gt; ' Paris'\n</code></pre>"},{"location":"examples/#token-position-specific-analysis","title":"Token-Position Specific Analysis","text":"<pre><code># Analyze activation at specific token position\ntext = \"The capital of France is\"\ntokens = model.encode(text)\n\nwith model.trace(text) as trace:\n    # Get layer 8 output\n    layer_8_output = model.layers[8].output.save()\n\n# Find position of \"France\" token\nfor i, token_id in enumerate(tokens):\n    if \"France\" in model.token_to_str(token_id):\n        france_pos = i\n        print(f\"'France' is at position {i}\")\n        break\n\n# Extract activation at \"France\" position\nfrance_activation = layer_8_output[0, france_pos, :]\nprint(f\"Activation at 'France': {france_activation.shape}\")\n</code></pre>"},{"location":"examples/#batch-encoding","title":"Batch Encoding","text":"<pre><code># Encode multiple texts\ntexts = [\"Hello\", \"World\", \"Test\"]\ntoken_lists = model.encode_batch(texts)\n\nfor text, tokens in zip(texts, token_lists):\n    print(f\"'{text}' -&gt; {tokens}\")\n\n# Output:\n# 'Hello' -&gt; [128000, 9906]\n# 'World' -&gt; [128000, 10343]\n# 'Test' -&gt; [128000, 2323]\n</code></pre>"},{"location":"examples/#working-with-token-arrays","title":"Working with Token Arrays","text":"<pre><code>import mlx.core as mx\n\n# Create token array manually\ntokens = mx.array([[128000, 9906, 1917]])  # [BOS, \"Hello\", \" world\"]\n\n# Trace with token array\nwith model.trace(tokens) as trace:\n    output = model.output.save()\n\n# Decode the tokens\ntext = model.decode(tokens[0])  # Pass 1D array or list\nprint(f\"Input text: '{text}'\")\n</code></pre>"},{"location":"examples/#custom-token-sequences","title":"Custom Token Sequences","text":"<pre><code># Build custom token sequence\nbos_token = 128000\ntokens = model.encode(\"The answer is\")\n\n# Add specific continuation tokens\ntokens.extend([220, 2983])  # \" 42\" tokens\n\n# Run model on custom sequence\nwith model.trace(tokens) as trace:\n    output = model.output.save()\n\n# Verify what we created\nfull_text = model.decode(tokens)\nprint(f\"Custom sequence: '{full_text}'\")\n</code></pre>"},{"location":"examples/#logit-lens-and-predictions","title":"Logit Lens and Predictions","text":""},{"location":"examples/#decoding-hidden-states-to-tokens","title":"Decoding Hidden States to Tokens","text":"<p>Convert any hidden state to token predictions:</p> <pre><code># Get predictions from layer 6\nwith model.trace(\"The capital of France is\") as trace:\n    layer_6 = trace.activations[\"model.model.layers.6\"]\n\n# Get last token's hidden state\nlast_token_hidden = layer_6[0, -1, :]\n\n# Get top 10 predictions\npredictions = model.get_token_predictions(last_token_hidden, top_k=10)\n\nprint(\"Layer 6 predicts:\")\nfor i, token_id in enumerate(predictions, 1):\n    token_str = model.token_to_str(token_id)\n    print(f\"  {i}. '{token_str}'\")\n\n# Output:\n#   1. ' Paris'\n#   2. ' Brussels'\n#   3. ' Lyon'\n#   ...\n</code></pre>"},{"location":"examples/#getting-prediction-scores","title":"Getting Prediction Scores","text":"<p>Include confidence scores with predictions:</p> <pre><code>with model.trace(\"Hello, how are\") as trace:\n    layer_10 = trace.activations[\"model.model.layers.10\"]\n\nhidden = layer_10[0, -1, :]\n\n# Get predictions with scores\npredictions = model.get_token_predictions(\n    hidden,\n    top_k=5,\n    return_scores=True\n)\n\nprint(\"Top predictions with scores:\")\nfor token_id, score in predictions:\n    token_str = model.token_to_str(token_id)\n    print(f\"  '{token_str}': {score:.2f}\")\n\n# Output:\n#   ' you': 18.45\n#   ' ya': 14.23\n#   ' doing': 12.87\n#   ...\n</code></pre>"},{"location":"examples/#logit-lens-analysis","title":"Logit Lens Analysis","text":"<p>The logit lens technique shows what each layer predicts at each token position in the input sequence. This reveals how the model builds up its understanding layer by layer.</p>"},{"location":"examples/#understanding-the-technique","title":"Understanding the Technique","text":"<pre><code># The logit lens analyzes predictions at EVERY position\ntext = \"The capital of France is\"\nresults = model.logit_lens(text)\n\n# Results structure: {layer_idx: [[pos_0_preds], [pos_1_preds], ...]}\n# Each layer has predictions for each input position\n\n# Get tokens for reference\ntokens = model.encode(text)\ntoken_strs = [model.token_to_str(t) for t in tokens]\nprint(f\"Input tokens: {token_strs}\")\n</code></pre>"},{"location":"examples/#show-predictions-at-last-position","title":"Show Predictions at Last Position","text":"<p>See how predictions for the last token evolve through layers:</p> <pre><code>text = \"The capital of France is\"\nresults = model.logit_lens(text, layers=[0, 5, 10, 15])\n\nprint(\"What each layer predicts for the LAST position:\\n\")\nfor layer_idx in [0, 5, 10, 15]:\n    # Get prediction at last position\n    last_pos_pred = results[layer_idx][-1][0][2]  # Top token string\n    print(f\"Layer {layer_idx:2d}: '{last_pos_pred}'\")\n\n# Output:\n# Layer  0: ' the'\n# Layer  5: ' a'\n# Layer 10: ' Paris'\n# Layer 15: ' Paris'\n</code></pre>"},{"location":"examples/#show-all-positions-for-a-specific-layer","title":"Show All Positions for a Specific Layer","text":"<p>See what a single layer predicts at each position:</p> <pre><code>text = \"The capital of France\"\nresults = model.logit_lens(text, layers=[10])\ntokens = model.encode(text)\n\nprint(\"Layer 10 predictions at each position:\\n\")\nfor pos_idx, predictions in enumerate(results[10]):\n    input_token = model.token_to_str(tokens[pos_idx])\n    pred_token = predictions[0][2]  # Top prediction\n    print(f\"  Position {pos_idx} ('{input_token}') -&gt; predicts: '{pred_token}'\")\n\n# Output:\n#   Position 0 ('The') -&gt; predicts: ' capital'\n#   Position 1 (' capital') -&gt; predicts: ' of'\n#   Position 2 (' of') -&gt; predicts: ' France'\n#   Position 3 (' France') -&gt; predicts: ' is'\n</code></pre>"},{"location":"examples/#get-multiple-predictions-per-position","title":"Get Multiple Predictions per Position","text":"<pre><code>text = \"Machine learning is\"\nresults = model.logit_lens(text, top_k=3, layers=[10, 15])\n\nfor layer_idx in [10, 15]:\n    print(f\"\\nLayer {layer_idx} - last position top 3:\")\n    last_pos_preds = results[layer_idx][-1]  # All predictions at last position\n\n    for i, (token_id, score, token_str) in enumerate(last_pos_preds, 1):\n        print(f\"  {i}. '{token_str}' (score: {score:.2f})\")\n</code></pre>"},{"location":"examples/#visualizing-with-heatmaps","title":"Visualizing with Heatmaps","text":"<p>Generate a visual heatmap showing what each layer predicts at each position:</p> <pre><code># Create visualization\nresults = model.logit_lens(\n    \"The Eiffel Tower is located in the city of\",\n    plot=True,\n    max_display_tokens=15,  # Show last 15 tokens\n    figsize=(16, 10)\n)\n\n# The heatmap shows:\n#   - X-axis: Input token positions (bottom row shows input tokens)\n#   - Y-axis: Model layers (Layer 0 to Layer N)\n#   - Cell values: Top predicted token at each (layer, position)\n#   - Colors: Different predictions shown with different colors\n</code></pre> <p>How to read the visualization:</p> <ol> <li>Bottom row (x-axis labels): The actual input tokens</li> <li>Left column (y-axis labels): Layer numbers</li> <li>Each cell: What that layer predicts after seeing tokens up to that position</li> <li>Last column: Shows how the final prediction evolves through layers</li> <li>Colors: Same color = same prediction across different positions/layers</li> </ol> <p>Customization options:</p> <pre><code># Show all layers\nresults = model.logit_lens(\n    \"Machine learning is fascinating\",\n    plot=True,\n    layers=list(range(16)),  # All 16 layers\n    figsize=(18, 12),\n    cmap='viridis'  # Try: 'plasma', 'inferno', 'cividis', etc.\n)\n\n# Focus on specific layers\nresults = model.logit_lens(\n    \"The quick brown fox\",\n    plot=True,\n    layers=[0, 4, 8, 12, 15],  # Early, middle, late layers\n    max_display_tokens=10,  # Show last 10 tokens only\n    figsize=(14, 8)\n)\n\n# Handle long sequences\nresults = model.logit_lens(\n    \"Very long input text that has many tokens...\",\n    plot=True,\n    max_display_tokens=20,  # Automatically shows last 20 tokens\n    figsize=(20, 10)\n)\n</code></pre> <p>Note: Requires matplotlib: <code>pip install matplotlib</code></p>"},{"location":"examples/#analyzing-different-token-positions","title":"Analyzing Different Token Positions","text":"<p>Look at predictions for specific positions in the sequence:</p> <pre><code>text = \"The capital of France is Paris\"\ntokens = model.encode(text)\n\n# Analyze what the model predicts at \"France\"\nfor i, token_id in enumerate(tokens):\n    if \"France\" in model.token_to_str(token_id):\n        france_position = i\n        break\n\n# Run logit lens at the France position\nresults = model.logit_lens(\n    text,\n    position=france_position,\n    top_k=3,\n    layers=[10, 12, 14, 15]\n)\n\nprint(f\"What model predicts at 'France' position ({france_position}):\\n\")\nfor layer_idx, predictions in results.items():\n    print(f\"Layer {layer_idx}:\")\n    for token_id, score, token_str in predictions:\n        print(f\"  '{token_str}': {score:.2f}\")\n</code></pre>"},{"location":"examples/#comparing-predictions-across-prompts","title":"Comparing Predictions Across Prompts","text":"<p>Analyze how different prompts affect layer predictions:</p> <pre><code>prompts = [\n    \"The capital of France is\",\n    \"The capital of Germany is\",\n    \"The capital of Italy is\"\n]\n\nfor prompt in prompts:\n    # Use position=-1 to analyze only the last token\n    results = model.logit_lens(prompt, layers=[15], top_k=1, position=-1)\n\n    # Get top prediction from layer 15 at the last position\n    token_id, score, token_str = results[15][0][0]\n    print(f\"'{prompt}' -&gt; '{token_str}' ({score:.1f})\")\n\n# Output:\n# 'The capital of France is' -&gt; ' Paris' (19.0)\n# 'The capital of Germany is' -&gt; ' Berlin' (18.5)\n# 'The capital of Italy is' -&gt; ' Rome' (17.8)\n</code></pre>"},{"location":"examples/#tuned-lens","title":"Tuned Lens","text":"<p>The tuned lens technique (Belrose et al., 2023) improves on the logit lens by learning small affine transformations for each layer that correct for coordinate system mismatches between layers.</p>"},{"location":"examples/#training-a-tuned-lens","title":"Training a Tuned Lens","text":"<pre><code>from mlxterp import InterpretableModel\n\n# Load model\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Prepare training data - any text corpus works\ntraining_texts = [\n    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n    \"Machine learning is a branch of artificial intelligence.\",\n    \"Python is a popular programming language for data science.\",\n    # Add more diverse training texts...\n]\n\n# Train tuned lens (takes a few minutes depending on dataset size)\ntuned_lens = model.train_tuned_lens(\n    dataset=training_texts,\n    num_steps=250,\n    save_path=\"my_tuned_lens.npz\",  # Auto-saves config + weights\n    verbose=True\n)\n</code></pre>"},{"location":"examples/#loading-a-pre-trained-tuned-lens","title":"Loading a Pre-trained Tuned Lens","text":"<pre><code># Load previously trained tuned lens\ntuned_lens = model.load_tuned_lens(\"my_tuned_lens.npz\")\n</code></pre>"},{"location":"examples/#using-the-tuned-lens","title":"Using the Tuned Lens","text":"<pre><code># Apply tuned lens (similar API to logit_lens)\nresults = model.tuned_logit_lens(\n    \"The capital of France is\",\n    tuned_lens,\n    layers=[0, 5, 10, 15]\n)\n\n# Print predictions at last position for each layer\nfor layer_idx in sorted(results.keys()):\n    top_pred = results[layer_idx][-1][0]  # Last position, top prediction\n    token_str = top_pred[2]\n    score = top_pred[1]\n    print(f\"Layer {layer_idx:2d}: '{token_str}' (score: {score:.2f})\")\n</code></pre>"},{"location":"examples/#comparing-logit-lens-vs-tuned-lens","title":"Comparing Logit Lens vs Tuned Lens","text":"<pre><code>text = \"The capital of France is\"\n\n# Regular logit lens\nregular = model.logit_lens(text, layers=[0, 5, 10, 15])\n\n# Tuned lens\ntuned = model.tuned_logit_lens(text, tuned_lens, layers=[0, 5, 10, 15])\n\nprint(\"Comparison: Logit Lens vs Tuned Lens\")\nprint(\"-\" * 45)\nfor layer_idx in [0, 5, 10, 15]:\n    reg_pred = regular[layer_idx][-1][0][2]  # Last pos, top pred\n    tun_pred = tuned[layer_idx][-1][0][2]\n    match = \"=\" if reg_pred == tun_pred else \"!\"\n    print(f\"Layer {layer_idx:2d}: Regular='{reg_pred:&gt;10}' | Tuned='{tun_pred:&gt;10}' {match}\")\n\n# Tuned lens often produces more accurate predictions in early layers\n</code></pre>"},{"location":"examples/#visualizing-tuned-lens-results","title":"Visualizing Tuned Lens Results","text":"<pre><code># Generate heatmap visualization\nresults = model.tuned_logit_lens(\n    \"The Eiffel Tower is located in the city of\",\n    tuned_lens,\n    plot=True,\n    max_display_tokens=15,\n    figsize=(16, 10)\n)\n</code></pre> <p>Reference: Belrose et al., \"Eliciting Latent Predictions from Transformers with the Tuned Lens\" (https://arxiv.org/abs/2303.08112)</p>"},{"location":"examples/#activation-collection","title":"Activation Collection","text":""},{"location":"examples/#collecting-specific-layers","title":"Collecting Specific Layers","text":"<pre><code>from mlxterp import get_activations\n\n# Collect activations from multiple prompts\nprompts = [\n    \"The quick brown fox\",\n    \"Hello world\",\n    \"Machine learning is\"\n]\n\nactivations = get_activations(\n    model,\n    prompts=prompts,\n    layers=[3, 8, 12],\n    positions=-1  # Last token position\n)\n\n# Access activations\nprint(f\"Layer 3 activations: {activations['layer_3'].shape}\")\n# Output: (3, hidden_dim) - batch of 3 prompts\n</code></pre>"},{"location":"examples/#multiple-token-positions","title":"Multiple Token Positions","text":"<pre><code># Get first and last token activations\nactivations = get_activations(\n    model,\n    prompts=\"Hello world\",\n    layers=[5],\n    positions=[0, -1]  # First and last tokens\n)\n\nprint(f\"Shape: {activations['layer_5'].shape}\")\n# Output: (1, 2, hidden_dim) - 1 prompt, 2 positions\n</code></pre>"},{"location":"examples/#batch-processing-large-datasets","title":"Batch Processing Large Datasets","text":"<pre><code>from mlxterp import batch_get_activations\n\n# Process 1000 prompts efficiently\nlarge_dataset = [f\"Sample prompt {i}\" for i in range(1000)]\n\nactivations = batch_get_activations(\n    model,\n    prompts=large_dataset,\n    layers=[3, 8, 12],\n    batch_size=32  # Process 32 at a time\n)\n\nprint(f\"Total activations: {activations['layer_3'].shape}\")\n# Output: (1000, hidden_dim)\n</code></pre>"},{"location":"examples/#interventions","title":"Interventions","text":""},{"location":"examples/#basic-interventions","title":"Basic Interventions","text":"<pre><code>from mlxterp import interventions as iv\n\n# Zero out a layer\nwith model.trace(\"Test input\", interventions={\"layers.4\": iv.zero_out}):\n    output = model.output.save()\n\n# Scale activations\nwith model.trace(\"Test input\", interventions={\"layers.3\": iv.scale(0.5)}):\n    output = model.output.save()\n\n# Clamp values\nwith model.trace(\"Test input\", interventions={\"layers.5\": iv.clamp(-1.0, 1.0)}):\n    output = model.output.save()\n\n# Add noise\nwith model.trace(\"Test input\", interventions={\"layers.2\": iv.noise(std=0.1)}):\n    output = model.output.save()\n</code></pre>"},{"location":"examples/#steering-vectors","title":"Steering Vectors","text":"<pre><code>import mlx.core as mx\nfrom mlxterp import interventions as iv\n\n# Create a steering vector\nhidden_dim = 4096  # Model hidden dimension\nsteering_vector = mx.random.normal((hidden_dim,))\n\n# Apply steering to multiple layers\ninterventions_dict = {\n    \"layers.5\": iv.add_vector(steering_vector * 1.0),\n    \"layers.6\": iv.add_vector(steering_vector * 1.5),\n    \"layers.7\": iv.add_vector(steering_vector * 2.0),\n}\n\nwith model.trace(\"Original prompt\", interventions=interventions_dict):\n    steered_output = model.output.save()\n</code></pre>"},{"location":"examples/#custom-interventions","title":"Custom Interventions","text":"<pre><code>import mlx.core as mx\n\ndef my_custom_intervention(activation: mx.array) -&gt; mx.array:\n    \"\"\"Apply custom transformation\"\"\"\n    # Example: ReLU clipping\n    return mx.maximum(activation, 0.0)\n\nwith model.trace(\"Test\", interventions={\"layers.3\": my_custom_intervention}):\n    output = model.output.save()\n</code></pre>"},{"location":"examples/#composed-interventions","title":"Composed Interventions","text":"<pre><code>from mlxterp import interventions as iv\n\n# Chain multiple interventions\ncombined = iv.compose() \\\n    .add(iv.scale(0.8)) \\\n    .add(iv.noise(std=0.05)) \\\n    .add(iv.clamp(-3.0, 3.0)) \\\n    .build()\n\nwith model.trace(\"Test\", interventions={\"layers.4\": combined}):\n    output = model.output.save()\n</code></pre>"},{"location":"examples/#activation-patching","title":"Activation Patching","text":"<p>Complete Guide Available</p> <p>For a comprehensive guide on activation patching including theory, interpretation, and common pitfalls, see the Activation Patching Guide.</p>"},{"location":"examples/#simple-one-line-activation-patching","title":"Simple One-Line Activation Patching","text":"<p>Use the built-in helper to automatically test all layers:</p> <pre><code># Find which MLPs are important - that's it!\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    plot=True\n)\n\n# Results show:\n# Layer  0: +43.1% recovery  \u2190 Very important!\n# Layer 10: -23.5% recovery  \u2190 Encodes corruption\n</code></pre> <p>Result interpretation: - Positive % (e.g., +40%): Layer is important - patching helps recover clean output - Negative % (e.g., -20%): Layer encodes the corruption - patching makes it worse - Near 0%: Layer not relevant for this task</p>"},{"location":"examples/#test-different-components","title":"Test Different Components","text":"<pre><code># Test MLPs\nmlp_results = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\"\n)\n\n# Test attention\nattn_results = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"self_attn\"\n)\n\n# Test specific sub-components\ngate_results = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp.gate_proj\"  # or \"mlp.up_proj\", \"self_attn.q_proj\", etc.\n)\n\n# Get most important layer\nsorted_layers = sorted(mlp_results.items(), key=lambda x: x[1], reverse=True)\nprint(f\"Most important: Layer {sorted_layers[0][0]} ({sorted_layers[0][1]:.1f}% recovery)\")\n</code></pre>"},{"location":"examples/#choosing-the-right-metric","title":"Choosing the Right Metric","text":"<p>For large vocabulary models (&gt; 100k tokens), use the <code>mse</code> metric for numerical stability:</p> <pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\n# Load large vocabulary model (e.g., Qwen with 151k tokens)\nbase_model, tokenizer = load('mlx-community/Qwen3-30B-A3B-Thinking-2507-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\nprint(f\"Vocabulary size: {model.vocab_size}\")  # 151,643 tokens!\n\n# Use MSE metric for stability\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    metric=\"mse\",  # \u2190 Important for large vocab models!\n    layers=[0, 10, 20, 30, 40, 47],  # Test subset\n    plot=True\n)\n\n# Results:\n# Layer 10: 17.9% recovery  \u2190 Most important!\n# Layer 30:  7.5% recovery\n# Layer  0: -298.6% recovery  \u2190 Encodes corruption\n</code></pre> <p>Metric recommendations: - &lt; 50k vocab: Use <code>metric=\"l2\"</code> (default) - 50k - 100k vocab: Use <code>metric=\"l2\"</code> or <code>metric=\"cosine\"</code> - &gt; 100k vocab: Use <code>metric=\"mse\"</code> or <code>metric=\"cosine\"</code></p> <p>Why it matters: With 150k vocabulary, computing L2 distance over 150k logits can overflow to <code>inf</code>, resulting in <code>nan</code> recovery percentages. MSE averages instead of summing, preventing overflow.</p>"},{"location":"examples/#manual-patching-advanced","title":"Manual Patching (Advanced)","text":"<p>If you need more control, you can still do manual patching:</p> <pre><code>import mlx.core as mx\nfrom mlxterp import interventions as iv\n\n# Get clean activation\nwith model.trace(\"Paris is the capital of France\") as trace:\n    clean_mlp = trace.activations[\"model.model.layers.10.mlp\"]\n\nmx.eval(clean_mlp)\n\n# Patch into corrupted run\nwith model.trace(\"London is the capital of France\",\n                interventions={\"layers.10.mlp\": iv.replace_with(clean_mlp)}):\n    patched_output = model.output.save()\n</code></pre>"},{"location":"examples/#available-components","title":"Available Components","text":"<p>You can patch any captured component:</p> <ul> <li>Full components: <code>\"mlp\"</code>, <code>\"self_attn\"</code></li> <li>MLP sub-components: <code>\"mlp.gate_proj\"</code>, <code>\"mlp.up_proj\"</code>, <code>\"mlp.down_proj\"</code></li> <li>Attention sub-components: <code>\"self_attn.q_proj\"</code>, <code>\"self_attn.k_proj\"</code>, <code>\"self_attn.v_proj\"</code>, <code>\"self_attn.o_proj\"</code></li> </ul>"},{"location":"examples/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/#attention-pattern-analysis","title":"Attention Pattern Analysis","text":"<pre><code>with model.trace(\"Analyze this text\") as trace:\n    # Collect all attention outputs (use self_attn for mlx-lm models)\n    attention_outputs = []\n    for i in range(len(model.layers)):\n        if hasattr(model.layers[i], 'self_attn'):\n            attn = model.layers[i].self_attn.output.save()\n            attention_outputs.append(attn)\n\n# Analyze attention patterns\nfor i, attn in enumerate(attention_outputs):\n    print(f\"Layer {i} attention: {attn.shape}\")\n</code></pre>"},{"location":"examples/#residual-stream-analysis","title":"Residual Stream Analysis","text":"<pre><code># Trace the residual stream through the network\nwith model.trace(\"Input text\") as trace:\n    residuals = []\n    for i in range(len(model.layers)):\n        # Save layer output (which includes residual)\n        layer_out = model.layers[i].output.save()\n        residuals.append(layer_out)\n\n# Analyze how information flows\nfor i in range(len(residuals) - 1):\n    diff = mx.linalg.norm(residuals[i+1] - residuals[i])\n    print(f\"Change from layer {i} to {i+1}: {diff:.4f}\")\n</code></pre>"},{"location":"examples/#gradient-based-analysis","title":"Gradient-Based Analysis","text":"<pre><code>import mlx.core as mx\nimport mlx.nn as nn\n\n# Get activations\nwith model.trace(\"Sample text\") as trace:\n    target_activation = model.layers[8].output.save()\n\n# Compute gradient w.r.t. input\ndef loss_fn(x):\n    with model.trace(x):\n        act = model.layers[8].output.save()\n    return mx.mean((act - target_activation) ** 2)\n\n# Analyze gradient\ngrad_fn = mx.grad(loss_fn)\ninput_grad = grad_fn(input_tokens)\nprint(f\"Input gradient shape: {input_grad.shape}\")\n</code></pre>"},{"location":"examples/#working-with-different-model-architectures","title":"Working with Different Model Architectures","text":""},{"location":"examples/#gpt-style-models","title":"GPT-Style Models","text":"<pre><code># GPT-2 uses 'h' for layers\ngpt2_model = load_gpt2()  # Your loading function\nmodel = InterpretableModel(gpt2_model, layer_attr=\"h\")\n\nwith model.trace(input_text):\n    layer_3 = model.layers[3].output.save()\n</code></pre>"},{"location":"examples/#custom-layer-names","title":"Custom Layer Names","text":"<pre><code># Model with nested structure\ncustom_model = load_custom_model()\nmodel = InterpretableModel(custom_model, layer_attr=\"transformer.blocks\")\n\nwith model.trace(input_text):\n    block_5 = model.layers[5].output.save()\n</code></pre>"},{"location":"examples/#debugging-and-inspection","title":"Debugging and Inspection","text":""},{"location":"examples/#inspecting-model-structure","title":"Inspecting Model Structure","text":"<pre><code># List all modules in the model\nfor name, module in model.named_modules():\n    print(f\"{name}: {type(module).__name__}\")\n\n# Check number of layers\nprint(f\"Total layers: {len(model.layers)}\")\n\n# Access model parameters\nparams = model.parameters()\nprint(f\"Total parameters: {sum(p.size for p in params.values())}\")\n</code></pre>"},{"location":"examples/#tracking-activation-statistics","title":"Tracking Activation Statistics","text":"<pre><code>import mlx.core as mx\n\nwith model.trace(\"Test input\") as trace:\n    stats = {}\n    for i in range(len(model.layers)):\n        act = model.layers[i].output.save()\n        stats[f\"layer_{i}\"] = {\n            \"mean\": float(mx.mean(act)),\n            \"std\": float(mx.std(act)),\n            \"max\": float(mx.max(act)),\n            \"min\": float(mx.min(act)),\n        }\n\n# Print statistics\nfor layer, stat in stats.items():\n    print(f\"{layer}: mean={stat['mean']:.3f}, std={stat['std']:.3f}\")\n</code></pre>"},{"location":"examples/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/#batch-processing","title":"Batch Processing","text":"<pre><code># Good: Process in batches\nfrom mlxterp import batch_get_activations\n\nactivations = batch_get_activations(\n    model,\n    prompts=large_list,\n    layers=[3, 8],\n    batch_size=32\n)\n\n# Avoid: Loading everything at once\n# with model.trace(large_list):  # May run out of memory\n#     acts = model.layers[3].output.save()\n</code></pre>"},{"location":"examples/#selective-activation-saving","title":"Selective Activation Saving","text":"<pre><code># Good: Only save what you need\nwith model.trace(input_text):\n    important_layers = [3, 8, 12]\n    acts = {i: model.layers[i].output.save() for i in important_layers}\n\n# Avoid: Saving everything\n# with model.trace(input_text):\n#     all_acts = [model.layers[i].output.save() for i in range(100)]\n</code></pre>"},{"location":"examples/#memory-management","title":"Memory Management","text":"<pre><code>import mlx.core as mx\n\n# Process in chunks and clear cache\nresults = []\nfor chunk in chunks(large_dataset, size=100):\n    acts = get_activations(model, chunk, layers=[8])\n    results.append(acts)\n    mx.eval(acts)  # Force evaluation\n    # Results are computed, can process them\n\n# Concatenate at the end\nfinal_results = mx.concatenate(results, axis=0)\n</code></pre>"},{"location":"examples/#custom-model-support","title":"Custom Model Support","text":"<p>mlxterp works with any MLX model architecture through automatic module discovery.</p>"},{"location":"examples/#default-support-no-configuration","title":"Default Support (No Configuration)","text":"<p>Models like mlx-lm (Llama, Mistral, Qwen) work out of the box:</p> <pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\n# Auto-detection works for standard models\nbase_model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\n# logit_lens and get_token_predictions work automatically\nresults = model.logit_lens(\"Hello world\")\n</code></pre>"},{"location":"examples/#gpt-2-style-models","title":"GPT-2 Style Models","text":"<p>Models with different attribute names (wte, ln_f, lm_head) are also auto-detected:</p> <pre><code># GPT-2 style models work automatically\nmodel = InterpretableModel(gpt2_model, tokenizer=tokenizer, layer_attr=\"h\")\nresults = model.logit_lens(\"Hello world\")\n</code></pre>"},{"location":"examples/#custom-model-paths","title":"Custom Model Paths","text":"<p>For models with non-standard attribute names, use constructor overrides:</p> <pre><code># Custom model with unusual attribute names\nmodel = InterpretableModel(\n    custom_model,\n    tokenizer=tokenizer,\n    embedding_path=\"my_custom_embeddings\",  # Override embedding location\n    norm_path=\"my_final_layer_norm\",        # Override final norm location\n    lm_head_path=\"my_output_projection\"     # Override lm_head location\n)\n\n# Now logit_lens and get_token_predictions work\nresults = model.logit_lens(\"Hello world\")\n</code></pre>"},{"location":"examples/#models-without-final-norm","title":"Models Without Final Norm","text":"<p>Some models don't have a final layer normalization:</p> <pre><code># Skip normalization for models without it\nresults = model.logit_lens(\"Hello world\", skip_norm=True)\n\n# Or provide a custom norm at call time\nresults = model.logit_lens(\"Hello world\", final_norm=my_custom_norm)\n</code></pre>"},{"location":"examples/#method-level-overrides","title":"Method-Level Overrides","text":"<p>Override components at call time without changing the model configuration:</p> <pre><code># Override lm_head for a specific call\npredictions = model.get_token_predictions(\n    hidden_state,\n    top_k=5,\n    lm_head=custom_lm_head  # Use this layer for this call\n)\n\n# Override embedding for weight-tied projection\npredictions = model.get_token_predictions(\n    hidden_state,\n    top_k=5,\n    embedding_layer=custom_embedding  # Use this for weight-tied projection\n)\n</code></pre>"},{"location":"examples/#fallback-chains","title":"Fallback Chains","text":"<p>mlxterp tries multiple paths to find components:</p> Component Tried Paths Embedding <code>model.embed_tokens</code>, <code>model.model.embed_tokens</code>, <code>embed_tokens</code>, <code>tok_embeddings</code>, <code>wte</code> Final Norm <code>model.norm</code>, <code>model.model.norm</code>, <code>norm</code>, <code>ln_f</code>, <code>model.ln_f</code> LM Head <code>lm_head</code>, <code>model.lm_head</code>, <code>output</code> (falls back to embedding if not found)"},{"location":"examples/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Quick Start - Getting started guide</li> <li>GitHub Examples - More examples</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS with Apple Silicon (M1/M2/M3/M4)</li> <li>Python 3.9 or higher</li> <li>MLX framework</li> </ul>"},{"location":"installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver written in Rust.</p>"},{"location":"installation/#install-uv","title":"Install uv","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or with pip\npip install uv\n</code></pre>"},{"location":"installation/#install-mlxterp","title":"Install mlxterp","text":"<pre><code># Clone the repository\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Create virtual environment and install dependencies\nuv sync\n\n# Activate the environment\nsource .venv/bin/activate\n\n# Verify installation\npython -c \"import mlxterp; print(mlxterp.__version__)\"\n</code></pre>"},{"location":"installation/#using-pip","title":"Using pip","text":""},{"location":"installation/#standard-installation","title":"Standard Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Create virtual environment (optional but recommended)\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install in development mode\npip install -e .\n\n# Or install normally\npip install .\n</code></pre>"},{"location":"installation/#with-optional-dependencies","title":"With Optional Dependencies","text":"<pre><code># Install with visualization tools\npip install -e \".[viz]\"\n\n# Install with development tools\npip install -e \".[dev]\"\n\n# Install everything\npip install -e \".[dev,viz]\"\n</code></pre>"},{"location":"installation/#from-pypi-future","title":"From PyPI (Future)","text":"<p>Once published to PyPI:</p> <pre><code>pip install mlxterp\n\n# With extras\npip install mlxterp[viz]\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import mlxterp\nimport mlx.core as mx\nimport mlx.nn as nn\n\n# Check version\nprint(f\"mlxterp version: {mlxterp.__version__}\")\n\n# Test basic functionality\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = [nn.Linear(32, 32) for _ in range(3)]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nmodel = mlxterp.InterpretableModel(SimpleModel())\nprint(f\"Model has {len(model.layers)} layers\")\nprint(\"\u2713 Installation successful!\")\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":""},{"location":"installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>mlx (&gt;=0.0.1): Apple's ML framework</li> <li>Python (&gt;=3.9): Minimum Python version</li> </ul>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#visualization-viz","title":"Visualization (<code>viz</code>)","text":"<ul> <li>matplotlib (&gt;=3.5): For plotting</li> <li>plotly (&gt;=5.0): Interactive visualizations</li> </ul>"},{"location":"installation/#development-dev","title":"Development (<code>dev</code>)","text":"<ul> <li>pytest (&gt;=7.0): Testing framework</li> <li>black (&gt;=22.0): Code formatting</li> <li>flake8 (&gt;=4.0): Linting</li> <li>mypy (&gt;=0.950): Type checking</li> <li>mkdocs-material: Documentation</li> <li>mkdocstrings: API documentation generation</li> </ul>"},{"location":"installation/#development-setup","title":"Development Setup","text":"<p>For contributing to mlxterp:</p>"},{"location":"installation/#using-uv","title":"Using uv","text":"<pre><code># Clone your fork\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Install with all dependencies\nuv sync --all-extras\n\n# Run tests\nuv run pytest\n\n# Build docs\nuv run mkdocs serve\n</code></pre>"},{"location":"installation/#using-pip_1","title":"Using pip","text":"<pre><code># Clone your fork\ngit clone https://github.com/coairesearch/mlxterp\ncd mlxterp\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev,viz]\"\n\n# Run tests\npytest\n\n# Build docs\nmkdocs serve\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#mlx-not-found","title":"MLX Not Found","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'mlx'</code></p> <p>Solution: Install MLX:</p> <pre><code>pip install mlx\n</code></pre>"},{"location":"installation/#apple-silicon-required","title":"Apple Silicon Required","text":"<p>Problem: MLX requires Apple Silicon</p> <p>Solution: mlxterp only works on Macs with M1/M2/M3/M4 chips. For Intel Macs or other platforms, consider using PyTorch-based alternatives like nnsight or TransformerLens.</p>"},{"location":"installation/#python-version","title":"Python Version","text":"<p>Problem: <code>Python 3.9+ required</code></p> <p>Solution: Upgrade Python:</p> <pre><code># Using Homebrew\nbrew install python@3.11\n\n# Or download from python.org\n</code></pre>"},{"location":"installation/#permission-errors","title":"Permission Errors","text":"<p>Problem: Permission denied during installation</p> <p>Solution: Use a virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e .\n</code></pre>"},{"location":"installation/#uninstalling","title":"Uninstalling","text":""},{"location":"installation/#with-uv","title":"With uv","text":"<pre><code>cd mlxterp\nuv pip uninstall mlxterp\n</code></pre>"},{"location":"installation/#with-pip","title":"With pip","text":"<pre><code>pip uninstall mlxterp\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Examples - Learn by example</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"license/","title":"License","text":"<p>mlxterp is licensed under the MIT License.</p>"},{"location":"license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2025 Sigurd Schacht</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>mlxterp depends on several third-party libraries, each with their own licenses:</p>"},{"location":"license/#mlx","title":"MLX","text":"<ul> <li>License: MIT License</li> <li>Copyright: \u00a9 2023 Apple Inc.</li> <li>Link: https://github.com/ml-explore/mlx</li> </ul>"},{"location":"license/#python","title":"Python","text":"<ul> <li>License: PSF License</li> <li>Link: https://www.python.org/psf/license/</li> </ul>"},{"location":"license/#acknowledgments","title":"Acknowledgments","text":"<p>mlxterp is inspired by:</p> <ul> <li>nnsight - Generic model wrapping approach</li> <li>nnterp - Clean API design</li> <li>TransformerLens - Comprehensive interpretability features</li> </ul> <p>We thank the authors of these projects for their contributions to the mechanistic interpretability community.</p>"},{"location":"dev/","title":"Development Documentation","text":"<p>This directory contains internal development documentation and design notes that are useful for contributors but not part of the main user-facing documentation.</p>"},{"location":"dev/#files","title":"Files","text":"<ul> <li>IMPLEMENTATION_SUMMARY.md - Summary of core implementation details and architecture decisions</li> <li>PERFORMANCE_NOTES.md - Performance optimization notes and benchmarks</li> <li>PHASE2_COMPLETE.md - SAE Phase 2 feature analysis completion summary</li> <li>SAE_PHASE2_SUMMARY.md - Detailed summary of SAE Phase 2 implementation</li> <li>SAELENS_COMPARISON.md - Comparison between mlxterp and SAELens approaches</li> </ul>"},{"location":"dev/#for-contributors","title":"For Contributors","text":"<p>If you're contributing to mlxterp, these documents provide context about: - Design decisions and rationale - Implementation patterns to follow - Performance considerations - Feature development roadmaps</p>"},{"location":"dev/#for-users","title":"For Users","text":"<p>User-facing documentation is available at: - Main docs: <code>docs/</code> directory - API reference: Generated from docstrings - Guides: <code>docs/guides/</code> directory</p> <p>Build and view the documentation with: <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"dev/PERFORMANCE_NOTES/","title":"SAE Training Performance Notes","text":""},{"location":"dev/PERFORMANCE_NOTES/#training-speed-expectations","title":"Training Speed Expectations","text":""},{"location":"dev/PERFORMANCE_NOTES/#streaming-mode-current-approach","title":"Streaming Mode (Current Approach)","text":"<p>Speed: 1.5-2.0 it/s on 8B models Memory: Low (no activation caching) Use case: Large datasets that don't fit in memory</p> <p>Why is it slow? Each iteration requires: 1. Model forward pass (~500ms for 8B model) 2. SAE training step (~100ms) 3. Total: ~600ms/it = 1.6 it/s</p> <p>This is EXPECTED and CORRECT for streaming mode.</p>"},{"location":"dev/PERFORMANCE_NOTES/#pre-collected-activations-mode","title":"Pre-Collected Activations Mode","text":"<p>Speed: 80-200 it/s (50-100x faster!) Memory: High (all activations in RAM) Use case: Smaller datasets (&lt;10GB activations)</p> <p>Why is it fast? - Model traced ONCE upfront - Each iteration is just SAE forward/backward - No repeated model traces</p>"},{"location":"dev/PERFORMANCE_NOTES/#dead-feature-behavior","title":"Dead Feature Behavior","text":""},{"location":"dev/PERFORMANCE_NOTES/#timeline","title":"Timeline","text":"Step Range Dead Features Why 0-1000 95-99% Ghost grads not active yet 1000-5000 85-95% Ghost grads starting to work 5000-10000 70-85% Ghost grads fully active 10000+ 60-75% Equilibrium reached"},{"location":"dev/PERFORMANCE_NOTES/#ghost-gradient-activation","title":"Ghost Gradient Activation","text":"<p>Ghost gradients don't activate immediately because: 1. Need <code>dead_feature_window</code> steps of history (default: 1000) 2. Features need time to prove they're actually dead 3. Early training is unstable, better to let network settle</p> <p>OLD default: 5000 steps (very conservative) NEW default: 1000 steps (faster activation)</p>"},{"location":"dev/PERFORMANCE_NOTES/#recommended-configs","title":"Recommended Configs","text":""},{"location":"dev/PERFORMANCE_NOTES/#fast-testing-small-model-pre-collected","title":"Fast Testing (small model, pre-collected)","text":"<pre><code>config = SAEConfig(\n    expansion_factor=16,\n    k=64,\n    num_epochs=1,\n    batch_size=256,\n    dead_feature_window=500,  # Start ghost grads early\n)\n\n# Pre-collect activations\nactivations = collect_activations(model, texts)  # Fast!\nsae = trainer.train(activations)  # 80-200 it/s\n</code></pre>"},{"location":"dev/PERFORMANCE_NOTES/#production-large-model-streaming","title":"Production (large model, streaming)","text":"<pre><code>config = SAEConfig(\n    expansion_factor=32,\n    k=128,\n    num_epochs=3,\n    batch_size=64,  # Smaller for memory\n    text_batch_size=32,  # Stream in chunks\n    dead_feature_window=1000,  # Reasonable balance\n    use_ghost_grads=True,\n)\n\n# Streaming mode\nsae = model.train_sae(\n    dataset=texts,  # 1.5-2.0 it/s (EXPECTED)\n    config=config\n)\n</code></pre>"},{"location":"dev/PERFORMANCE_NOTES/#trade-offs","title":"Trade-offs","text":"Approach Speed Memory Use When Streaming 1-2 it/s Low &gt;10k texts, large models Pre-collected 80-200 it/s High &lt;5k texts, small models"},{"location":"dev/PERFORMANCE_NOTES/#current-training-analysis","title":"Current Training Analysis","text":"<p>Your current training: - Model: Qwen3-8B (8B params) - Dataset: 10,000 texts - Mode: Streaming - Speed: 1.8 it/s \u2713 EXPECTED - Dead features: 98% at step 200 - Ghost grads activate at step 1000+</p> <p>This is working as designed! Ghost gradients will reduce dead features once they activate.</p>"},{"location":"dev/PHASE2_COMPLETE/","title":"Phase 2: SAE Feature Analysis - COMPLETE \u2705","text":""},{"location":"dev/PHASE2_COMPLETE/#whats-been-added","title":"What's Been Added","text":""},{"location":"dev/PHASE2_COMPLETE/#1-feature-analysis-api-integrated-into-interpretablemodel","title":"1. Feature Analysis API (Integrated into <code>InterpretableModel</code>)","text":"<p>Two main methods for analyzing SAE features:</p>"},{"location":"dev/PHASE2_COMPLETE/#get_top_features_for_text","title":"<code>get_top_features_for_text()</code>","text":"<p>Find which features activate for specific text: <pre><code>features = model.get_top_features_for_text(\n    \"Paris is the capital of France\",\n    sae=sae, layer=10, component=\"mlp\", top_k=10\n)\n# Returns: [(feature_id, activation), ...]\n</code></pre></p>"},{"location":"dev/PHASE2_COMPLETE/#get_top_texts_for_feature","title":"<code>get_top_texts_for_feature()</code>","text":"<p>Find texts where a feature activates strongly: <pre><code>examples = model.get_top_texts_for_feature(\n    feature_id=1234,\n    sae=sae,\n    texts=dataset,\n    layer=10, component=\"mlp\", top_k=20\n)\n# Returns: [(text, activation, position), ...]\n</code></pre></p>"},{"location":"dev/PHASE2_COMPLETE/#2-neuronpedia-style-visualization","title":"2. Neuronpedia-Style Visualization","text":"<p>New visualization module (<code>mlxterp.sae.visualization</code>) with:</p>"},{"location":"dev/PHASE2_COMPLETE/#visualize_feature_activations","title":"<code>visualize_feature_activations()</code>","text":"<p>Display tokens colored by activation strength (like Neuronpedia): <pre><code>from mlxterp.sae import visualize_feature_activations\n\nvisualize_feature_activations(\n    model,\n    \"The Eiffel Tower is in Paris\",\n    sae,\n    layer=10,\n    component=\"mlp\",\n    top_k_features=5,  # Show top 5 features\n    mode=\"auto\"  # Auto-detect: HTML in Jupyter, ANSI in terminal\n)\n</code></pre></p> <p>Output: Tokens highlighted in color: - Blue = Positive activation - Red = Negative activation - Bold/Dark = Stronger activation</p> <p>Display Modes: - <code>mode=\"auto\"</code> (default): Auto-detects Jupyter vs terminal - <code>mode=\"html\"</code>: Force HTML rendering (best for Jupyter notebooks) - <code>mode=\"terminal\"</code>: Force ANSI color codes (for terminal)</p>"},{"location":"dev/PHASE2_COMPLETE/#get_top_activating_tokens","title":"<code>get_top_activating_tokens()</code>","text":"<p>Find which tokens activate a feature most: <pre><code>from mlxterp.sae import get_top_activating_tokens\n\ntop_tokens = get_top_activating_tokens(\n    model, text, sae, layer=10, component=\"mlp\",\n    feature_id=1234, top_k=10\n)\n# Returns: [(token, activation, position), ...]\n</code></pre></p>"},{"location":"dev/PHASE2_COMPLETE/#3-complete-documentation","title":"3. Complete Documentation","text":"<p>New documentation pages:</p> <ul> <li><code>docs/guides/sae_feature_analysis.md</code> - Complete Phase 2 guide</li> <li>API reference for all methods</li> <li>Feature interpretation tips</li> <li>Complete workflow examples</li> <li>Performance considerations</li> <li> <p>Common use cases</p> </li> <li> <p>Updated <code>mkdocs.yml</code> - New guide integrated into navigation</p> </li> </ul>"},{"location":"dev/PHASE2_COMPLETE/#4-example-scripts","title":"4. Example Scripts","text":""},{"location":"dev/PHASE2_COMPLETE/#examplesphase2_feature_analysispy","title":"<code>examples/phase2_feature_analysis.py</code>","text":"<p>Basic feature analysis workflow: - Find top features for texts - Find top texts for features - Understand what features represent</p>"},{"location":"dev/PHASE2_COMPLETE/#examplesneuronpedia_style_vizpy","title":"<code>examples/neuronpedia_style_viz.py</code>","text":"<p>Neuronpedia-style visualization demo: - Visualize top features - Visualize specific features - Show activation values - Find top activating tokens</p>"},{"location":"dev/PHASE2_COMPLETE/#usage-examples","title":"Usage Examples","text":""},{"location":"dev/PHASE2_COMPLETE/#basic-feature-analysis","title":"Basic Feature Analysis","text":"<pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\n# Setup\nmlx_model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\nmodel = InterpretableModel(mlx_model, tokenizer=tokenizer)\nsae = model.load_sae(\"sae_layer10.mlx\")\n\n# What features activate for this text?\nfeatures = model.get_top_features_for_text(\n    \"Machine learning processes data\",\n    sae=sae, layer=10, component=\"mlp\"\n)\n\nfor feat_id, act in features:\n    print(f\"Feature {feat_id}: {act:.3f}\")\n</code></pre>"},{"location":"dev/PHASE2_COMPLETE/#feature-investigation","title":"Feature Investigation","text":"<pre><code>from datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\ntexts = [item[\"text\"] for item in dataset if len(item[\"text\"]) &gt; 50][:1000]\n\n# What does feature 1234 represent?\nexamples = model.get_top_texts_for_feature(\n    feature_id=1234,\n    sae=sae,\n    texts=texts,\n    layer=10,\n    component=\"mlp\",\n    top_k=20\n)\n\nprint(\"Feature 1234 activates on:\")\nfor text, act, pos in examples[:5]:\n    print(f\"  [{act:.3f}] {text[:100]}...\")\n</code></pre>"},{"location":"dev/PHASE2_COMPLETE/#neuronpedia-style-visualization","title":"Neuronpedia-Style Visualization","text":"<pre><code>from mlxterp.sae import visualize_feature_activations\n\n# Visualize top 5 features\nvisualize_feature_activations(\n    model,\n    \"The Eiffel Tower is in Paris, France\",\n    sae,\n    layer=10,\n    component=\"mlp\",\n    top_k_features=5\n)\n\n# Output shows tokens colored by activation:\n# The Eiffel Tower is in Paris , France\n# ^^^ (blue = geographic feature activates)\n#                         ^^^^^ (darker blue = stronger activation)\n</code></pre>"},{"location":"dev/PHASE2_COMPLETE/#your-trained-sae","title":"Your Trained SAE","text":"<p>Your SAE is ready to use with all Phase 2 tools:</p> <p>File: <code>sae_layer23_mlp_10000samples.mlx</code></p> <p>Specs: - Type: BatchTopKSAE - d_model: 4096 - d_hidden: 65,536 (16x expansion) - k: 128 - Layer: 23, Component: MLP - Model: Qwen3-8B-ShiningValiant3-mlx-4Bit</p> <p>Quality (from evaluation): - Reconstruction: Cosine similarity 0.80 (fair) - Dead features: ~94% (high, due to k=128 being very sparse) - Features: Show interpretable patterns</p>"},{"location":"dev/PHASE2_COMPLETE/#quick-test","title":"Quick Test","text":"<p>Try this now:</p> <pre><code># Run basic feature analysis\npython examples/phase2_feature_analysis.py\n\n# Run Neuronpedia-style visualization\npython examples/neuronpedia_style_viz.py\n</code></pre>"},{"location":"dev/PHASE2_COMPLETE/#files-created","title":"Files Created","text":""},{"location":"dev/PHASE2_COMPLETE/#code","title":"Code","text":"<ol> <li><code>mlxterp/sae_mixin.py</code> - Added <code>get_top_features_for_text()</code> and <code>get_top_texts_for_feature()</code></li> <li><code>mlxterp/sae/visualization.py</code> - New visualization module (Neuronpedia-style)</li> <li><code>mlxterp/sae/__init__.py</code> - Exports visualization functions</li> </ol>"},{"location":"dev/PHASE2_COMPLETE/#documentation","title":"Documentation","text":"<ol> <li><code>docs/guides/sae_feature_analysis.md</code> - Complete Phase 2 guide</li> <li><code>mkdocs.yml</code> - Updated navigation</li> </ol>"},{"location":"dev/PHASE2_COMPLETE/#examples","title":"Examples","text":"<ol> <li><code>examples/phase2_feature_analysis.py</code> - Basic feature analysis demo</li> <li><code>examples/neuronpedia_style_viz.py</code> - Visualization demo</li> </ol>"},{"location":"dev/PHASE2_COMPLETE/#summary","title":"Summary","text":"<ol> <li><code>PHASE2_COMPLETE.md</code> - This file</li> </ol>"},{"location":"dev/PHASE2_COMPLETE/#whats-next","title":"What's Next","text":""},{"location":"dev/PHASE2_COMPLETE/#immediate-use","title":"Immediate Use","text":"<ul> <li>Analyze your trained SAE features</li> <li>Understand what concepts it learned</li> <li>Create feature registry documenting findings</li> </ul>"},{"location":"dev/PHASE2_COMPLETE/#future-enhancements-phase-25","title":"Future Enhancements (Phase 2.5+)","text":"<ul> <li>Feature steering: Ablate/amplify features during generation</li> <li>Interactive dashboard: Web-based feature browser</li> <li>Feature clustering: Find similar features automatically</li> <li>Circuit discovery: Map feature interactions across layers</li> </ul>"},{"location":"dev/PHASE2_COMPLETE/#recommended-next-steps","title":"Recommended Next Steps","text":"<ol> <li>Run <code>python examples/neuronpedia_style_viz.py</code> to see visualization</li> <li>Pick interesting features and investigate them</li> <li>Document findings in a feature registry</li> <li>Use insights for interpretability research</li> </ol>"},{"location":"dev/PHASE2_COMPLETE/#documentation_1","title":"Documentation","text":"<p>All documentation is now integrated into mkdocs:</p> <pre><code># Build and serve documentation\nmkdocs serve\n</code></pre> <p>Then visit: http://localhost:8000</p> <p>Navigate to: Guides &amp; Procedures \u2192 SAE Feature Analysis</p>"},{"location":"dev/PHASE2_COMPLETE/#summary_1","title":"Summary","text":"<p>\u2705 Phase 2 is complete and integrated into mlxterp!</p> <p>You now have: - Full-featured SAE analysis API - Neuronpedia-style visualization - Complete documentation - Working examples with your trained SAE</p> <p>Your 16-hour trained SAE is ready for interpretability research!</p>"},{"location":"dev/SAELENS_COMPARISON/","title":"mlxterp vs SAELens: Comprehensive Implementation Comparison","text":""},{"location":"dev/SAELENS_COMPARISON/#executive-summary","title":"Executive Summary","text":"<p>This document compares our mlxterp SAE implementation with SAELens (the leading PyTorch-based SAE framework). We identify 3 critical missing features, 2 architectural differences, and 4 recommended improvements.</p> <p>Overall Assessment: Our implementation is solid for basic TopK SAE training but missing several production-critical features that SAELens has battle-tested over 191 releases.</p>"},{"location":"dev/SAELENS_COMPARISON/#architecture-comparison","title":"Architecture Comparison","text":""},{"location":"dev/SAELENS_COMPARISON/#saelens-state-of-the-art","title":"SAELens (State-of-the-Art)","text":"<p>Supported Architectures: 1. \u2705 BatchTopK - Modern variant (fixes mean L0 across batches, not per-sample) 2. \u2705 JumpReLU - Used by Anthropic &amp; DeepMind's Gemma Scope 3. \u2705 MatryoshkaBatchTopK - Nested reconstruction preventing feature absorption 4. \u2705 Standard L1 SAE - Classic ReLU + L1 penalty 5. \u2705 TopK - Original TopK implementation 6. \u2705 Gated SAE - Alternative L1-based approach</p> <p>Training Infrastructure: - <code>LanguageModelSAETrainingRunner</code> class - Nested configuration: <code>LanguageModelSAERunnerConfig</code> + architecture-specific configs - Pre-tokenized dataset support - Activation caching for rapid hyperparameter experimentation</p>"},{"location":"dev/SAELENS_COMPARISON/#mlxterp-our-implementation","title":"mlxterp (Our Implementation)","text":"<p>Supported Architectures: 1. \u2705 TopK - Standard TopK with per-sample sparsity</p> <p>Training Infrastructure: - <code>SAETrainer</code> class - Single configuration: <code>SAEConfig</code> - Text-based streaming (tokenization on-the-fly) - No activation caching</p>"},{"location":"dev/SAELENS_COMPARISON/#critical-missing-features","title":"Critical Missing Features","text":""},{"location":"dev/SAELENS_COMPARISON/#1-ghost-gradients-for-dead-features","title":"\u274c 1. Ghost Gradients for Dead Features","text":"<p>SAELens: <pre><code>use_ghost_grads: bool = True           # Always recommended\nfeature_sampling_window: int = 1000\ndead_feature_window: int = 5000        # Track dead neurons\n</code></pre></p> <p>How it works: - Tracks which features haven't activated in last N forward passes - Applies \"ghost gradients\" to dead features to revive them - Anthropic found this critical for larger models (though less important for 1L models)</p> <p>Our implementation: <pre><code># \u274c NOT IMPLEMENTED\n# We have:\ndead_neuron_threshold: float = 1e-6\nresample_dead_every: int = 10000\n# But no actual ghost grad or resampling logic!\n</code></pre></p> <p>Impact: We get 90-99% dead features, SAELens gets 60-80%</p> <p>Fix needed: Implement ghost gradient mechanism in trainer.py</p>"},{"location":"dev/SAELENS_COMPARISON/#2-advanced-normalization-strategies","title":"\u274c 2. Advanced Normalization Strategies","text":"<p>SAELens: <pre><code>normalize_activations = \"expected_average_only_in\"  # Recommended\n</code></pre></p> <p>Normalization approaches: 1. expected_average_only_in - Normalize to sqrt(n_dense) L2 norm 2. Layer norm - Standardize to mean=0, std=1 3. None - No normalization</p> <p>Anthropic's approach (from Circuits Updates Feb 2024): - Normalize activation vectors to L2 norm = sqrt(n_dense) - Take sum over dense dimension in MSE loss - Critical for JumpReLU tanh variant</p> <p>Our implementation: <pre><code># \u2705 Basic normalization (mean/std)\nif self.normalize_input:\n    x = (x - self.input_mean) / (self.input_std + 1e-8)\n</code></pre></p> <p>Impact: Our simple mean/std normalization may not be optimal for reconstruction quality</p> <p>Fix needed: Add <code>normalize_activations</code> config option with multiple strategies</p>"},{"location":"dev/SAELENS_COMPARISON/#3-l0l1-sparsity-warmup","title":"\u274c 3. L0/L1 Sparsity Warmup","text":"<p>SAELens: <pre><code>l1_warm_up_steps: int = total_training_steps  # Gradual sparsity increase\nl0_warm_up_steps: int = total_training_steps  # For JumpReLU\n</code></pre></p> <p>How it works: - Gradually increases sparsity penalty from 0 \u2192 target value - Prevents dead features early in training - Gives network time to learn useful representations before enforcing sparsity</p> <p>Our implementation: <pre><code># \u274c NOT IMPLEMENTED\n# We only have LR warmup:\nwarmup_steps: int = 1000\n</code></pre></p> <p>Impact: May explain why our training plateaus early (~1k steps) and has high dead features</p> <p>Fix needed: Add separate sparsity warmup schedule</p>"},{"location":"dev/SAELENS_COMPARISON/#architectural-differences","title":"Architectural Differences","text":""},{"location":"dev/SAELENS_COMPARISON/#1-batchtopk-vs-topk","title":"\ud83d\udd04 1. BatchTopK vs TopK","text":"<p>SAELens's BatchTopK (recommended): - Fixes mean L0 across batch, not per-sample - More stable training - Better feature utilization - Saves as JumpReLU for inference compatibility</p> <p>Our TopK: - Fixes k per sample (traditional approach) - Can lead to variable batch statistics - Simpler but less stable</p> <p>Example: <pre><code># SAELens BatchTopK:\n# Batch of 32 samples, k=64\n# Total active features in batch \u2248 32 \u00d7 64 = 2048\n# But adjusts per-sample k to hit target mean\n\n# Our TopK:\n# Each sample has exactly k=64 active features\n# Total = exactly 32 \u00d7 64 = 2048\n</code></pre></p> <p>Impact: BatchTopK is more robust for production training</p> <p>Recommendation: Implement BatchTopK as default (keep TopK for backward compatibility)</p>"},{"location":"dev/SAELENS_COMPARISON/#2-loss-function-differences","title":"\ud83d\udd04 2. Loss Function Differences","text":"<p>SAELens (for standard L1 SAE): <pre><code>loss = reconstruction_loss + l1_coefficient * l1_penalty\n\n# For JumpReLU tanh:\nloss = recon_loss + l0_coef * l0_penalty + pre_act_coef * pre_act_loss\n</code></pre></p> <p>Reconstruction loss variations: - Standard: <code>mean((x - x_recon) ** 2)</code> - Anthropic: <code>sum((x - x_recon) ** 2, dim=-1)</code> with normalized inputs</p> <p>Our implementation: <pre><code># \u2705 Standard MSE\nrecon_loss = mx.mean((x - x_recon) ** 2)\n\n# \u274c No pre-activation loss\n# \u274c No separate L0 penalty (we use structural TopK)\n</code></pre></p> <p>Impact: Our loss is correct for TopK but missing advanced variants</p> <p>Recommendation: Add configurable loss functions for future architectures</p>"},{"location":"dev/SAELENS_COMPARISON/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"dev/SAELENS_COMPARISON/#1-gradient-clipping-strategy","title":"\u26a0\ufe0f 1. Gradient Clipping Strategy","text":"<p>SAELens: - Uses PyTorch's built-in <code>torch.nn.utils.clip_grad_norm_</code> - Well-tested across many models</p> <p>Our implementation: <pre><code># \u2705 Custom implementation\ndef _clip_gradients(self, grads: dict, max_norm: float) -&gt; dict:\n    # Compute global norm recursively\n    # Scale gradients if norm &gt; max_norm\n</code></pre></p> <p>Status: \u2705 Our implementation looks correct</p>"},{"location":"dev/SAELENS_COMPARISON/#2-learning-rate-scheduling","title":"\u26a0\ufe0f 2. Learning Rate Scheduling","text":"<p>SAELens: <pre><code>lr_warm_up_steps: int = 1000\nlr_decay_steps: int = total_steps\n# Uses cosine decay or linear decay\n</code></pre></p> <p>Our implementation: <pre><code>def _get_lr_schedule(self):\n    def schedule(step):\n        if step &lt; self.config.warmup_steps:\n            # Linear warmup\n            return self.config.learning_rate * (step / self.config.warmup_steps)\n        else:\n            # Constant LR after warmup\n            return self.config.learning_rate\n    return schedule\n</code></pre></p> <p>Issue: \u274c No LR decay! We use constant LR after warmup</p> <p>Impact: May explain training plateau after warmup completes</p> <p>Fix needed: Add LR decay (cosine or linear)</p>"},{"location":"dev/SAELENS_COMPARISON/#3-decoder-norm-constraints","title":"\u26a0\ufe0f 3. Decoder Norm Constraints","text":"<p>SAELens (from research): - \"One of the most common additions being some sort of constraint on decoder vector norms\" - Prevents decoder weights from growing unbounded - Improves feature interpretability</p> <p>Our implementation: <pre><code># \u274c NOT IMPLEMENTED\n# No decoder norm constraints\n</code></pre></p> <p>Impact: Decoder weights may become too large, hurting interpretability</p> <p>Recommendation: Add decoder weight normalization option</p>"},{"location":"dev/SAELENS_COMPARISON/#4-activation-caching","title":"\u26a0\ufe0f 4. Activation Caching","text":"<p>SAELens: - Pre-compute and cache activations for entire dataset - Enables rapid hyperparameter sweeps - Trade-off: disk space vs. compute time</p> <p>Our implementation: <pre><code># \u274c NOT IMPLEMENTED\n# We generate activations on-the-fly (streaming)\n</code></pre></p> <p>Trade-offs: - Our approach (streaming): Lower memory, slower training, good for large datasets - SAELens (caching): Higher memory, faster iteration, good for hyperparameter tuning</p> <p>Recommendation: Add optional activation caching for small-scale experiments</p>"},{"location":"dev/SAELENS_COMPARISON/#data-handling-comparison","title":"Data Handling Comparison","text":""},{"location":"dev/SAELENS_COMPARISON/#saelens","title":"SAELens","text":"<p>Dataset support: - \u2705 Streaming large datasets - \u2705 Pre-tokenized datasets (eliminates tokenization bottleneck) - \u2705 HuggingFace datasets integration - \u2705 Activation caching</p> <p>Optimization: <pre><code>streaming: bool = True\nuse_cached_activations: bool = False  # Optional for hyperparameter tuning\n</code></pre></p>"},{"location":"dev/SAELENS_COMPARISON/#mlxterp","title":"mlxterp","text":"<p>Dataset support: - \u2705 Streaming text datasets - \u2705 HuggingFace datasets (via user code) - \u274c Pre-tokenized datasets - \u274c Activation caching</p> <p>Our streaming optimization: <pre><code>text_batch_size: int = 32  # Process 32 texts at once\n# Reduces model traces, improves GPU utilization\n</code></pre></p> <p>Status: \u2705 Our streaming is efficient, but could add pre-tokenization support</p>"},{"location":"dev/SAELENS_COMPARISON/#configuration-comparison","title":"Configuration Comparison","text":""},{"location":"dev/SAELENS_COMPARISON/#saelens-default-config","title":"SAELens Default Config","text":"<pre><code># For BatchTopK (recommended):\nexpansion_factor: 32          # Larger than ours\nk: 100                        # Similar to ours\nlearning_rate: 3e-4           # Higher than ours\nnormalize_activations: \"expected_average_only_in\"\nuse_ghost_grads: True\nfeature_sampling_window: 1000\ndead_feature_window: 5000\nl1_warm_up_steps: total_steps\nautocast: True\ndtype: \"float32\"\n</code></pre>"},{"location":"dev/SAELENS_COMPARISON/#mlxterp-default-config","title":"mlxterp Default Config","text":"<pre><code># For TopK:\nexpansion_factor: 16          # Conservative\nk: 100                        # Similar\nlearning_rate: 1e-4           # More conservative\nnormalize_input: True         # Basic normalization\nuse_ghost_grads: False        # \u274c NOT IMPLEMENTED\nwarmup_steps: 1000            # Only LR warmup\ngradient_clip: 1.0\n</code></pre> <p>Differences: 1. SAELens uses higher expansion factors (32 vs 16) 2. SAELens uses higher learning rates (3e-4 vs 1e-4) 3. SAELens has ghost grads (critical feature) 4. SAELens has sparsity warmup (we don't) 5. SAELens has LR decay (we don't)</p>"},{"location":"dev/SAELENS_COMPARISON/#recommended-improvements","title":"Recommended Improvements","text":""},{"location":"dev/SAELENS_COMPARISON/#priority-1-critical-for-production","title":"Priority 1: Critical for Production","text":""},{"location":"dev/SAELENS_COMPARISON/#11-implement-ghost-gradients","title":"1.1 Implement Ghost Gradients","text":"<pre><code># Add to SAEConfig:\nuse_ghost_grads: bool = True\nfeature_sampling_window: int = 1000\ndead_feature_window: int = 5000\n\n# Add to trainer.py:\ndef _apply_ghost_grads(self, sae, batch, dead_mask):\n    \"\"\"Apply ghost gradients to revive dead features.\"\"\"\n    # Sample from dead features\n    # Compute auxiliary loss\n    # Add to main loss\n</code></pre> <p>Expected impact: Reduce dead features from 95% \u2192 70%</p>"},{"location":"dev/SAELENS_COMPARISON/#12-add-learning-rate-decay","title":"1.2 Add Learning Rate Decay","text":"<pre><code># Update _get_lr_schedule():\ndef schedule(step):\n    if step &lt; warmup_steps:\n        return lr * (step / warmup_steps)\n    else:\n        # Cosine decay\n        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n        return lr * 0.5 * (1 + math.cos(math.pi * progress))\n</code></pre> <p>Expected impact: Better convergence, lower final loss</p>"},{"location":"dev/SAELENS_COMPARISON/#13-add-sparsity-warmup","title":"1.3 Add Sparsity Warmup","text":"<pre><code># Add to SAEConfig:\nsparsity_warm_up_steps: int = total_steps\n\n# In loss computation:\nsparsity_coef = min(1.0, step / sparsity_warm_up_steps)\nloss = recon_loss + sparsity_coef * sparsity_penalty\n</code></pre> <p>Expected impact: Fewer dead features early in training</p>"},{"location":"dev/SAELENS_COMPARISON/#priority-2-better-architectures","title":"Priority 2: Better Architectures","text":""},{"location":"dev/SAELENS_COMPARISON/#21-implement-batchtopk","title":"2.1 Implement BatchTopK","text":"<pre><code>class BatchTopKSAE(BaseSAE):\n    \"\"\"Modern BatchTopK SAE (SAELens-style).\"\"\"\n\n    def encode(self, x: mx.array) -&gt; mx.array:\n        h = self.encoder(x)\n        h = mx.maximum(h, 0)\n\n        # BatchTopK: fix mean L0 across batch\n        z = batchtopk_activation(h, k=self.k)\n        return z\n</code></pre> <p>Expected impact: More stable training, better feature utilization</p>"},{"location":"dev/SAELENS_COMPARISON/#22-implement-jumprelu-future","title":"2.2 Implement JumpReLU (Future)","text":"<pre><code>class JumpReLUSAE(BaseSAE):\n    \"\"\"JumpReLU SAE (Anthropic/DeepMind approach).\"\"\"\n\n    def __init__(self, ..., jumprelu_sparsity_loss_mode=\"tanh\"):\n        # Implementation following SAELens\n</code></pre> <p>Expected impact: State-of-the-art results, used by Anthropic</p>"},{"location":"dev/SAELENS_COMPARISON/#priority-3-advanced-features","title":"Priority 3: Advanced Features","text":""},{"location":"dev/SAELENS_COMPARISON/#31-decoder-weight-normalization","title":"3.1 Decoder Weight Normalization","text":"<pre><code># In decode():\nif self.constrain_decoder_norms:\n    # Normalize decoder weights to unit norm\n    decoder_norms = mx.sqrt(mx.sum(self.decoder.weight ** 2, axis=1, keepdims=True))\n    normalized_decoder = self.decoder.weight / (decoder_norms + 1e-8)\n    x_recon = mx.matmul(z, normalized_decoder.T) + self.decoder_bias\n</code></pre> <p>Expected impact: Better feature interpretability</p>"},{"location":"dev/SAELENS_COMPARISON/#32-advanced-normalization","title":"3.2 Advanced Normalization","text":"<pre><code># Add config option:\nnormalize_activations: str = \"expected_average_only_in\"  # or \"layer_norm\" or \"none\"\n\n# In encode():\nif normalize_activations == \"expected_average_only_in\":\n    # Normalize to L2 norm = sqrt(d_model)\n    x = x / (mx.sqrt(mx.sum(x**2, axis=-1, keepdims=True)) + 1e-8) * math.sqrt(d_model)\n</code></pre> <p>Expected impact: Better reconstruction quality (especially for JumpReLU)</p>"},{"location":"dev/SAELENS_COMPARISON/#33-activation-caching-optional","title":"3.3 Activation Caching (Optional)","text":"<pre><code># Add utility:\ndef cache_activations(model, layer, component, dataset, cache_path):\n    \"\"\"Pre-compute and cache activations for dataset.\"\"\"\n    # Generate all activations\n    # Save to disk\n    # Load during training\n</code></pre> <p>Expected impact: Faster hyperparameter experimentation</p>"},{"location":"dev/SAELENS_COMPARISON/#summary-of-differences","title":"Summary of Differences","text":"Feature SAELens mlxterp Priority Architecture TopK \u2705 \u2705 - BatchTopK \u2705 \u274c HIGH JumpReLU \u2705 \u274c MEDIUM Gated SAE \u2705 \u274c LOW Training Features Ghost Gradients \u2705 \u274c CRITICAL Dead Feature Tracking \u2705 \u274c CRITICAL Sparsity Warmup \u2705 \u274c HIGH LR Warmup \u2705 \u2705 \u2705 LR Decay \u2705 \u274c HIGH Gradient Clipping \u2705 \u2705 \u2705 Normalization Basic (mean/std) \u2705 \u2705 \u2705 L2 norm constraint \u2705 \u274c MEDIUM Decoder norm constraint \u2705 \u274c MEDIUM Data Handling Streaming \u2705 \u2705 \u2705 Pre-tokenized datasets \u2705 \u274c LOW Activation caching \u2705 \u274c MEDIUM Loss Functions MSE reconstruction \u2705 \u2705 \u2705 L1 penalty \u2705 \u2705 \u2705 L0 penalty \u2705 \u274c MEDIUM Pre-activation loss \u2705 \u274c LOW Metrics L0 count \u2705 \u2705 \u2705 L0 sparsity fraction \u2705 \u2705 \u2705 L1 magnitude \u2705 \u2705 \u2705 Dead features \u2705 \u2705 \u2705 Feature sampling stats \u2705 \u274c MEDIUM"},{"location":"dev/SAELENS_COMPARISON/#action-items","title":"Action Items","text":""},{"location":"dev/SAELENS_COMPARISON/#immediate-fix-current-issues","title":"Immediate (Fix Current Issues)","text":"<ol> <li>Implement LR decay - Add cosine decay after warmup</li> <li>Implement sparsity warmup - Gradually increase sparsity penalty</li> <li>Update default config - Use SAELens-validated hyperparameters</li> <li>Fix training plateau - Combination of #1 and #2</li> </ol>"},{"location":"dev/SAELENS_COMPARISON/#short-term-production-readiness","title":"Short-term (Production Readiness)","text":"<ol> <li>Implement ghost gradients - Critical for reducing dead features</li> <li>Implement BatchTopK - Modern, more stable architecture</li> <li>Add decoder weight constraints - Improve interpretability</li> <li>Add advanced normalization - L2 norm constraint option</li> </ol>"},{"location":"dev/SAELENS_COMPARISON/#long-term-feature-parity","title":"Long-term (Feature Parity)","text":"<ol> <li>Implement JumpReLU - State-of-the-art architecture</li> <li>Add activation caching - Optional for hyperparameter tuning</li> <li>Implement Gated SAE - Alternative architecture</li> <li>Add MatryoshkaBatchTopK - Advanced nested approach</li> </ol>"},{"location":"dev/SAELENS_COMPARISON/#conclusion","title":"Conclusion","text":"<p>Our implementation is solid but missing critical production features:</p> <p>\u2705 What we do well: - Clean, simple TopK implementation - Efficient streaming training - Good basic normalization - Proper gradient clipping</p> <p>\u274c Critical gaps: 1. No ghost gradients \u2192 95% dead features (should be 70%) 2. No LR decay \u2192 training plateaus early 3. No sparsity warmup \u2192 dead features from start 4. Only TopK architecture (SAELens has 6)</p> <p>Recommended path forward: 1. Fix immediate issues (LR decay, sparsity warmup) - 2-3 days 2. Implement ghost gradients - 1 week 3. Implement BatchTopK - 3-4 days 4. Then proceed with Phase 2 feature analysis tools</p> <p>With these fixes, we expect: - Dead features: 95% \u2192 70% - Reconstruction loss: 0.27 \u2192 0.10-0.15 - Training stability: Much improved - Production readiness: Ready for research use</p>"},{"location":"guides/activation_patching/","title":"Activation Patching Guide","text":"<p>Activation patching is a fundamental technique in mechanistic interpretability for identifying which components of a neural network are important for specific tasks.</p>"},{"location":"guides/activation_patching/#overview","title":"Overview","text":"<p>Goal: Determine which layers or components are critical for a task by:</p> <ol> <li>Running a \"clean\" input through the model</li> <li>Running a \"corrupted\" input through the model</li> <li>Patching clean activations into the corrupted run at different locations</li> <li>Measuring how much this recovers the clean output</li> </ol> <p>Key insight: If patching a component significantly recovers the clean output, that component is important for the task.</p>"},{"location":"guides/activation_patching/#quick-start-using-the-helper-function","title":"Quick Start: Using the Helper Function","text":"<p>The easiest way to perform activation patching is with the built-in <code>activation_patching()</code> method:</p> <pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\n# Load model\nbase_model, tokenizer = load('mlx-community/Llama-3.2-1B-Instruct-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\n# Find important layers - that's it!\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    component=\"mlp\",\n    plot=True\n)\n\n# Analyze\nsorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 3 most important layers:\")\nfor layer_idx, recovery in sorted_results[:3]:\n    print(f\"  Layer {layer_idx}: {recovery:.1f}% recovery\")\n</code></pre> <p>The helper function handles all the boilerplate: - Running clean and corrupted inputs - Patching each layer automatically - Measuring recovery with L2 distance - Optional visualization</p> <p>Continue reading for details on interpretation and manual implementation.</p>"},{"location":"guides/activation_patching/#quick-example","title":"Quick Example","text":"<pre><code>import mlx.core as mx\nfrom mlxterp import InterpretableModel, interventions as iv\nfrom mlx_lm import load\n\n# Load model\nbase_model, tokenizer = load('mlx-community/Llama-3.2-1B-Instruct-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n\n# Define clean vs corrupted inputs\nclean_text = \"Paris is the capital of France\"\ncorrupted_text = \"London is the capital of France\"\n\n# Get baseline outputs\nwith model.trace(clean_text):\n    clean_output = model.output.save()\n\nwith model.trace(corrupted_text):\n    corrupted_output = model.output.save()\n\nmx.eval(clean_output, corrupted_output)\n\n# Helper function to measure distance\ndef l2_distance(a, b):\n    return float(mx.sqrt(mx.sum((a - b) ** 2)))\n\nbaseline = l2_distance(corrupted_output[0, -1], clean_output[0, -1])\nprint(f\"Baseline L2 distance: {baseline:.2f}\")\n\n# Patch MLP at layer 10\nwith model.trace(clean_text) as trace:\n    clean_mlp = trace.activations[\"model.model.layers.10.mlp\"]\n\nmx.eval(clean_mlp)\n\nwith model.trace(corrupted_text,\n                interventions={\"layers.10.mlp\": iv.replace_with(clean_mlp)}):\n    patched_output = model.output.save()\n\nmx.eval(patched_output)\n\ndist = l2_distance(patched_output[0, -1], clean_output[0, -1])\nrecovery = (baseline - dist) / baseline * 100\nprint(f\"Layer 10 MLP: {recovery:.1f}% recovery\")\n</code></pre>"},{"location":"guides/activation_patching/#complete-procedure","title":"Complete Procedure","text":""},{"location":"guides/activation_patching/#step-1-define-clean-and-corrupted-inputs","title":"Step 1: Define Clean and Corrupted Inputs","text":"<p>Choose inputs that differ in exactly the aspect you want to study:</p> <pre><code># Factual knowledge task\nclean_text = \"Paris is the capital of France\"\ncorrupted_text = \"London is the capital of France\"\n\n# Sentiment task\nclean_text = \"This movie was amazing\"\ncorrupted_text = \"This movie was terrible\"\n\n# Grammatical task\nclean_text = \"The cat sits on the mat\"\ncorrupted_text = \"The cat sit on the mat\"\n</code></pre>"},{"location":"guides/activation_patching/#step-2-get-baseline-measurements","title":"Step 2: Get Baseline Measurements","text":"<pre><code># Get clean output\nwith model.trace(clean_text):\n    clean_output = model.output.save()\n\n# Get corrupted output\nwith model.trace(corrupted_text):\n    corrupted_output = model.output.save()\n\nmx.eval(clean_output, corrupted_output)\n\n# Measure baseline distance\ndef l2_distance(a, b):\n    \"\"\"L2 (Euclidean) distance between output logits\"\"\"\n    return float(mx.sqrt(mx.sum((a - b) ** 2)))\n\nbaseline = l2_distance(corrupted_output[0, -1], clean_output[0, -1])\n</code></pre>"},{"location":"guides/activation_patching/#step-3-patch-each-component","title":"Step 3: Patch Each Component","text":"<pre><code>results = {}\n\nfor layer_idx in range(len(model.layers)):\n    # Get clean activation for this component\n    with model.trace(clean_text) as trace:\n        clean_mlp = trace.activations[f\"model.model.layers.{layer_idx}.mlp\"]\n\n    mx.eval(clean_mlp)\n\n    # Patch into corrupted run\n    with model.trace(corrupted_text,\n                    interventions={f\"layers.{layer_idx}.mlp\": iv.replace_with(clean_mlp)}):\n        patched_output = model.output.save()\n\n    mx.eval(patched_output)\n\n    # Measure recovery\n    dist = l2_distance(patched_output[0, -1], clean_output[0, -1])\n    recovery = (baseline - dist) / baseline * 100\n    results[layer_idx] = recovery\n\n    print(f\"Layer {layer_idx:2d}: {recovery:6.1f}% recovery\")\n</code></pre>"},{"location":"guides/activation_patching/#step-4-analyze-results","title":"Step 4: Analyze Results","text":"<pre><code># Sort by importance\nsorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\nMost important layers:\")\nfor layer_idx, recovery in sorted_results[:5]:\n    print(f\"  Layer {layer_idx:2d}: {recovery:5.1f}% recovery\")\n</code></pre>"},{"location":"guides/activation_patching/#understanding-results","title":"Understanding Results","text":""},{"location":"guides/activation_patching/#positive-recovery","title":"Positive Recovery","text":"<p>High positive recovery (e.g., +40%) = Important layer</p> <ul> <li>Patching this component significantly recovers clean output</li> <li>This layer is critical for the task</li> <li>Often seen in early layers (feature extraction) and final layers (output formation)</li> </ul> <p>Example: <pre><code>Layer  0: +43.1% recovery  \u2190 Very important!\nLayer 15: +24.2% recovery  \u2190 Important\n</code></pre></p>"},{"location":"guides/activation_patching/#negative-recovery","title":"Negative Recovery","text":"<p>Negative recovery (e.g., -20%) = Encodes corruption</p> <ul> <li>Patching makes output WORSE than corrupted baseline</li> <li>This layer actively encodes the corrupted information</li> <li>This is expected and informative!</li> </ul> <p>Example: <pre><code>Layer  7: -18.4% recovery  \u2190 Encodes \"London\"\nLayer 10: -23.5% recovery  \u2190 Strongly encodes corruption\n</code></pre></p>"},{"location":"guides/activation_patching/#near-zero-recovery","title":"Near-Zero Recovery","text":"<p>~0% recovery = Not relevant</p> <ul> <li>Patching has minimal effect</li> <li>Layer doesn't significantly contribute to this specific task</li> <li>Might be important for other tasks</li> </ul>"},{"location":"guides/activation_patching/#components-to-patch","title":"Components to Patch","text":"<p>You can patch different granularities:</p>"},{"location":"guides/activation_patching/#full-mlp","title":"Full MLP","text":"<pre><code># Patch entire MLP block\ninterventions={\"layers.{i}.mlp\": iv.replace_with(clean_mlp)}\n</code></pre>"},{"location":"guides/activation_patching/#mlp-sub-components","title":"MLP Sub-components","text":"<pre><code># Gate projection\ninterventions={\"layers.{i}.mlp.gate_proj\": iv.replace_with(clean_gate)}\n\n# Up projection\ninterventions={\"layers.{i}.mlp.up_proj\": iv.replace_with(clean_up)}\n\n# Down projection\ninterventions={\"layers.{i}.mlp.down_proj\": iv.replace_with(clean_down)}\n</code></pre>"},{"location":"guides/activation_patching/#attention-components","title":"Attention Components","text":"<pre><code># Full attention\ninterventions={\"layers.{i}.self_attn\": iv.replace_with(clean_attn)}\n\n# Query projection\ninterventions={\"layers.{i}.self_attn.q_proj\": iv.replace_with(clean_q)}\n\n# Key projection\ninterventions={\"layers.{i}.self_attn.k_proj\": iv.replace_with(clean_k)}\n\n# Value projection\ninterventions={\"layers.{i}.self_attn.v_proj\": iv.replace_with(clean_v)}\n\n# Output projection\ninterventions={\"layers.{i}.self_attn.o_proj\": iv.replace_with(clean_o)}\n</code></pre>"},{"location":"guides/activation_patching/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"guides/activation_patching/#dont-patch-entire-layer-output","title":"\u274c DON'T: Patch Entire Layer Output","text":"<pre><code># WRONG - patches entire residual stream\nwith model.trace(corrupted_text,\n                interventions={\"layers.10\": iv.replace_with(clean_act)}):\n    pass\n</code></pre> <p>Why wrong: This replaces the entire residual stream, affecting ALL downstream layers. You'll get perfect recovery for every layer.</p>"},{"location":"guides/activation_patching/#dont-use-lambda-closures","title":"\u274c DON'T: Use Lambda Closures","text":"<pre><code># WRONG - lambda closure bug\nfor i in range(num_layers):\n    with model.trace(clean_text):\n        clean_act = model.layers[i].output.save()\n\n    # Bug: all interventions use the SAME activation\n    with model.trace(corrupted_text,\n                    interventions={f\"layers.{i}\": lambda x: clean_act}):\n        pass\n</code></pre> <p>Why wrong: Lambda captures <code>clean_act</code> by reference, so all interventions end up using the last layer's activation.</p>"},{"location":"guides/activation_patching/#do-use-ivreplace_with","title":"\u2705 DO: Use iv.replace_with()","text":"<pre><code># CORRECT\nwith model.trace(corrupted_text,\n                interventions={\"layers.10.mlp\": iv.replace_with(clean_mlp)}):\n    pass\n</code></pre>"},{"location":"guides/activation_patching/#do-choose-the-right-distance-metric","title":"\u2705 DO: Choose the Right Distance Metric","text":"<p>The <code>activation_patching()</code> helper supports three distance metrics. Choose based on your model's vocabulary size:</p> <pre><code># For small/medium models (&lt; 50k vocab)\nresults = model.activation_patching(\n    clean_text=\"...\",\n    corrupted_text=\"...\",\n    metric=\"l2\"  # Default - Euclidean distance\n)\n\n# For large vocabulary models (&gt; 100k vocab)\nresults = model.activation_patching(\n    clean_text=\"...\",\n    corrupted_text=\"...\",\n    metric=\"mse\"  # Most stable for huge models\n)\n</code></pre> <p>Why: KL divergence can give NaN, and L2 can overflow on large vocabularies. See Distance Metrics section below.</p>"},{"location":"guides/activation_patching/#advanced-position-specific-patching","title":"Advanced: Position-Specific Patching","text":"<p>Patch activations only at specific token positions:</p> <pre><code># Patch only the last token's MLP activation\nwith model.trace(clean_text) as trace:\n    clean_mlp = trace.activations[\"model.model.layers.10.mlp\"]\n\n# Create patched activation: clean for last token, corrupted for others\ndef selective_patch(corrupted_activation):\n    patched = corrupted_activation.copy()\n    patched[0, -1, :] = clean_mlp[0, -1, :]  # Patch last token only\n    return patched\n\nwith model.trace(corrupted_text,\n                interventions={\"layers.10.mlp\": selective_patch}):\n    patched_output = model.output.save()\n</code></pre>"},{"location":"guides/activation_patching/#distance-metrics","title":"Distance Metrics","text":"<p>The <code>activation_patching()</code> helper uses distance metrics to measure how different the outputs are. Choosing the right metric is crucial, especially for large models.</p>"},{"location":"guides/activation_patching/#available-metrics","title":"Available Metrics","text":""},{"location":"guides/activation_patching/#1-l2-distance-euclidean-default","title":"1. L2 Distance (Euclidean) - Default","text":"<p>Formula: <pre><code>d(a, b) = \u221a(\u03a3(a\u1d62 - b\u1d62)\u00b2)\n</code></pre></p> <p>When to use: Small to medium models (vocabulary &lt; 50k tokens)</p> <p>Implementation: <pre><code>def l2_distance(a, b):\n    diff = a - b\n    # Use float32 for accumulation to prevent overflow\n    diff_f32 = diff.astype(mx.float32)\n    squared_sum = mx.sum(diff_f32 * diff_f32)\n\n    # Check for overflow\n    if mx.isinf(squared_sum):\n        # Fallback to MSE-based calculation\n        mse = mx.mean(diff_f32 * diff_f32)\n        return float(mx.sqrt(mse) * mx.sqrt(float(diff.size)))\n\n    return float(mx.sqrt(squared_sum))\n</code></pre></p> <p>Why it can fail: With large vocabularies (e.g., 150k tokens), summing 150k squared differences can overflow to <code>inf</code>, especially in float16.</p> <p>Example: <pre><code>results = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    metric=\"l2\"  # Default\n)\n</code></pre></p>"},{"location":"guides/activation_patching/#2-cosine-distance","title":"2. Cosine Distance","text":"<p>Formula: <pre><code>d(a, b) = 1 - (a \u00b7 b) / (||a|| \u00d7 ||b||)\n\nwhere:\n  a \u00b7 b = \u03a3(a\u1d62 \u00d7 b\u1d62)           # Dot product\n  ||a|| = \u221a(\u03a3 a\u1d62\u00b2)             # L2 norm\n</code></pre></p> <p>When to use: Medium to large models (50k - 150k tokens), or when you want direction-based similarity</p> <p>Implementation: <pre><code>def cosine_distance(a, b):\n    a_f32 = a.astype(mx.float32)\n    b_f32 = b.astype(mx.float32)\n\n    a_norm = mx.sqrt(mx.sum(a_f32 * a_f32))\n    b_norm = mx.sqrt(mx.sum(b_f32 * b_f32))\n\n    if mx.isinf(a_norm) or mx.isinf(b_norm):\n        # Fallback: normalize by mean instead of sum\n        a_normalized = a_f32 / mx.sqrt(mx.mean(a_f32 * a_f32))\n        b_normalized = b_f32 / mx.sqrt(mx.mean(b_f32 * b_f32))\n        return float(1.0 - mx.mean(a_normalized * b_normalized))\n\n    a_normalized = a_f32 / a_norm\n    b_normalized = b_f32 / b_norm\n    return float(1.0 - mx.sum(a_normalized * b_normalized))\n</code></pre></p> <p>Why it's better for large models: Normalization prevents overflow by dividing before accumulation.</p> <p>Example: <pre><code>results = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    metric=\"cosine\"\n)\n</code></pre></p>"},{"location":"guides/activation_patching/#3-mean-squared-error-mse-most-stable","title":"3. Mean Squared Error (MSE) - Most Stable","text":"<p>Formula: <pre><code>d(a, b) = (1/N) \u00d7 \u03a3(a\u1d62 - b\u1d62)\u00b2\n\nwhere N = number of elements\n</code></pre></p> <p>When to use: Very large models (vocabulary &gt; 100k tokens), or when numerical stability is critical</p> <p>Implementation: <pre><code>def mse_distance(a, b):\n    diff = a.astype(mx.float32) - b.astype(mx.float32)\n    return float(mx.mean(diff * diff))\n</code></pre></p> <p>Why it's most stable: Averages over all elements instead of summing, preventing overflow even with millions of dimensions.</p> <p>Example: <pre><code># Recommended for Qwen (151k vocab), GPT-4 scale models\nresults = model.activation_patching(\n    clean_text=\"Paris is the capital of France\",\n    corrupted_text=\"London is the capital of France\",\n    metric=\"mse\"  # Most stable\n)\n</code></pre></p>"},{"location":"guides/activation_patching/#metric-selection-guide","title":"Metric Selection Guide","text":"Model Characteristics Recommended Metric Reason Vocab &lt; 50k tokens(e.g., Llama-3.2-1B) <code>\"l2\"</code> (default) Fast, accurate, no overflow risk Vocab 50k - 100k(e.g., Llama-3-8B) <code>\"l2\"</code> or <code>\"cosine\"</code> L2 with overflow protection works well Vocab &gt; 100k tokens(e.g., Qwen-30B: 151k) <code>\"mse\"</code> or <code>\"cosine\"</code> Most numerically stable Direction matters(studying vector directions) <code>\"cosine\"</code> Measures angle, not magnitude Magnitude matters(studying activation sizes) <code>\"l2\"</code> or <code>\"mse\"</code> Measures absolute difference"},{"location":"guides/activation_patching/#real-world-example-qwen-model","title":"Real-World Example: Qwen Model","text":"<p>The Qwen3-30B model has 151,936 tokens. Here's what happens with each metric:</p> <pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\nbase_model, tokenizer = load('mlx-community/Qwen3-30B-A3B-Thinking-2507-4bit')\nmodel = InterpretableModel(base_model, tokenizer=tokenizer)\n</code></pre> <p>With L2 (without protection): <pre><code>Output shape: (1, 6, 151936)  # 151k logits!\nBaseline: inf                  # Overflow!\nRecovery: nan, nan, nan...     # All NaN\n</code></pre></p> <p>With MSE \u2705: <pre><code>Baseline: 0.6480\nLayer 10: 17.9% recovery  \u2190 Works perfectly!\nLayer 30:  7.5% recovery\nLayer  0: -298.6% recovery\n</code></pre></p> <p>With Cosine \u2705: <pre><code>Baseline: 0.0079\nLayer 10: 11.9% recovery  \u2190 Also works!\nLayer 40:  9.1% recovery\nLayer  0: -45.0% recovery\n</code></pre></p>"},{"location":"guides/activation_patching/#recovery-calculation","title":"Recovery Calculation","text":"<p>Recovery percentage is computed as:</p> <pre><code>baseline_dist = distance(corrupted_output, clean_output)\npatched_dist = distance(patched_output, clean_output)\n\nrecovery = (baseline_dist - patched_dist) / baseline_dist * 100\n</code></pre> <p>Interpretation: - High positive %: Patching reduced distance significantly \u2192 layer is important - Negative %: Patching increased distance \u2192 layer encodes the corruption - ~0%: Patching had no effect \u2192 layer is not relevant</p>"},{"location":"guides/activation_patching/#why-not-kl-divergence","title":"Why Not KL Divergence?","text":"<p>KL divergence is commonly used in research papers, but it has numerical issues:</p> <pre><code># KL Divergence (NOT recommended)\ndef kl_divergence(p, q):\n    p = mx.softmax(p, axis=-1)\n    q = mx.softmax(q, axis=-1)\n    return mx.sum(p * (mx.log(p) - mx.log(q)))  # NaN from log(0)!\n</code></pre> <p>Problems: 1. <code>log(0)</code> produces <code>-inf</code> 2. Very small probabilities (&lt; 1e-7) cause numerical instability 3. Requires adding epsilon: <code>log(p + \u03b5)</code> - but what epsilon? 4. With 150k vocab, many probabilities are ~0</p> <p>Better alternatives: L2, MSE, or cosine distance are more robust.</p>"},{"location":"guides/activation_patching/#interpreting-example-results","title":"Interpreting Example Results","text":"<pre><code>Layer  0 MLP: +43.1% recovery\nLayer  2 MLP: +16.7% recovery\nLayer  6 MLP: +17.6% recovery\nLayer  7 MLP: -18.4% recovery\nLayer 10 MLP: -23.5% recovery\nLayer 15 MLP: +24.2% recovery\n</code></pre> <p>Interpretation:</p> <ol> <li>Layer 0 (43% recovery): Critical for early feature extraction</li> <li>Layers 2, 6 (16-17% recovery): Contribute to task but not critical</li> <li>Layers 7, 10 (negative): Encode the corruption (\"London\")</li> <li>Layer 15 (24% recovery): Important for final output formation</li> </ol> <p>Insight: The model processes the factual knowledge primarily in early (Layer 0) and late (Layer 15) layers, while middle layers (7-10) encode the specific entity mentioned (\"London\").</p>"},{"location":"guides/activation_patching/#complete-working-example","title":"Complete Working Example","text":"<p>See <code>examples/activation_patching_example.py</code> for a complete, tested implementation.</p>"},{"location":"guides/activation_patching/#references","title":"References","text":"<ul> <li>Classic paper: Causal Tracing for GPT-2</li> <li>TransformerLens: Similar techniques in PyTorch</li> <li>nnsight: Generic activation patching framework</li> </ul>"},{"location":"guides/dictionary_learning/","title":"Dictionary Learning with Sparse Autoencoders","text":""},{"location":"guides/dictionary_learning/#overview","title":"Overview","text":"<p>Dictionary learning aims to decompose neural network activations into interpretable, sparse features using Sparse Autoencoders (SAEs). This technique helps researchers understand what individual neurons or circuits in a model are computing.</p>"},{"location":"guides/dictionary_learning/#what-are-sparse-autoencoders","title":"What are Sparse Autoencoders?","text":"<p>SAEs learn an overcomplete dictionary of features from layer activations:</p> <ul> <li>Input: Dense activations from a layer (e.g., MLP outputs)</li> <li>Output: Sparse feature representation (typically 8-32x more features than inputs)</li> <li>Goal: Each feature should represent a monosemantic concept or pattern</li> </ul>"},{"location":"guides/dictionary_learning/#why-dictionary-learning","title":"Why Dictionary Learning?","text":"<p>Neural networks often exhibit polysemanticity - individual neurons respond to multiple unrelated concepts. SAEs help address this by:</p> <ol> <li>Decomposing polysemantic neurons into monosemantic features</li> <li>Discovering interpretable circuits by analyzing feature activations</li> <li>Enabling model steering by manipulating specific features</li> <li>Understanding model behavior through feature analysis</li> </ol>"},{"location":"guides/dictionary_learning/#quick-start","title":"Quick Start","text":""},{"location":"guides/dictionary_learning/#basic-training","title":"Basic Training","text":"<p>Train an SAE on a model layer with just one line:</p> <pre><code>from mlxterp import InterpretableModel\n\n# Load model\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\n\n# Train SAE\nsae = model.train_sae(\n    layer=10,\n    dataset=[\"Sample text 1\", \"Sample text 2\", ...],\n    save_path=\"my_sae.mlx\"\n)\n</code></pre>"},{"location":"guides/dictionary_learning/#using-pre-trained-saes","title":"Using Pre-trained SAEs","text":"<pre><code># Load saved SAE\nsae = model.load_sae(\"my_sae.mlx\")\n\n# Check compatibility\nif sae.is_compatible(model, layer=10, component=\"mlp\"):\n    print(\"\u2713 SAE is compatible!\")\n\n# Encode activations to features\nwith model.trace(\"Test input\") as trace:\n    pass\n\nactivation = trace.activations[\"model.model.layers.10.mlp\"]\nfeatures = sae.encode(activation)  # Sparse features\nreconstructed = sae.decode(features)  # Reconstructed activation\n</code></pre>"},{"location":"guides/dictionary_learning/#training-saes","title":"Training SAEs","text":""},{"location":"guides/dictionary_learning/#dataset-requirements","title":"Dataset Requirements","text":"<p>For meaningful feature learning, use a large, diverse dataset:</p> <ul> <li>Minimum: 10,000-20,000 text samples</li> <li>Recommended: 50,000+ samples</li> <li>Quality: Diverse topics and writing styles</li> </ul>"},{"location":"guides/dictionary_learning/#loading-huggingface-datasets","title":"Loading HuggingFace Datasets","text":"<pre><code>from datasets import load_dataset\nfrom mlxterp import InterpretableModel, SAEConfig\n\n# Load dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n\n# Prepare texts\ntexts = []\nfor item in dataset:\n    text = item['text'].strip()\n    if len(text) &gt; 50:  # Filter short texts\n        texts.append(text[:512])  # Truncate long texts\n    if len(texts) &gt;= 20000:\n        break\n\n# Train SAE\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\nsae = model.train_sae(\n    layer=10,\n    dataset=texts,\n    save_path=\"sae_wikitext_20k.mlx\"\n)\n</code></pre> <p>See <code>examples/sae_realistic_training.py</code> for a complete example.</p>"},{"location":"guides/dictionary_learning/#configuration","title":"Configuration","text":"<p>Customize SAE training with <code>SAEConfig</code>:</p> <pre><code>from mlxterp import SAEConfig\n\nconfig = SAEConfig(\n    expansion_factor=32,    # 32x more features than input dimension\n    k=128,                  # Keep top-128 features active (sparsity level)\n    learning_rate=3e-4,     # Learning rate\n    num_epochs=5,           # Training epochs\n    batch_size=64,          # Batch size\n    warmup_steps=500,       # Learning rate warmup steps\n    validation_split=0.05,  # 5% validation set\n    normalize_input=True,   # Normalize activations before encoding\n    tied_weights=False,     # Whether to tie encoder/decoder weights\n    gradient_clip=1.0,      # Gradient clipping threshold\n)\n\nsae = model.train_sae(layer=10, dataset=texts, config=config)\n</code></pre>"},{"location":"guides/dictionary_learning/#training-parameters","title":"Training Parameters","text":""},{"location":"guides/dictionary_learning/#expansion-factor","title":"Expansion Factor","text":"<ul> <li>Definition: Ratio of hidden features to input dimension</li> <li>Typical values: 8x to 64x</li> <li>Trade-offs:</li> <li>Higher = More fine-grained features, but more computation and memory</li> <li>Lower = Faster training, but may miss subtle features</li> </ul>"},{"location":"guides/dictionary_learning/#sparsity-k","title":"Sparsity (k)","text":"<ul> <li>Definition: Number of top features to keep active per sample</li> <li>Typical values: 50-200</li> <li>Guidelines:</li> <li>k \u2248 0.5% to 2% of hidden dimension</li> <li>For d_hidden=16384, try k=64-256</li> </ul>"},{"location":"guides/dictionary_learning/#learning-rate","title":"Learning Rate","text":"<ul> <li>Typical values: 1e-4 to 5e-4</li> <li>With warmup: Start with 500-1000 warmup steps</li> <li>Recommendation: 3e-4 with 500-step warmup works well</li> </ul>"},{"location":"guides/dictionary_learning/#dataset-size","title":"Dataset Size","text":"<ul> <li>Minimum: 10,000 samples for basic features</li> <li>Recommended: 50,000+ samples for robust features</li> <li>Large-scale: 100,000+ samples for production SAEs</li> </ul>"},{"location":"guides/dictionary_learning/#training-monitoring","title":"Training Monitoring","text":"<p>During training, monitor these metrics:</p> <pre><code>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15625/15625 [05:23, loss=0.245, l0=128.0, dead=8.5%]\n</code></pre> <ul> <li>loss: Reconstruction loss (should decrease)</li> <li>l0: Average number of active features (should match k)</li> <li>dead: Percentage of features that never activate</li> </ul> <p>Healthy training: - \u2705 Loss decreases over time - \u2705 L0 \u2248 k (target sparsity) - \u2705 Dead fraction &lt; 20%</p> <p>Warning signs: - \u274c <code>loss=nan</code> - Numerical instability - \u274c <code>l0=0.0</code> - No features activating - \u274c <code>dead=80%+</code> - Most features unused</p>"},{"location":"guides/dictionary_learning/#sae-architecture","title":"SAE Architecture","text":""},{"location":"guides/dictionary_learning/#topk-sparse-autoencoder","title":"TopK Sparse Autoencoder","text":"<p>mlxterp implements TopK SAEs, which enforce exact sparsity:</p> <pre><code>Input (d_model)\n    \u2193\nNormalize (optional)\n    \u2193\nEncoder: Linear + ReLU\n    \u2193\nTopK(k) - Keep only top-k features\n    \u2193\nFeatures (d_hidden) - SPARSE\n    \u2193\nDecoder: Linear\n    \u2193\nDenormalize (optional)\n    \u2193\nReconstruction (d_model)\n</code></pre>"},{"location":"guides/dictionary_learning/#forward-pass","title":"Forward Pass","text":"<pre><code># Encode: activation \u2192 sparse features\nfeatures = sae.encode(activation)\n# Shape: (batch, seq_len, d_hidden)\n# Most values are zero (sparse!)\n\n# Decode: sparse features \u2192 reconstruction\nreconstructed = sae.decode(features)\n# Shape: (batch, seq_len, d_model)\n</code></pre>"},{"location":"guides/dictionary_learning/#loss-function","title":"Loss Function","text":"<p>SAEs are trained to minimize reconstruction error:</p> <pre><code>loss = MSE(input, reconstruction)\n</code></pre> <p>Optional L1 sparsity penalty (not typically used with TopK): <pre><code>loss = MSE(input, reconstruction) + \u03bb * L1(features)\n</code></pre></p>"},{"location":"guides/dictionary_learning/#using-trained-saes","title":"Using Trained SAEs","text":""},{"location":"guides/dictionary_learning/#encoding-activations","title":"Encoding Activations","text":"<pre><code># Get activation from model\nwith model.trace(\"The capital of France is\") as trace:\n    pass\n\nactivation = trace.activations[\"model.model.layers.10.mlp\"]\n\n# Encode to sparse features\nfeatures = sae.encode(activation)\n\n# Check sparsity\nnum_active = int(mx.sum(features != 0))\nprint(f\"Active features: {num_active}/{sae.d_hidden}\")  # e.g., 128/16384\n\n# Get active feature indices\nactive_indices = mx.where(features[0, -1] != 0)[0]  # Last token\nprint(f\"Active feature IDs: {active_indices.tolist()}\")\n</code></pre>"},{"location":"guides/dictionary_learning/#reconstruction-quality","title":"Reconstruction Quality","text":"<pre><code># Decode back to activation space\nreconstructed = sae.decode(features)\n\n# Measure reconstruction error\nmse = float(mx.mean((activation - reconstructed) ** 2))\nprint(f\"Reconstruction MSE: {mse:.6f}\")\n\n# Good reconstruction: MSE &lt; 0.01\n# Poor reconstruction: MSE &gt; 0.1\n</code></pre>"},{"location":"guides/dictionary_learning/#save-and-load","title":"Save and Load","text":"<pre><code># Save trained SAE\nsae.save(\"my_sae.mlx\")\n\n# Load later\nsae = model.load_sae(\"my_sae.mlx\")\n\n# Check metadata\nprint(sae.metadata)\n# {'layer': 10, 'component': 'mlp', 'model_name': '...', ...}\n</code></pre>"},{"location":"guides/dictionary_learning/#experiment-tracking-with-weights-biases","title":"Experiment Tracking with Weights &amp; Biases","text":"<p>mlxterp supports automatic logging to Weights &amp; Biases for experiment tracking and visualization.</p>"},{"location":"guides/dictionary_learning/#setup","title":"Setup","text":"<pre><code># Install wandb\npip install wandb\n\n# Login (first time only)\nwandb login\n</code></pre>"},{"location":"guides/dictionary_learning/#basic-usage","title":"Basic Usage","text":"<p>Enable W&amp;B logging in your SAE configuration:</p> <pre><code>from mlxterp import InterpretableModel, SAEConfig\n\nconfig = SAEConfig(\n    expansion_factor=16,\n    k=100,\n    learning_rate=3e-4,\n\n    # Enable W&amp;B logging\n    use_wandb=True,\n    wandb_project=\"my-sae-experiments\",\n    wandb_name=\"sae_layer10_mlp\",\n    wandb_tags=[\"sae\", \"layer10\", \"mlp\"],\n)\n\nmodel = InterpretableModel(\"mlx-community/Llama-3.2-1B-Instruct\")\nsae = model.train_sae(\n    layer=10,\n    dataset=texts,\n    config=config,\n)\n</code></pre>"},{"location":"guides/dictionary_learning/#logged-metrics","title":"Logged Metrics","text":"<p>W&amp;B automatically logs:</p> <p>Training metrics (every step): - <code>train/loss</code> - Reconstruction loss - <code>train/recon_loss</code> - MSE reconstruction error - <code>train/l0</code> - Average active features per sample - <code>train/dead_fraction</code> - Percentage of dead features - <code>train/l1_loss</code> - L1 sparsity penalty (if used) - <code>train/learning_rate</code> - Current learning rate</p> <p>Validation metrics (per epoch): - <code>val/loss</code> - Validation reconstruction loss - <code>val/l0</code> - Validation feature activation - <code>val/dead_fraction</code> - Validation dead features</p> <p>Configuration (logged once): - All SAEConfig parameters - Model metadata (d_model, d_hidden, layer, component) - Dataset size and activation counts</p>"},{"location":"guides/dictionary_learning/#example","title":"Example","text":"<p>See <code>examples/sae_with_wandb.py</code> for a complete example.</p>"},{"location":"guides/dictionary_learning/#tips-for-experiment-tracking","title":"Tips for Experiment Tracking","text":"<ol> <li> <p>Organize with projects: Use different projects for different models or research questions <pre><code>config = SAEConfig(\n    use_wandb=True,\n    wandb_project=\"llama-sae-research\",  # One project per model family\n)\n</code></pre></p> </li> <li> <p>Use descriptive names: Include layer, component, and key hyperparameters <pre><code>config = SAEConfig(\n    use_wandb=True,\n    wandb_name=f\"sae_l{layer}_{component}_k{k}_exp{expansion}\",\n)\n</code></pre></p> </li> <li> <p>Tag for filtering: Add tags to group related experiments <pre><code>config = SAEConfig(\n    use_wandb=True,\n    wandb_tags=[\"baseline\", \"layer-10\", \"high-sparsity\"],\n)\n</code></pre></p> </li> <li> <p>Compare runs: Use W&amp;B's comparison features to analyze:</p> </li> <li>How expansion factor affects reconstruction quality</li> <li>Impact of sparsity (k) on feature interpretability</li> <li>Training stability across different learning rates</li> </ol>"},{"location":"guides/dictionary_learning/#visualizations-in-wb","title":"Visualizations in W&amp;B","text":"<p>W&amp;B provides automatic visualizations: - Loss curves: Track training and validation loss - Feature activation: Monitor l0 (number of active features) - Dead neurons: Track feature utilization over time - Learning rate: Visualize warmup and scheduling - Hyperparameter importance: Compare runs to find optimal settings</p>"},{"location":"guides/dictionary_learning/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guides/dictionary_learning/#component-selection","title":"Component Selection","text":"<p>Train SAEs on different components:</p> <pre><code># MLP output (recommended starting point)\nsae_mlp = model.train_sae(layer=10, component=\"mlp\", dataset=texts)\n\n# Attention output\nsae_attn = model.train_sae(layer=10, component=\"attn\", dataset=texts)\n\n# Residual stream (after layer)\nsae_resid = model.train_sae(layer=10, component=\"output\", dataset=texts)\n</code></pre>"},{"location":"guides/dictionary_learning/#layer-selection","title":"Layer Selection","text":"<p>Choose layers based on your research question:</p> <ul> <li>Early layers (0-5): Low-level features (syntax, simple patterns)</li> <li>Middle layers (6-15): Mid-level concepts and facts</li> <li>Late layers (16+): High-level reasoning and task-specific features</li> </ul> <pre><code># Train SAEs on multiple layers\nfor layer in [6, 12, 18, 24]:\n    sae = model.train_sae(\n        layer=layer,\n        dataset=texts,\n        save_path=f\"sae_layer{layer}.mlx\"\n    )\n</code></pre>"},{"location":"guides/dictionary_learning/#normalization","title":"Normalization","text":"<p>Input normalization stabilizes training:</p> <pre><code># With normalization (recommended)\nconfig = SAEConfig(normalize_input=True)\nsae = model.train_sae(layer=10, dataset=texts, config=config)\n\n# During training, SAE computes running mean/std\n# These are used to normalize inputs during encoding\n</code></pre>"},{"location":"guides/dictionary_learning/#tied-weights","title":"Tied Weights","text":"<p>Reduce parameters by tying encoder and decoder weights:</p> <pre><code>config = SAEConfig(\n    tied_weights=True,  # decoder = encoder^T\n)\n\nsae = model.train_sae(layer=10, dataset=texts, config=config)\n\n# Benefits:\n# - Fewer parameters (2x reduction)\n# - Faster training\n# - May improve interpretability\n\n# Trade-offs:\n# - Slightly worse reconstruction\n# - Less flexible decoder\n</code></pre>"},{"location":"guides/dictionary_learning/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/dictionary_learning/#training-speed","title":"Training Speed","text":"<p>Training time depends on: - Dataset size - Model dimension - Expansion factor - Batch size - Hardware</p> <p>Typical speeds (M1 Max): - Small dataset (10k samples): 1-2 minutes - Medium dataset (50k samples): 5-10 minutes - Large dataset (200k samples): 30-60 minutes</p>"},{"location":"guides/dictionary_learning/#memory-usage","title":"Memory Usage","text":"<pre><code># SAE parameters\nparams = d_model * d_hidden * 2  # encoder + decoder weights\n\n# Example: d_model=2048, expansion=32x\n# d_hidden = 2048 * 32 = 65,536\n# params = 2048 * 65,536 * 2 = 268M parameters\n# memory \u2248 1GB (float32)\n</code></pre> <p>Memory optimization: - Use smaller expansion factors (8x-16x) - Train on smaller batches - Process dataset in chunks - Use MLX's lazy evaluation</p>"},{"location":"guides/dictionary_learning/#mlx-optimizations","title":"MLX Optimizations","text":"<p>mlxterp leverages MLX's features:</p> <ol> <li>Unified Memory: No CPU\u2194GPU transfers</li> <li>Lazy Evaluation: Compute only what's needed</li> <li>Efficient Kernels: Optimized Metal operations</li> <li>Batch Processing: Vectorized operations</li> </ol>"},{"location":"guides/dictionary_learning/#best-practices","title":"Best Practices","text":""},{"location":"guides/dictionary_learning/#dataset-preparation","title":"Dataset Preparation","text":"<ol> <li>Diversity: Include varied topics and styles</li> <li>Quality: Filter out low-quality or corrupted text</li> <li>Length: Use texts with 50-500 tokens</li> <li>Size: Aim for 20,000+ samples minimum</li> </ol> <pre><code>def prepare_dataset(raw_texts, min_len=50, max_len=512):\n    \"\"\"Prepare high-quality dataset for SAE training.\"\"\"\n    filtered = []\n    for text in raw_texts:\n        # Remove very short texts\n        if len(text) &lt; min_len:\n            continue\n        # Truncate long texts\n        if len(text) &gt; max_len:\n            text = text[:max_len]\n        # Remove duplicates\n        if text not in filtered:\n            filtered.append(text.strip())\n    return filtered\n</code></pre>"},{"location":"guides/dictionary_learning/#hyperparameter-selection","title":"Hyperparameter Selection","text":"<p>Start with these defaults:</p> <pre><code>config = SAEConfig(\n    expansion_factor=16,   # Good balance\n    k=100,                 # ~0.5-1% sparsity\n    learning_rate=3e-4,    # Stable learning rate\n    num_epochs=5,          # Sufficient for convergence\n    batch_size=64,         # Good for most GPUs\n    warmup_steps=500,      # Stabilizes early training\n)\n</code></pre> <p>Adjust based on results: - Poor reconstruction \u2192 Increase expansion factor or decrease k - Too slow \u2192 Decrease expansion factor or dataset size - Training unstable \u2192 Decrease learning rate, increase warmup - High dead fraction \u2192 Increase dataset diversity</p>"},{"location":"guides/dictionary_learning/#validation","title":"Validation","text":"<p>Check SAE quality:</p> <pre><code># 1. Reconstruction quality\ntest_texts = [\"Sample 1\", \"Sample 2\", ...]\nwith model.trace(test_texts) as trace:\n    pass\n\nactivation = trace.activations[\"model.model.layers.10.mlp\"]\nreconstructed = sae.decode(sae.encode(activation))\nmse = float(mx.mean((activation - reconstructed) ** 2))\n\n# Target: MSE &lt; 0.05\n\n# 2. Feature activation distribution\nfeatures = sae.encode(activation)\nactivation_counts = mx.sum(features != 0, axis=(0, 1))\n\n# Check for dead features\ndead_features = int(mx.sum(activation_counts == 0))\nprint(f\"Dead features: {dead_features}/{sae.d_hidden}\")\n\n# Target: &lt; 20% dead\n</code></pre>"},{"location":"guides/dictionary_learning/#coming-soon-phase-2-features","title":"Coming Soon: Phase 2 Features","text":"<p>Future functionality for feature analysis:</p>"},{"location":"guides/dictionary_learning/#feature-analysis","title":"Feature Analysis","text":"<pre><code># Find top features for text (Phase 2)\ntop_features = sae.analyze_text(\n    model,\n    prompt=\"Artificial intelligence is transforming society\",\n    top_k=10\n)\n\n# Find texts that activate a feature (Phase 2)\nactivating_texts = sae.get_top_activating_texts(\n    feature_idx=1234,\n    dataset=texts,\n    top_k=20\n)\n</code></pre>"},{"location":"guides/dictionary_learning/#feature-visualization","title":"Feature Visualization","text":"<pre><code># Visualize feature activations (Phase 2)\nsae.visualize_feature(\n    feature_idx=1234,\n    model=model,\n    dataset=texts\n)\n\n# Interactive feature dashboard (Phase 2)\nsae.launch_dashboard(model=model, dataset=texts)\n</code></pre>"},{"location":"guides/dictionary_learning/#model-steering","title":"Model Steering","text":"<pre><code># Steer model behavior with features (Phase 2)\nwith model.trace(\"Generate text about...\") as trace:\n    sae.steer(\n        model=model,\n        layer=10,\n        feature_idx=1234,\n        strength=2.0  # Amplify this feature\n    )\n    output = model.output.save()\n</code></pre>"},{"location":"guides/dictionary_learning/#resources","title":"Resources","text":""},{"location":"guides/dictionary_learning/#examples","title":"Examples","text":"<ul> <li><code>examples/sae_quickstart.py</code> - Basic SAE training</li> <li><code>examples/sae_realistic_training.py</code> - Training with HuggingFace datasets</li> </ul>"},{"location":"guides/dictionary_learning/#related-research","title":"Related Research","text":"<ul> <li>Toy Models of Superposition - Anthropic</li> <li>Sparse Autoencoders Find Highly Interpretable Features</li> <li>Scaling Monosemanticity - Anthropic</li> </ul>"},{"location":"guides/dictionary_learning/#api-reference","title":"API Reference","text":"<ul> <li>API Documentation - Core mlxterp API reference</li> </ul>"},{"location":"guides/dictionary_learning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dictionary_learning/#nan-losses","title":"NaN Losses","text":"<p>Symptoms: <code>loss=nan</code> during training</p> <p>Causes: 1. Learning rate too high 2. Numerical instability 3. Bad initialization</p> <p>Solutions: <pre><code>config = SAEConfig(\n    learning_rate=1e-4,      # Lower learning rate\n    warmup_steps=1000,       # More warmup\n    gradient_clip=0.5,       # Stricter clipping\n    normalize_input=True,    # Enable normalization\n)\n</code></pre></p>"},{"location":"guides/dictionary_learning/#no-active-features","title":"No Active Features","text":"<p>Symptoms: <code>l0=0.0</code> or very low</p> <p>Causes: 1. k too small 2. Poor initialization 3. Dataset issues</p> <p>Solutions: <pre><code>config = SAEConfig(\n    k=100,  # Increase k\n)\n# Use more diverse dataset\n</code></pre></p>"},{"location":"guides/dictionary_learning/#high-dead-fraction","title":"High Dead Fraction","text":"<p>Symptoms: <code>dead=50%+</code></p> <p>Causes: 1. Dataset not diverse enough 2. Expansion factor too high 3. Not enough training</p> <p>Solutions: - Increase dataset size and diversity - Reduce expansion factor - Train for more epochs - Consider dead neuron resampling (Phase 2)</p>"},{"location":"guides/dictionary_learning/#slow-training","title":"Slow Training","text":"<p>Symptoms: Training takes hours</p> <p>Causes: 1. Dataset too large 2. Model too large 3. Expansion factor too high</p> <p>Solutions: <pre><code>config = SAEConfig(\n    batch_size=128,        # Larger batches\n    expansion_factor=8,    # Reduce expansion\n)\n# Use subset of dataset for faster iteration\n</code></pre></p>"},{"location":"guides/dictionary_learning/#summary","title":"Summary","text":"<p>Dictionary learning with SAEs enables:</p> <ol> <li>\u2705 Interpretable features from dense activations</li> <li>\u2705 Monosemantic decomposition of polysemantic neurons</li> <li>\u2705 Circuit discovery through feature analysis</li> <li>\u2705 Model steering via feature manipulation</li> </ol> <p>Key takeaways: - Use large, diverse datasets (20k+ samples) - Start with expansion=16x, k=100 - Monitor loss, l0, and dead fraction - Validate reconstruction quality - Experiment with different layers and components</p> <p>Next steps: - Train your first SAE with <code>examples/sae_realistic_training.py</code> - Explore different hyperparameters - Prepare for Phase 2: feature analysis and visualization</p>"},{"location":"guides/sae_evaluation/","title":"SAE Evaluation and Feature Analysis","text":"<p>This guide shows how to evaluate trained SAEs and analyze the features they have learned.</p>"},{"location":"guides/sae_evaluation/#overview","title":"Overview","text":"<p>After training an SAE, you need to: 1. Evaluate quality - How well does it reconstruct activations? 2. Analyze features - What concepts do the features represent? 3. Validate utility - Is it useful for interpretability research?</p>"},{"location":"guides/sae_evaluation/#quick-start","title":"Quick Start","text":""},{"location":"guides/sae_evaluation/#evaluate-your-sae","title":"Evaluate Your SAE","text":"<p>Run the comprehensive evaluation script:</p> <pre><code>python examples/evaluate_sae.py\n</code></pre> <p>This produces: - Reconstruction quality metrics - Sparsity analysis - Feature interpretability assessment - Overall quality score (0-5) - Detailed JSON report</p>"},{"location":"guides/sae_evaluation/#analyze-features","title":"Analyze Features","text":"<p>Explore what your SAE has learned:</p> <pre><code>python examples/sae_feature_analysis.py\n</code></pre> <p>This demonstrates: - Finding top features for any text - Finding top texts for any feature - Understanding feature representations</p>"},{"location":"guides/sae_evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"guides/sae_evaluation/#1-reconstruction-quality-0-2-points","title":"1. Reconstruction Quality (0-2 points)","text":"<p>Measures: How well the SAE can reconstruct the original activations</p> <p>Metrics: - MSE (Mean Squared Error): Lower is better   - Excellent: &lt; 0.01   - Good: &lt; 0.05   - Fair: &lt; 0.10   - Poor: &gt; 0.10</p> <ul> <li>Cosine Similarity: Higher is better</li> <li>Excellent: &gt; 0.95 \u2705</li> <li>Good: &gt; 0.90 \u2705</li> <li>Fair: &gt; 0.85 \u26a0\ufe0f</li> <li> <p>Poor: &lt; 0.85 \u274c</p> </li> <li> <p>Explained Variance: Percentage of variance captured</p> </li> <li>Excellent: &gt; 90%</li> <li>Good: &gt; 80%</li> <li>Fair: &gt; 70%</li> <li>Poor: &lt; 70%</li> </ul> <p>Interpretation: <pre><code># Good reconstruction means:\n# - SAE captures most important information\n# - Features represent meaningful patterns\n# - Safe to use for interpretability\n\n# Poor reconstruction means:\n# - Information loss is too high\n# - May miss important features\n# - Consider retraining with larger expansion factor\n</code></pre></p>"},{"location":"guides/sae_evaluation/#2-sparsity-metrics-0-2-points","title":"2. Sparsity Metrics (0-2 points)","text":"<p>Measures: How efficiently the SAE uses its features</p> <p>Metrics: - L0 (Active Features): Average features active per sample   - Should match target <code>k</code> value   - Example: If k=128, expect L0 \u2248 128</p> <ul> <li>Dead Features: Features that never activate</li> <li>Excellent: &lt; 30% \u2705</li> <li>Good: &lt; 50% \u2705</li> <li>Fair: &lt; 70% \u26a0\ufe0f</li> <li>Poor: &gt; 70% \u274c</li> </ul> <p>With ghost gradients: Expect ~60-75% dead features (improvement from 95% without)</p> <p>Dead Feature Timeline: <pre><code>Training Steps    Dead Features    Why\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n0-1000           95-99%           Ghost grads not active yet\n1000-5000        85-95%           Ghost grads starting to work\n5000-10000       70-85%           Ghost grads fully active\n10000+           60-75%           Equilibrium reached\n</code></pre></p> <p>Interpretation: <pre><code># Dead features are normal and expected\n# - Caused by feature specialization\n# - Batch-level vs sample-level activation mismatch\n# - Some features only activate on rare patterns\n\n# Target: 60-75% dead with ghost gradients\n# - This is considered good performance\n# - Matches SAELens benchmarks\n# - Better than 95%+ without ghost grads\n</code></pre></p>"},{"location":"guides/sae_evaluation/#3-feature-interpretability-0-1-point","title":"3. Feature Interpretability (0-1 point)","text":"<p>Measures: Can humans understand what features represent?</p> <p>Metrics: - Feature Diversity: Number of interpretable features found   - Good: \u2265 8 distinct features   - Fair: \u2265 5 distinct features   - Poor: &lt; 5 distinct features</p> <p>Example: <pre><code># Good interpretability:\nFeature 1234: Activates on mathematical equations\n  - \"2 + 2 = 4\"\n  - \"E = mc\u00b2\"\n  - \"ax\u00b2 + bx + c = 0\"\n\nFeature 5678: Activates on geographic locations\n  - \"Paris, France\"\n  - \"Tokyo, Japan\"\n  - \"New York City\"\n\n# Poor interpretability:\nFeature 9999: No clear pattern\n  - Random mix of unrelated texts\n  - No semantic coherence\n</code></pre></p>"},{"location":"guides/sae_evaluation/#overall-quality-score","title":"Overall Quality Score","text":"<p>Formula: <code>score = reconstruction (0-2) + sparsity (0-2) + interpretability (0-1)</code></p> <p>Rating: - 4.5-5.0: Excellent - Ready for production research \u2705 - 3.5-4.5: Good - Suitable for interpretability work \u2705 - 2.5-3.5: Fair - Consider retraining with different hyperparameters \u26a0\ufe0f - &lt; 2.5: Poor - Retraining strongly recommended \u274c</p>"},{"location":"guides/sae_evaluation/#feature-analysis","title":"Feature Analysis","text":""},{"location":"guides/sae_evaluation/#finding-top-features-for-text","title":"Finding Top Features for Text","text":"<p>Goal: Discover which features activate when processing specific input</p> <pre><code>from mlx_lm import load\nfrom mlxterp import InterpretableModel\nfrom mlxterp.sae import BatchTopKSAE\nfrom examples.sae_feature_analysis import get_top_activating_features\n\n# Load SAE and model\nsae = BatchTopKSAE.load(\"my_sae.mlx\")\nmodel, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\ninterp = InterpretableModel(model, tokenizer=tokenizer)\n\n# Analyze a text\ntext = \"Machine learning models learn patterns from data\"\ntop_features = get_top_activating_features(\n    sae, interp, text,\n    layer=12, component=\"mlp\",\n    top_k=10\n)\n\nprint(f\"Top features for: '{text}'\")\nfor feat_id, activation in top_features:\n    print(f\"  Feature {feat_id:6d}: {activation:.4f}\")\n</code></pre> <p>Output: <pre><code>Top features for: 'Machine learning models learn patterns from data'\n  Feature   4251: 0.8523\n  Feature  12089: 0.7341\n  Feature   8765: 0.6892\n  ...\n</code></pre></p> <p>Use cases: - Understanding model's internal representation of concepts - Discovering unexpected feature activations - Validating feature interpretability - Finding relevant features for interventions</p>"},{"location":"guides/sae_evaluation/#finding-top-texts-for-feature","title":"Finding Top Texts for Feature","text":"<p>Goal: Understand what a specific feature represents</p> <pre><code>from examples.sae_feature_analysis import get_top_activating_texts\n\n# Pick a feature to investigate\nfeature_id = 4251\n\n# Find texts that activate it strongly\nexamples = get_top_activating_texts(\n    sae, interp,\n    feature_id=feature_id,\n    texts=dataset_texts,  # Your dataset\n    layer=12, component=\"mlp\",\n    top_k=20\n)\n\nprint(f\"Feature {feature_id} activates most on:\")\nfor text, activation, pos in examples[:5]:\n    print(f\"  [{activation:.4f}] {text[:100]}...\")\n</code></pre> <p>Output: <pre><code>Feature 4251 activates most on:\n  [0.9234] Machine learning algorithms can identify patterns...\n  [0.8876] Neural networks learn hierarchical representations...\n  [0.8654] Deep learning has revolutionized AI research...\n  [0.8432] Supervised learning requires labeled training data...\n  [0.8210] Artificial intelligence systems use statistical models...\n</code></pre></p> <p>Interpretation: Feature 4251 appears to represent \"machine learning/AI concepts\"</p> <p>Workflow: 1. Find top activating texts 2. Read and analyze the examples 3. Look for common patterns/themes 4. Form hypothesis about feature meaning 5. Validate with additional examples 6. Test with feature ablation/amplification</p>"},{"location":"guides/sae_evaluation/#feature-interpretation-example","title":"Feature Interpretation Example","text":"<p>Complete analysis of a feature:</p> <pre><code># Step 1: Find a feature of interest\ntext = \"The quick brown fox jumps over the lazy dog\"\ntop_features = get_top_activating_features(\n    sae, interp, text, layer=12, component=\"mlp\", top_k=5\n)\n\n# Step 2: Pick the top feature\nfeature_id = top_features[0][0]\nprint(f\"Analyzing feature {feature_id}...\")\n\n# Step 3: Find many examples\nexamples = get_top_activating_texts(\n    sae, interp, feature_id,\n    texts=large_dataset,  # Use large dataset\n    layer=12, component=\"mlp\",\n    top_k=50  # Get many examples\n)\n\n# Step 4: Analyze patterns\nprint(f\"\\nTop 10 activating texts:\")\nfor i, (text, activation, pos) in enumerate(examples[:10], 1):\n    print(f\"{i:2d}. [{activation:.4f}] {text[:150]}...\")\n\n# Step 5: Form hypothesis\n# Look for:\n# - Common words or phrases\n# - Shared semantic themes\n# - Syntactic patterns\n# - Domain-specific vocabulary\n\n# Step 6: Give feature a name\n# Example: \"animal-related\" or \"physical movement\" or \"common phrases\"\n</code></pre>"},{"location":"guides/sae_evaluation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/sae_evaluation/#poor-reconstruction-quality","title":"Poor Reconstruction Quality","text":"<p>Symptoms: - Cosine similarity &lt; 0.85 - High MSE (&gt; 0.10) - Low explained variance (&lt; 70%)</p> <p>Possible causes: 1. Expansion factor too small 2. k (sparsity) too low 3. Not enough training epochs 4. Poor training data quality</p> <p>Solutions: <pre><code># Increase expansion factor\nconfig = SAEConfig(\n    expansion_factor=64,  # Was 32\n    k=256,  # Proportionally increase k\n)\n\n# Or decrease sparsity (keep more features active)\nconfig = SAEConfig(\n    expansion_factor=32,\n    k=200,  # Was 128\n)\n\n# Or train longer\nconfig = SAEConfig(\n    num_epochs=10,  # Was 5\n)\n</code></pre></p>"},{"location":"guides/sae_evaluation/#too-many-dead-features","title":"Too Many Dead Features","text":"<p>Symptoms: - Dead features &gt; 80% - Most features never activate</p> <p>Possible causes: 1. Dataset not diverse enough 2. Training stopped before ghost grads activated 3. Expansion factor too high 4. k (sparsity) too low</p> <p>Solutions: <pre><code># Use more diverse dataset\ntexts = load_diverse_dataset(size=50000)\n\n# Enable ghost grads with earlier activation\nconfig = SAEConfig(\n    use_ghost_grads=True,\n    dead_feature_window=500,  # Start early (was 1000)\n    sparsity_warm_up_steps=2000,\n)\n\n# Or reduce expansion factor\nconfig = SAEConfig(\n    expansion_factor=16,  # Was 32\n    k=64,  # Proportionally reduce k\n)\n</code></pre></p>"},{"location":"guides/sae_evaluation/#features-not-interpretable","title":"Features Not Interpretable","text":"<p>Symptoms: - Top activating texts seem random - No clear semantic patterns - Features activate on unrelated concepts</p> <p>Possible causes: 1. SAE not well-trained 2. Looking at wrong layer/component 3. Dataset too homogeneous 4. Need more examples to see pattern</p> <p>Solutions: <pre><code># Get more examples (patterns may emerge)\nexamples = get_top_activating_texts(\n    sae, interp, feature_id,\n    texts=dataset,\n    top_k=100,  # Was 20\n)\n\n# Try different layers\nfor layer in [6, 12, 18, 24]:\n    sae_layer = BatchTopKSAE.load(f\"sae_layer{layer}.mlx\")\n    # Analyze features...\n\n# Use more diverse dataset\ndiverse_texts = load_multiple_sources([\n    \"wikipedia\", \"books\", \"news\", \"code\", \"conversations\"\n])\n</code></pre></p>"},{"location":"guides/sae_evaluation/#best-practices","title":"Best Practices","text":""},{"location":"guides/sae_evaluation/#1-evaluation-workflow","title":"1. Evaluation Workflow","text":"<pre><code># Step 1: Quick evaluation\npython examples/evaluate_sae.py\n\n# Check quality score:\n# - If score \u2265 3.5: Proceed to feature analysis\n# - If score &lt; 3.5: Consider retraining\n\n# Step 2: Feature analysis (if quality is good)\npython examples/sae_feature_analysis.py\n\n# Step 3: Detailed investigation of interesting features\n# (Use the functions in sae_feature_analysis.py)\n\n# Step 4: Document findings\n# - Which features are interpretable?\n# - What concepts do they represent?\n# - Are they useful for your research question?\n</code></pre>"},{"location":"guides/sae_evaluation/#2-feature-naming-convention","title":"2. Feature Naming Convention","text":"<p>When you identify interpretable features, document them:</p> <pre><code># Create a feature registry\nfeature_registry = {\n    1234: {\n        \"name\": \"mathematical_equations\",\n        \"description\": \"Activates on mathematical formulas and equations\",\n        \"examples\": [\"2+2=4\", \"E=mc\u00b2\", \"ax\u00b2+bx+c=0\"],\n        \"confidence\": \"high\"\n    },\n    5678: {\n        \"name\": \"geographic_locations\",\n        \"description\": \"Activates on city and country names\",\n        \"examples\": [\"Paris, France\", \"Tokyo, Japan\"],\n        \"confidence\": \"high\"\n    },\n    9999: {\n        \"name\": \"unknown_pattern\",\n        \"description\": \"No clear pattern identified yet\",\n        \"examples\": [...],\n        \"confidence\": \"low\"\n    }\n}\n\n# Save registry\nimport json\nwith open(\"sae_features.json\", \"w\") as f:\n    json.dump(feature_registry, f, indent=2)\n</code></pre>"},{"location":"guides/sae_evaluation/#3-iterative-analysis","title":"3. Iterative Analysis","text":"<p>Feature interpretation is iterative:</p> <pre><code>1. Get initial examples (top 10-20)\n2. Form hypothesis about feature meaning\n3. Get more examples (top 50-100) to validate\n4. Refine hypothesis based on all examples\n5. Test hypothesis with ablation/steering\n6. Document findings\n</code></pre>"},{"location":"guides/sae_evaluation/#4-multiple-perspectives","title":"4. Multiple Perspectives","text":"<p>Analyze features from different angles:</p> <pre><code># Perspective 1: What activates the feature?\nexamples = get_top_activating_texts(...)\n\n# Perspective 2: When does it activate in context?\n# (Look at token positions in examples)\n\n# Perspective 3: What happens when you remove it?\n# (Feature ablation - Phase 2.5)\n\n# Perspective 4: What happens when you amplify it?\n# (Feature steering - Phase 2.5)\n</code></pre>"},{"location":"guides/sae_evaluation/#example-analysis-session","title":"Example Analysis Session","text":"<p>Here's a complete example of analyzing an SAE:</p> <pre><code>from mlx_lm import load\nfrom mlxterp import InterpretableModel\nfrom mlxterp.sae import BatchTopKSAE\nfrom examples.sae_feature_analysis import (\n    get_top_activating_features,\n    get_top_activating_texts\n)\nfrom datasets import load_dataset\n\n# Setup\nsae = BatchTopKSAE.load(\"sae_layer12_mlp.mlx\")\nmodel, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\ninterp = InterpretableModel(model, tokenizer=tokenizer)\n\n# Load test dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\ntexts = [item[\"text\"] for item in dataset if len(item[\"text\"]) &gt; 50][:1000]\n\nprint(\"=\"*80)\nprint(\"SAE FEATURE ANALYSIS SESSION\")\nprint(\"=\"*80)\n\n# Analysis 1: Pick interesting texts\ntest_texts = [\n    \"Python is a programming language used for data science\",\n    \"The Eiffel Tower is located in Paris, France\",\n    \"Photosynthesis converts sunlight into chemical energy\",\n]\n\nfor text in test_texts:\n    print(f\"\\nText: '{text}'\")\n\n    # Find top features\n    features = get_top_activating_features(\n        sae, interp, text,\n        layer=12, component=\"mlp\",\n        top_k=3\n    )\n\n    print(\"Top 3 features:\")\n    for feat_id, activation in features:\n        print(f\"  Feature {feat_id}: {activation:.4f}\")\n\n        # Get examples for this feature\n        examples = get_top_activating_texts(\n            sae, interp, feat_id,\n            texts=texts[:200],  # Search subset for speed\n            layer=12, component=\"mlp\",\n            top_k=3\n        )\n\n        print(f\"    Top examples:\")\n        for ex_text, ex_act, pos in examples:\n            print(f\"      [{ex_act:.4f}] {ex_text[:60]}...\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ANALYSIS COMPLETE\")\nprint(\"=\"*80)\n</code></pre>"},{"location":"guides/sae_evaluation/#next-steps","title":"Next Steps","text":""},{"location":"guides/sae_evaluation/#immediate-actions","title":"Immediate Actions","text":"<ol> <li> <p>Run evaluation on your trained SAE:    <pre><code>python examples/evaluate_sae.py\n</code></pre></p> </li> <li> <p>Analyze features if quality is good:    <pre><code>python examples/sae_feature_analysis.py\n</code></pre></p> </li> <li> <p>Document findings in a feature registry</p> </li> </ol>"},{"location":"guides/sae_evaluation/#future-capabilities-phase-25","title":"Future Capabilities (Phase 2.5)","text":"<ul> <li>Feature steering: Ablate or amplify features during generation</li> <li>Visualization dashboard: Interactive feature browser</li> <li>Automated clustering: Find similar features automatically</li> <li>Circuit discovery: Map feature interactions across layers</li> </ul>"},{"location":"guides/sae_evaluation/#resources","title":"Resources","text":"<ul> <li>Dictionary Learning Guide - How to train SAEs</li> <li>SAE Examples - Complete code examples</li> <li>SAE Roadmap - Development status</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"guides/sae_evaluation/#references","title":"References","text":"<ul> <li>Scaling Monosemanticity - Anthropic (2024)</li> <li>Sparse Autoencoders Find Highly Interpretable Features - Cunningham et al. (2023)</li> <li>SAELens - Production SAE framework</li> <li>Toy Models of Superposition - Anthropic (2022)</li> </ul>"},{"location":"guides/sae_feature_analysis/","title":"SAE Feature Analysis (Phase 2)","text":"<p>This guide shows how to analyze SAE features to understand what concepts they represent.</p>"},{"location":"guides/sae_feature_analysis/#overview","title":"Overview","text":"<p>After training an SAE, you can use Phase 2 tools to:</p> <ol> <li>Find top features for text - Which features activate when processing specific inputs</li> <li>Find top texts for features - What examples activate a specific feature most strongly</li> <li>Visualize feature activations - See activations highlighted in context</li> </ol>"},{"location":"guides/sae_feature_analysis/#quick-start","title":"Quick Start","text":""},{"location":"guides/sae_feature_analysis/#basic-feature-analysis","title":"Basic Feature Analysis","text":"<pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\n\n# Load model and SAE\nmlx_model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\nmodel = InterpretableModel(mlx_model, tokenizer=tokenizer)\nsae = model.load_sae(\"sae_layer10.mlx\")\n\n# Example 1: What features activate for this text?\ntop_features = model.get_top_features_for_text(\n    text=\"Paris is the capital of France\",\n    sae=sae,\n    layer=10,\n    component=\"mlp\",\n    top_k=10\n)\n\nprint(\"Top features:\")\nfor feature_id, activation in top_features:\n    print(f\"  Feature {feature_id}: {activation:.3f}\")\n\n# Example 2: What texts activate feature #1234?\nfrom datasets import load_dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\ntexts = [item[\"text\"] for item in dataset if len(item[\"text\"]) &gt; 50][:1000]\n\nexamples = model.get_top_texts_for_feature(\n    feature_id=1234,\n    sae=sae,\n    texts=texts,\n    layer=10,\n    component=\"mlp\",\n    top_k=20\n)\n\nprint(f\"\\nFeature 1234 activates most on:\")\nfor text, activation, pos in examples[:5]:\n    print(f\"  [{activation:.3f}] {text[:100]}...\")\n</code></pre>"},{"location":"guides/sae_feature_analysis/#api-reference","title":"API Reference","text":""},{"location":"guides/sae_feature_analysis/#get_top_features_for_text","title":"<code>get_top_features_for_text()</code>","text":"<p>Find which SAE features activate most strongly for a given text.</p> <p>Signature: <pre><code>model.get_top_features_for_text(\n    text: str,\n    sae: Union[SAE, BatchTopKSAE, str],\n    layer: int,\n    component: str = \"mlp\",\n    top_k: int = 10\n) -&gt; List[Tuple[int, float]]\n</code></pre></p> <p>Parameters: - <code>text</code> - Input text to analyze - <code>sae</code> - SAE instance or path to saved SAE - <code>layer</code> - Layer number where SAE was trained - <code>component</code> - Component name (\"mlp\", \"attn\", etc.) - <code>top_k</code> - Number of top features to return</p> <p>Returns: - List of <code>(feature_id, activation_value)</code> tuples, sorted by activation strength</p> <p>Example: <pre><code># Analyze what the model \"thinks about\" when processing this text\nfeatures = model.get_top_features_for_text(\n    \"The Eiffel Tower is in Paris\",\n    sae=sae,\n    layer=10,\n    component=\"mlp\",\n    top_k=5\n)\n\n# Output:\n# Feature 42: 0.856  (might represent \"landmarks\")\n# Feature 128: 0.743  (might represent \"France/French\")\n# Feature 91: 0.621  (might represent \"locations\")\n</code></pre></p>"},{"location":"guides/sae_feature_analysis/#get_top_texts_for_feature","title":"<code>get_top_texts_for_feature()</code>","text":"<p>Find texts where a specific SAE feature activates most strongly.</p> <p>Signature: <pre><code>model.get_top_texts_for_feature(\n    feature_id: int,\n    sae: Union[SAE, BatchTopKSAE, str],\n    texts: List[str],\n    layer: int,\n    component: str = \"mlp\",\n    top_k: int = 10\n) -&gt; List[Tuple[str, float, int]]\n</code></pre></p> <p>Parameters: - <code>feature_id</code> - The feature index to analyze - <code>sae</code> - SAE instance or path to saved SAE - <code>texts</code> - Dataset of texts to search through - <code>layer</code> - Layer number where SAE was trained - <code>component</code> - Component name (\"mlp\", \"attn\", etc.) - <code>top_k</code> - Number of top examples to return</p> <p>Returns: - List of <code>(text, activation_value, token_position)</code> tuples</p> <p>Example: <pre><code># What does feature 1234 represent?\nexamples = model.get_top_texts_for_feature(\n    feature_id=1234,\n    sae=sae,\n    texts=dataset_texts,\n    layer=10,\n    component=\"mlp\",\n    top_k=20\n)\n\n# Examine examples to understand the feature\nfor text, activation, pos in examples[:5]:\n    print(f\"[{activation:.3f}] {text}\")\n\n# If they all relate to \"geography\", then feature 1234\n# likely represents geographic concepts\n</code></pre></p>"},{"location":"guides/sae_feature_analysis/#visualize_feature_activations-coming-soon","title":"<code>visualize_feature_activations()</code> (Coming Soon)","text":"<p>Visualize feature activations highlighted in text (Neuronpedia-style).</p> <pre><code># Future API\nmodel.visualize_feature_activations(\n    text=\"Paris is the capital of France\",\n    sae=sae,\n    layer=10,\n    component=\"mlp\",\n    feature_ids=[42, 128, 91]\n)\n</code></pre> <p>This will show an interactive visualization with tokens colored by activation strength.</p>"},{"location":"guides/sae_feature_analysis/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's a complete workflow for understanding what your SAE has learned:</p> <pre><code>from mlxterp import InterpretableModel\nfrom mlx_lm import load\nfrom datasets import load_dataset\n\n# 1. Setup\nprint(\"Loading model and SAE...\")\nmlx_model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\nmodel = InterpretableModel(mlx_model, tokenizer=tokenizer)\nsae = model.load_sae(\"sae_layer10.mlx\")\n\n# 2. Load dataset for searching\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\ntexts = [item[\"text\"] for item in dataset if len(item[\"text\"].strip()) &gt; 50][:1000]\n\n# 3. Pick an interesting text to analyze\ntest_text = \"The Eiffel Tower is a famous landmark in Paris, France\"\n\n# 4. Find top features\nprint(f\"\\nAnalyzing: '{test_text}'\")\ntop_features = model.get_top_features_for_text(\n    text=test_text,\n    sae=sae,\n    layer=10,\n    component=\"mlp\",\n    top_k=10\n)\n\n# 5. For each top feature, understand what it represents\nfor feature_id, activation in top_features[:3]:  # Analyze top 3\n    print(f\"\\n{'='*80}\")\n    print(f\"Feature {feature_id} (activation: {activation:.3f})\")\n    print('='*80)\n\n    # Find examples where this feature activates\n    examples = model.get_top_texts_for_feature(\n        feature_id=feature_id,\n        sae=sae,\n        texts=texts,\n        layer=10,\n        component=\"mlp\",\n        top_k=10\n    )\n\n    print(\"\\nTop activating texts:\")\n    for i, (text, act, pos) in enumerate(examples[:5], 1):\n        clean_text = ' '.join(text.split())\n        print(f\"  {i}. [{act:.3f}] {clean_text[:100]}...\")\n\n    # Form hypothesis about feature meaning\n    print(\"\\nHypothesis: (examine examples above)\")\n    # If examples all relate to landmarks: \"This feature represents landmarks\"\n    # If examples all relate to France: \"This feature represents French things\"\n    # etc.\n</code></pre>"},{"location":"guides/sae_feature_analysis/#feature-interpretation-tips","title":"Feature Interpretation Tips","text":""},{"location":"guides/sae_feature_analysis/#1-look-for-patterns","title":"1. Look for Patterns","text":"<p>When analyzing top activating texts for a feature, look for:</p> <ul> <li>Common words/phrases - Does \"Paris\" appear in all examples?</li> <li>Semantic themes - Are they all about geography? Science? Politics?</li> <li>Syntactic patterns - Do they share grammatical structure?</li> <li>Domain-specific terms - Medical terms? Technical jargon?</li> </ul>"},{"location":"guides/sae_feature_analysis/#2-use-multiple-perspectives","title":"2. Use Multiple Perspectives","text":"<p>Understand a feature by:</p> <ol> <li>Forward analysis: Text \u2192 Features (what activates for this text?)</li> <li>Backward analysis: Feature \u2192 Texts (what activates this feature?)</li> <li>Comparative analysis: How does this feature relate to others?</li> </ol>"},{"location":"guides/sae_feature_analysis/#3-validate-hypotheses","title":"3. Validate Hypotheses","text":"<p>Once you form a hypothesis about a feature:</p> <ol> <li>Find more examples (increase <code>top_k</code>)</li> <li>Test with custom texts</li> <li>Look for counter-examples</li> <li>Compare to similar features</li> </ol>"},{"location":"guides/sae_feature_analysis/#4-document-findings","title":"4. Document Findings","text":"<p>Create a feature registry:</p> <pre><code>feature_registry = {\n    1234: {\n        \"name\": \"geographic_locations\",\n        \"description\": \"Activates on city and country names\",\n        \"examples\": [\"Paris, France\", \"Tokyo, Japan\", \"New York City\"],\n        \"confidence\": \"high\",\n        \"notes\": \"Especially strong for European locations\"\n    },\n    5678: {\n        \"name\": \"mathematical_notation\",\n        \"description\": \"Activates on equations and formulas\",\n        \"examples\": [\"E=mc\u00b2\", \"ax\u00b2+bx+c=0\"],\n        \"confidence\": \"medium\"\n    }\n}\n\nimport json\nwith open(\"sae_features.json\", \"w\") as f:\n    json.dump(feature_registry, f, indent=2)\n</code></pre>"},{"location":"guides/sae_feature_analysis/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/sae_feature_analysis/#memory-usage","title":"Memory Usage","text":"<p>When analyzing many texts:</p> <pre><code># Instead of loading all at once\ntexts = load_all_texts()  # Might use too much memory\n\n# Process in batches\nbatch_size = 500\nfor i in range(0, len(all_texts), batch_size):\n    batch = all_texts[i:i+batch_size]\n    examples = model.get_top_texts_for_feature(\n        feature_id=feature_id,\n        sae=sae,\n        texts=batch,\n        layer=10,\n        component=\"mlp\"\n    )\n    # Process examples...\n</code></pre>"},{"location":"guides/sae_feature_analysis/#speed-optimization","title":"Speed Optimization","text":"<p>For large-scale analysis:</p> <ol> <li>Pre-compute activations - If analyzing many features on same dataset</li> <li>Use smaller dataset - 500-1000 texts often sufficient for understanding</li> <li>Parallel processing - Analyze multiple features concurrently</li> <li>Cache results - Save feature\u2192text mappings for reuse</li> </ol>"},{"location":"guides/sae_feature_analysis/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/sae_feature_analysis/#use-case-1-understanding-model-behavior","title":"Use Case 1: Understanding Model Behavior","text":"<p>Goal: Why did the model generate this specific output?</p> <pre><code># 1. Get the output\noutput = model.generate(\"Paris is\")\n\n# 2. Find which features were active\nfeatures = model.get_top_features_for_text(\n    \"Paris is\",\n    sae=sae,\n    layer=10,\n    component=\"mlp\"\n)\n\n# 3. Understand what those features represent\nfor feat_id, act in features[:3]:\n    examples = model.get_top_texts_for_feature(\n        feat_id, sae, texts, layer=10, component=\"mlp\"\n    )\n    print(f\"Feature {feat_id}:\", [ex[0][:50] for ex in examples[:3]])\n</code></pre>"},{"location":"guides/sae_feature_analysis/#use-case-2-feature-discovery","title":"Use Case 2: Feature Discovery","text":"<p>Goal: What concepts has the SAE learned?</p> <pre><code># Sample random features and understand them\nimport random\nall_features = range(sae.d_hidden)\nsample_features = random.sample(all_features, 50)\n\nfor feat_id in sample_features:\n    examples = model.get_top_texts_for_feature(\n        feat_id, sae, texts, layer=10, component=\"mlp\", top_k=5\n    )\n\n    if examples:  # If feature activates\n        print(f\"\\nFeature {feat_id}:\")\n        for text, act, pos in examples:\n            print(f\"  [{act:.3f}] {text[:80]}\")\n</code></pre>"},{"location":"guides/sae_feature_analysis/#use-case-3-circuit-discovery","title":"Use Case 3: Circuit Discovery","text":"<p>Goal: Find features that work together</p> <pre><code># For a specific behavior, find active features\nbehavior_text = \"The capital of France is Paris\"\nfeatures_for_behavior = model.get_top_features_for_text(\n    behavior_text, sae, layer=10, component=\"mlp\", top_k=20\n)\n\n# Hypothesis: These features form a \"geographic knowledge\" circuit\ncircuit_features = [f[0] for f in features_for_behavior]\n\n# Validate by checking if they co-activate\nfor test_text in [\"London is in England\", \"Tokyo is in Japan\"]:\n    active = model.get_top_features_for_text(\n        test_text, sae, layer=10, component=\"mlp\", top_k=20\n    )\n    active_ids = [f[0] for f in active]\n    overlap = set(circuit_features) &amp; set(active_ids)\n    print(f\"{test_text}: {len(overlap)}/{len(circuit_features)} features overlap\")\n</code></pre>"},{"location":"guides/sae_feature_analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation: See SAE Evaluation Guide for quality metrics</li> <li>Training: See Dictionary Learning Guide for training SAEs</li> <li>Examples: Check phase2_feature_analysis.py for complete code</li> <li>API Reference: See API Documentation for full details</li> </ul>"},{"location":"guides/sae_feature_analysis/#references","title":"References","text":"<ul> <li>Neuronpedia - Interactive feature visualization platform</li> <li>Anthropic's Scaling Monosemanticity - Feature interpretation methodology</li> <li>SAELens - Production SAE framework with analysis tools</li> <li>Toy Models of Superposition - Understanding feature representations</li> </ul>"}]}