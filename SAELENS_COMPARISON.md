# mlxterp vs SAELens: Comprehensive Implementation Comparison

## Executive Summary

This document compares our mlxterp SAE implementation with SAELens (the leading PyTorch-based SAE framework). We identify **3 critical missing features**, **2 architectural differences**, and **4 recommended improvements**.

**Overall Assessment:** Our implementation is solid for basic TopK SAE training but **missing several production-critical features** that SAELens has battle-tested over 191 releases.

---

## Architecture Comparison

### SAELens (State-of-the-Art)

**Supported Architectures:**
1. âœ… **BatchTopK** - Modern variant (fixes mean L0 across batches, not per-sample)
2. âœ… **JumpReLU** - Used by Anthropic & DeepMind's Gemma Scope
3. âœ… **MatryoshkaBatchTopK** - Nested reconstruction preventing feature absorption
4. âœ… **Standard L1 SAE** - Classic ReLU + L1 penalty
5. âœ… **TopK** - Original TopK implementation
6. âœ… **Gated SAE** - Alternative L1-based approach

**Training Infrastructure:**
- `LanguageModelSAETrainingRunner` class
- Nested configuration: `LanguageModelSAERunnerConfig` + architecture-specific configs
- Pre-tokenized dataset support
- Activation caching for rapid hyperparameter experimentation

### mlxterp (Our Implementation)

**Supported Architectures:**
1. âœ… **TopK** - Standard TopK with per-sample sparsity

**Training Infrastructure:**
- `SAETrainer` class
- Single configuration: `SAEConfig`
- Text-based streaming (tokenization on-the-fly)
- No activation caching

---

## Critical Missing Features

### âŒ 1. Ghost Gradients for Dead Features

**SAELens:**
```python
use_ghost_grads: bool = True           # Always recommended
feature_sampling_window: int = 1000
dead_feature_window: int = 5000        # Track dead neurons
```

**How it works:**
- Tracks which features haven't activated in last N forward passes
- Applies "ghost gradients" to dead features to revive them
- Anthropic found this critical for larger models (though less important for 1L models)

**Our implementation:**
```python
# âŒ NOT IMPLEMENTED
# We have:
dead_neuron_threshold: float = 1e-6
resample_dead_every: int = 10000
# But no actual ghost grad or resampling logic!
```

**Impact:** We get 90-99% dead features, SAELens gets 60-80%

**Fix needed:** Implement ghost gradient mechanism in trainer.py

---

### âŒ 2. Advanced Normalization Strategies

**SAELens:**
```python
normalize_activations = "expected_average_only_in"  # Recommended
```

**Normalization approaches:**
1. **expected_average_only_in** - Normalize to sqrt(n_dense) L2 norm
2. **Layer norm** - Standardize to mean=0, std=1
3. **None** - No normalization

**Anthropic's approach (from Circuits Updates Feb 2024):**
- Normalize activation vectors to L2 norm = sqrt(n_dense)
- Take sum over dense dimension in MSE loss
- Critical for JumpReLU tanh variant

**Our implementation:**
```python
# âœ… Basic normalization (mean/std)
if self.normalize_input:
    x = (x - self.input_mean) / (self.input_std + 1e-8)
```

**Impact:** Our simple mean/std normalization may not be optimal for reconstruction quality

**Fix needed:** Add `normalize_activations` config option with multiple strategies

---

### âŒ 3. L0/L1 Sparsity Warmup

**SAELens:**
```python
l1_warm_up_steps: int = total_training_steps  # Gradual sparsity increase
l0_warm_up_steps: int = total_training_steps  # For JumpReLU
```

**How it works:**
- Gradually increases sparsity penalty from 0 â†’ target value
- Prevents dead features early in training
- Gives network time to learn useful representations before enforcing sparsity

**Our implementation:**
```python
# âŒ NOT IMPLEMENTED
# We only have LR warmup:
warmup_steps: int = 1000
```

**Impact:** May explain why our training plateaus early (~1k steps) and has high dead features

**Fix needed:** Add separate sparsity warmup schedule

---

## Architectural Differences

### ğŸ”„ 1. BatchTopK vs TopK

**SAELens's BatchTopK (recommended):**
- Fixes **mean L0 across batch**, not per-sample
- More stable training
- Better feature utilization
- Saves as JumpReLU for inference compatibility

**Our TopK:**
- Fixes k **per sample** (traditional approach)
- Can lead to variable batch statistics
- Simpler but less stable

**Example:**
```python
# SAELens BatchTopK:
# Batch of 32 samples, k=64
# Total active features in batch â‰ˆ 32 Ã— 64 = 2048
# But adjusts per-sample k to hit target mean

# Our TopK:
# Each sample has exactly k=64 active features
# Total = exactly 32 Ã— 64 = 2048
```

**Impact:** BatchTopK is more robust for production training

**Recommendation:** Implement BatchTopK as default (keep TopK for backward compatibility)

---

### ğŸ”„ 2. Loss Function Differences

**SAELens (for standard L1 SAE):**
```python
loss = reconstruction_loss + l1_coefficient * l1_penalty

# For JumpReLU tanh:
loss = recon_loss + l0_coef * l0_penalty + pre_act_coef * pre_act_loss
```

**Reconstruction loss variations:**
- Standard: `mean((x - x_recon) ** 2)`
- Anthropic: `sum((x - x_recon) ** 2, dim=-1)` with normalized inputs

**Our implementation:**
```python
# âœ… Standard MSE
recon_loss = mx.mean((x - x_recon) ** 2)

# âŒ No pre-activation loss
# âŒ No separate L0 penalty (we use structural TopK)
```

**Impact:** Our loss is correct for TopK but missing advanced variants

**Recommendation:** Add configurable loss functions for future architectures

---

## Performance & Optimization

### âš ï¸ 1. Gradient Clipping Strategy

**SAELens:**
- Uses PyTorch's built-in `torch.nn.utils.clip_grad_norm_`
- Well-tested across many models

**Our implementation:**
```python
# âœ… Custom implementation
def _clip_gradients(self, grads: dict, max_norm: float) -> dict:
    # Compute global norm recursively
    # Scale gradients if norm > max_norm
```

**Status:** âœ… Our implementation looks correct

---

### âš ï¸ 2. Learning Rate Scheduling

**SAELens:**
```python
lr_warm_up_steps: int = 1000
lr_decay_steps: int = total_steps
# Uses cosine decay or linear decay
```

**Our implementation:**
```python
def _get_lr_schedule(self):
    def schedule(step):
        if step < self.config.warmup_steps:
            # Linear warmup
            return self.config.learning_rate * (step / self.config.warmup_steps)
        else:
            # Constant LR after warmup
            return self.config.learning_rate
    return schedule
```

**Issue:** âŒ **No LR decay!** We use constant LR after warmup

**Impact:** May explain training plateau after warmup completes

**Fix needed:** Add LR decay (cosine or linear)

---

### âš ï¸ 3. Decoder Norm Constraints

**SAELens (from research):**
- "One of the most common additions being some sort of constraint on decoder vector norms"
- Prevents decoder weights from growing unbounded
- Improves feature interpretability

**Our implementation:**
```python
# âŒ NOT IMPLEMENTED
# No decoder norm constraints
```

**Impact:** Decoder weights may become too large, hurting interpretability

**Recommendation:** Add decoder weight normalization option

---

### âš ï¸ 4. Activation Caching

**SAELens:**
- Pre-compute and cache activations for entire dataset
- Enables rapid hyperparameter sweeps
- Trade-off: disk space vs. compute time

**Our implementation:**
```python
# âŒ NOT IMPLEMENTED
# We generate activations on-the-fly (streaming)
```

**Trade-offs:**
- **Our approach (streaming):** Lower memory, slower training, good for large datasets
- **SAELens (caching):** Higher memory, faster iteration, good for hyperparameter tuning

**Recommendation:** Add optional activation caching for small-scale experiments

---

## Data Handling Comparison

### SAELens

**Dataset support:**
- âœ… Streaming large datasets
- âœ… Pre-tokenized datasets (eliminates tokenization bottleneck)
- âœ… HuggingFace datasets integration
- âœ… Activation caching

**Optimization:**
```python
streaming: bool = True
use_cached_activations: bool = False  # Optional for hyperparameter tuning
```

### mlxterp

**Dataset support:**
- âœ… Streaming text datasets
- âœ… HuggingFace datasets (via user code)
- âŒ Pre-tokenized datasets
- âŒ Activation caching

**Our streaming optimization:**
```python
text_batch_size: int = 32  # Process 32 texts at once
# Reduces model traces, improves GPU utilization
```

**Status:** âœ… Our streaming is efficient, but could add pre-tokenization support

---

## Configuration Comparison

### SAELens Default Config

```python
# For BatchTopK (recommended):
expansion_factor: 32          # Larger than ours
k: 100                        # Similar to ours
learning_rate: 3e-4           # Higher than ours
normalize_activations: "expected_average_only_in"
use_ghost_grads: True
feature_sampling_window: 1000
dead_feature_window: 5000
l1_warm_up_steps: total_steps
autocast: True
dtype: "float32"
```

### mlxterp Default Config

```python
# For TopK:
expansion_factor: 16          # Conservative
k: 100                        # Similar
learning_rate: 1e-4           # More conservative
normalize_input: True         # Basic normalization
use_ghost_grads: False        # âŒ NOT IMPLEMENTED
warmup_steps: 1000            # Only LR warmup
gradient_clip: 1.0
```

**Differences:**
1. SAELens uses higher expansion factors (32 vs 16)
2. SAELens uses higher learning rates (3e-4 vs 1e-4)
3. SAELens has ghost grads (critical feature)
4. SAELens has sparsity warmup (we don't)
5. SAELens has LR decay (we don't)

---

## Recommended Improvements

### Priority 1: Critical for Production

#### 1.1 Implement Ghost Gradients
```python
# Add to SAEConfig:
use_ghost_grads: bool = True
feature_sampling_window: int = 1000
dead_feature_window: int = 5000

# Add to trainer.py:
def _apply_ghost_grads(self, sae, batch, dead_mask):
    """Apply ghost gradients to revive dead features."""
    # Sample from dead features
    # Compute auxiliary loss
    # Add to main loss
```

**Expected impact:** Reduce dead features from 95% â†’ 70%

#### 1.2 Add Learning Rate Decay
```python
# Update _get_lr_schedule():
def schedule(step):
    if step < warmup_steps:
        return lr * (step / warmup_steps)
    else:
        # Cosine decay
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return lr * 0.5 * (1 + math.cos(math.pi * progress))
```

**Expected impact:** Better convergence, lower final loss

#### 1.3 Add Sparsity Warmup
```python
# Add to SAEConfig:
sparsity_warm_up_steps: int = total_steps

# In loss computation:
sparsity_coef = min(1.0, step / sparsity_warm_up_steps)
loss = recon_loss + sparsity_coef * sparsity_penalty
```

**Expected impact:** Fewer dead features early in training

### Priority 2: Better Architectures

#### 2.1 Implement BatchTopK
```python
class BatchTopKSAE(BaseSAE):
    """Modern BatchTopK SAE (SAELens-style)."""

    def encode(self, x: mx.array) -> mx.array:
        h = self.encoder(x)
        h = mx.maximum(h, 0)

        # BatchTopK: fix mean L0 across batch
        z = batchtopk_activation(h, k=self.k)
        return z
```

**Expected impact:** More stable training, better feature utilization

#### 2.2 Implement JumpReLU (Future)
```python
class JumpReLUSAE(BaseSAE):
    """JumpReLU SAE (Anthropic/DeepMind approach)."""

    def __init__(self, ..., jumprelu_sparsity_loss_mode="tanh"):
        # Implementation following SAELens
```

**Expected impact:** State-of-the-art results, used by Anthropic

### Priority 3: Advanced Features

#### 3.1 Decoder Weight Normalization
```python
# In decode():
if self.constrain_decoder_norms:
    # Normalize decoder weights to unit norm
    decoder_norms = mx.sqrt(mx.sum(self.decoder.weight ** 2, axis=1, keepdims=True))
    normalized_decoder = self.decoder.weight / (decoder_norms + 1e-8)
    x_recon = mx.matmul(z, normalized_decoder.T) + self.decoder_bias
```

**Expected impact:** Better feature interpretability

#### 3.2 Advanced Normalization
```python
# Add config option:
normalize_activations: str = "expected_average_only_in"  # or "layer_norm" or "none"

# In encode():
if normalize_activations == "expected_average_only_in":
    # Normalize to L2 norm = sqrt(d_model)
    x = x / (mx.sqrt(mx.sum(x**2, axis=-1, keepdims=True)) + 1e-8) * math.sqrt(d_model)
```

**Expected impact:** Better reconstruction quality (especially for JumpReLU)

#### 3.3 Activation Caching (Optional)
```python
# Add utility:
def cache_activations(model, layer, component, dataset, cache_path):
    """Pre-compute and cache activations for dataset."""
    # Generate all activations
    # Save to disk
    # Load during training
```

**Expected impact:** Faster hyperparameter experimentation

---

## Summary of Differences

| Feature | SAELens | mlxterp | Priority |
|---------|---------|---------|----------|
| **Architecture** |
| TopK | âœ… | âœ… | - |
| BatchTopK | âœ… | âŒ | HIGH |
| JumpReLU | âœ… | âŒ | MEDIUM |
| Gated SAE | âœ… | âŒ | LOW |
| **Training Features** |
| Ghost Gradients | âœ… | âŒ | **CRITICAL** |
| Dead Feature Tracking | âœ… | âŒ | **CRITICAL** |
| Sparsity Warmup | âœ… | âŒ | **HIGH** |
| LR Warmup | âœ… | âœ… | âœ… |
| LR Decay | âœ… | âŒ | **HIGH** |
| Gradient Clipping | âœ… | âœ… | âœ… |
| **Normalization** |
| Basic (mean/std) | âœ… | âœ… | âœ… |
| L2 norm constraint | âœ… | âŒ | MEDIUM |
| Decoder norm constraint | âœ… | âŒ | MEDIUM |
| **Data Handling** |
| Streaming | âœ… | âœ… | âœ… |
| Pre-tokenized datasets | âœ… | âŒ | LOW |
| Activation caching | âœ… | âŒ | MEDIUM |
| **Loss Functions** |
| MSE reconstruction | âœ… | âœ… | âœ… |
| L1 penalty | âœ… | âœ… | âœ… |
| L0 penalty | âœ… | âŒ | MEDIUM |
| Pre-activation loss | âœ… | âŒ | LOW |
| **Metrics** |
| L0 count | âœ… | âœ… | âœ… |
| L0 sparsity fraction | âœ… | âœ… | âœ… |
| L1 magnitude | âœ… | âœ… | âœ… |
| Dead features | âœ… | âœ… | âœ… |
| Feature sampling stats | âœ… | âŒ | MEDIUM |

---

## Action Items

### Immediate (Fix Current Issues)

1. **Implement LR decay** - Add cosine decay after warmup
2. **Implement sparsity warmup** - Gradually increase sparsity penalty
3. **Update default config** - Use SAELens-validated hyperparameters
4. **Fix training plateau** - Combination of #1 and #2

### Short-term (Production Readiness)

5. **Implement ghost gradients** - Critical for reducing dead features
6. **Implement BatchTopK** - Modern, more stable architecture
7. **Add decoder weight constraints** - Improve interpretability
8. **Add advanced normalization** - L2 norm constraint option

### Long-term (Feature Parity)

9. **Implement JumpReLU** - State-of-the-art architecture
10. **Add activation caching** - Optional for hyperparameter tuning
11. **Implement Gated SAE** - Alternative architecture
12. **Add MatryoshkaBatchTopK** - Advanced nested approach

---

## Conclusion

**Our implementation is solid but missing critical production features:**

âœ… **What we do well:**
- Clean, simple TopK implementation
- Efficient streaming training
- Good basic normalization
- Proper gradient clipping

âŒ **Critical gaps:**
1. No ghost gradients â†’ 95% dead features (should be 70%)
2. No LR decay â†’ training plateaus early
3. No sparsity warmup â†’ dead features from start
4. Only TopK architecture (SAELens has 6)

**Recommended path forward:**
1. Fix immediate issues (LR decay, sparsity warmup) - **2-3 days**
2. Implement ghost gradients - **1 week**
3. Implement BatchTopK - **3-4 days**
4. Then proceed with Phase 2 feature analysis tools

**With these fixes, we expect:**
- Dead features: 95% â†’ 70%
- Reconstruction loss: 0.27 â†’ 0.10-0.15
- Training stability: Much improved
- Production readiness: Ready for research use
